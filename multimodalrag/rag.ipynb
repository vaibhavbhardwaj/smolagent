{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "57192aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "OPEN_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "23068008",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract data\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "\n",
    "output_path = \"./content/\"\n",
    "file_path = 'attention.pdf'\n",
    "\n",
    "# Reference: https://docs.unstructured.io/open-source/core-functionality/chunking\n",
    "chunks = partition_pdf(\n",
    "    filename=file_path,\n",
    "   \n",
    "    infer_table_structure=True,            # extract tables\n",
    "    strategy=\"hi_res\",                     # mandatory to infer tables\n",
    "\n",
    "    extract_image_block_types=[\"Image\",\"Table\"],   # Add 'Table' to list to extract image of tables\n",
    "    #image_output_dir_path=output_path,   # if None, images and tables will saved in base64\n",
    "\n",
    "    extract_image_block_to_payload=True,   # if true, will extract base64 for API usage\n",
    "\n",
    "    chunking_strategy=\"by_title\",          # or 'basic'\n",
    "    max_characters=10000,                  # defaults to 500\n",
    "    combine_text_under_n_chars=2000,       # defaults to 0\n",
    "    new_after_n_chars=6000,\n",
    "\n",
    "    # extract_images_in_pdf=True,          # deprecated\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b7fb1754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "[<unstructured.documents.elements.Title object at 0x0000022AB6A7F310>, <unstructured.documents.elements.NarrativeText object at 0x0000022AB6A7F460>, <unstructured.documents.elements.NarrativeText object at 0x0000022AB6A7F620>, <unstructured.documents.elements.Title object at 0x0000022AB6BE2A50>, <unstructured.documents.elements.NarrativeText object at 0x0000022AB6A7F690>, <unstructured.documents.elements.NarrativeText object at 0x0000022AB6A7F230>, <unstructured.documents.elements.NarrativeText object at 0x0000022AB6A7F770>, <unstructured.documents.elements.NarrativeText object at 0x0000022AB6A7F850>, <unstructured.documents.elements.NarrativeText object at 0x0000022AB6A7F930>, <unstructured.documents.elements.Title object at 0x0000022AB6A7FA10>, <unstructured.documents.elements.NarrativeText object at 0x0000022AB6A7FAF0>, <unstructured.documents.elements.Title object at 0x0000022AB6A7FBD0>, <unstructured.documents.elements.NarrativeText object at 0x0000022AB6A7FCB0>, <unstructured.documents.elements.Title object at 0x0000022AB6A7FD90>, <unstructured.documents.elements.NarrativeText object at 0x0000022AB6A7FE70>, <unstructured.documents.elements.NarrativeText object at 0x0000022AB6A7FF50>, <unstructured.documents.elements.NarrativeText object at 0x0000022AB6C39E10>, <unstructured.documents.elements.NarrativeText object at 0x0000022AB69B9940>, <unstructured.documents.elements.NarrativeText object at 0x0000022AB6DE8280>, <unstructured.documents.elements.NarrativeText object at 0x0000022AB6A7F2A0>, <unstructured.documents.elements.NarrativeText object at 0x0000022AB6A7F380>, <unstructured.documents.elements.NarrativeText object at 0x0000022AB6A7F540>]\n",
      "[<unstructured.documents.elements.Title object at 0x0000022AB6A7F7E0>, <unstructured.documents.elements.NarrativeText object at 0x0000022AB6A7F9A0>, <unstructured.documents.elements.NarrativeText object at 0x0000022AB6A7FB60>, <unstructured.documents.elements.NarrativeText object at 0x0000022AB6A7FD20>, <unstructured.documents.elements.NarrativeText object at 0x0000022AB6A7FEE0>, <unstructured.documents.elements.Title object at 0x0000022AB6DE8050>, <unstructured.documents.elements.NarrativeText object at 0x0000022AB6DE82F0>, <unstructured.documents.elements.NarrativeText object at 0x0000022AB6DE83D0>]\n",
      "[<unstructured.documents.elements.Title object at 0x0000022AB6DE84B0>, <unstructured.documents.elements.NarrativeText object at 0x0000022AB6DE8590>, <unstructured.documents.elements.Footer object at 0x0000022AB6DE8670>, <unstructured.documents.elements.Image object at 0x0000022AA39D7EE0>, <unstructured.documents.elements.FigureCaption object at 0x0000022AB6E2FA10>, <unstructured.documents.elements.NarrativeText object at 0x0000022AB6A7F700>, <unstructured.documents.elements.NarrativeText object at 0x0000022AB6A7FA80>, <unstructured.documents.elements.Title object at 0x0000022AB6A7FE00>, <unstructured.documents.elements.NarrativeText object at 0x0000022AB6DE8210>]\n",
      "[<unstructured.documents.elements.Title object at 0x0000022AB6DE8360>, <unstructured.documents.elements.NarrativeText object at 0x0000022AB6DE8520>, <unstructured.documents.elements.Footer object at 0x0000022AB6DE8750>, <unstructured.documents.elements.Title object at 0x0000022AB6C3A120>, <unstructured.documents.elements.Title object at 0x0000022AB6C3A0B0>, <unstructured.documents.elements.Image object at 0x0000022AB6A6D090>, <unstructured.documents.elements.Image object at 0x0000022AB6A6FA70>, <unstructured.documents.elements.NarrativeText object at 0x0000022AB6A7F8C0>, <unstructured.documents.elements.Text object at 0x0000022AB6C3A190>, <unstructured.documents.elements.NarrativeText object at 0x0000022AB6DE8130>, <unstructured.documents.elements.NarrativeText object at 0x0000022AB6DE87C0>, <unstructured.documents.elements.Formula object at 0x0000022AB6DE88A0>, <unstructured.documents.elements.NarrativeText object at 0x0000022AB6DE8980>, <unstructured.documents.elements.NarrativeText object at 0x0000022AB6DE8A60>, <unstructured.documents.elements.Title object at 0x0000022AB6DE8B40>, <unstructured.documents.elements.NarrativeText object at 0x0000022AB6DE8C20>, <unstructured.documents.elements.NarrativeText object at 0x0000022AB6DE8D00>, <unstructured.documents.elements.NarrativeText object at 0x0000022AB6DE8DE0>, <unstructured.documents.elements.Text object at 0x0000022AB69499B0>, <unstructured.documents.elements.Formula object at 0x0000022AB6A7F3F0>, <unstructured.documents.elements.NarrativeText object at 0x0000022AB6A7FC40>, <unstructured.documents.elements.NarrativeText object at 0x0000022AB6DE8600>]\n",
      "[<unstructured.documents.elements.Title object at 0x0000022AB6DE8830>, <unstructured.documents.elements.NarrativeText object at 0x0000022AB6DE89F0>, <unstructured.documents.elements.ListItem object at 0x0000022AB6DE8BB0>, <unstructured.documents.elements.ListItem object at 0x0000022AB6DE8D70>, <unstructured.documents.elements.ListItem object at 0x0000022AB6DE8F30>, <unstructured.documents.elements.Title object at 0x0000022AB6DE9160>, <unstructured.documents.elements.NarrativeText object at 0x0000022AB6DE9240>, <unstructured.documents.elements.Formula object at 0x0000022AB6DE9320>, <unstructured.documents.elements.NarrativeText object at 0x0000022AB6DE9400>, <unstructured.documents.elements.Title object at 0x0000022AB6DE94E0>, <unstructured.documents.elements.NarrativeText object at 0x0000022AB6DE95C0>]\n",
      "[<unstructured.documents.elements.Title object at 0x0000022AB6DE96A0>, <unstructured.documents.elements.NarrativeText object at 0x0000022AB6DE9780>, <unstructured.documents.elements.Footer object at 0x0000022AB6DE9860>, <unstructured.documents.elements.NarrativeText object at 0x0000022AB6A7F4D0>, <unstructured.documents.elements.Table object at 0x0000022AA39D7A10>, <unstructured.documents.elements.NarrativeText object at 0x0000022AB6DE8440>, <unstructured.documents.elements.NarrativeText object at 0x0000022AB6DE8AD0>, <unstructured.documents.elements.Formula object at 0x0000022AB6DE8E50>, <unstructured.documents.elements.NarrativeText object at 0x0000022AB6DE91D0>, <unstructured.documents.elements.NarrativeText object at 0x0000022AB6DE9390>, <unstructured.documents.elements.Title object at 0x0000022AB6DE9550>, <unstructured.documents.elements.NarrativeText object at 0x0000022AB6DE9710>, <unstructured.documents.elements.NarrativeText object at 0x0000022AB6DE9940>, <unstructured.documents.elements.NarrativeText object at 0x0000022AB6DE9A20>, <unstructured.documents.elements.NarrativeText object at 0x0000022AB6DE9B00>, <unstructured.documents.elements.Text object at 0x0000022AB6949320>, <unstructured.documents.elements.NarrativeText object at 0x0000022AB6DE9B70>, <unstructured.documents.elements.NarrativeText object at 0x0000022AB6DE9010>, <unstructured.documents.elements.NarrativeText object at 0x0000022AB6DE8C90>]\n",
      "[<unstructured.documents.elements.Title object at 0x0000022AB6DE92B0>, <unstructured.documents.elements.NarrativeText object at 0x0000022AB6DE9630>, <unstructured.documents.elements.Title object at 0x0000022AB6DE99B0>, <unstructured.documents.elements.NarrativeText object at 0x0000022AB6DE9C50>, <unstructured.documents.elements.Title object at 0x0000022AB6DE9D30>, <unstructured.documents.elements.NarrativeText object at 0x0000022AB6DE9E10>, <unstructured.documents.elements.Title object at 0x0000022AB6DE9EF0>, <unstructured.documents.elements.NarrativeText object at 0x0000022AB6DE9FD0>, <unstructured.documents.elements.Formula object at 0x0000022AB6DEA0B0>, <unstructured.documents.elements.NarrativeText object at 0x0000022AB6DEA190>, <unstructured.documents.elements.Title object at 0x0000022AB6DEA270>, <unstructured.documents.elements.NarrativeText object at 0x0000022AB6DEA350>, <unstructured.documents.elements.NarrativeText object at 0x0000022AB6DEA430>, <unstructured.documents.elements.Footer object at 0x0000022AB6DEA510>, <unstructured.documents.elements.FigureCaption object at 0x0000022AB6CC6A50>, <unstructured.documents.elements.Table object at 0x0000022AB6A6EAD0>, <unstructured.documents.elements.NarrativeText object at 0x0000022AB6DE8910>]\n",
      "[<unstructured.documents.elements.Title object at 0x0000022AB6DE9470>, <unstructured.documents.elements.Title object at 0x0000022AB6DE9A90>, <unstructured.documents.elements.NarrativeText object at 0x0000022AB6DE9DA0>, <unstructured.documents.elements.NarrativeText object at 0x0000022AB6DE9F60>, <unstructured.documents.elements.NarrativeText object at 0x0000022AB6DEA120>, <unstructured.documents.elements.NarrativeText object at 0x0000022AB6DEA2E0>, <unstructured.documents.elements.Title object at 0x0000022AB6DEA580>, <unstructured.documents.elements.NarrativeText object at 0x0000022AB6DEA660>, <unstructured.documents.elements.NarrativeText object at 0x0000022AB6DEA740>, <unstructured.documents.elements.Text object at 0x0000022AB6A7C670>, <unstructured.documents.elements.Footer object at 0x0000022AB6DEA900>, <unstructured.documents.elements.NarrativeText object at 0x0000022AB6DEA890>, <unstructured.documents.elements.Table object at 0x0000022AB6A6D770>]\n",
      "[<unstructured.documents.elements.Title object at 0x0000022AB6A7E660>, <unstructured.documents.elements.Text object at 0x0000022AB6A7E4A0>, <unstructured.documents.elements.Text object at 0x0000022AB6A7E510>, <unstructured.documents.elements.Text object at 0x0000022AB6A7E6D0>, <unstructured.documents.elements.Text object at 0x0000022AB6A7EF90>, <unstructured.documents.elements.Text object at 0x0000022AB6A7EEB0>, <unstructured.documents.elements.Text object at 0x0000022AB6A7E580>, <unstructured.documents.elements.Text object at 0x0000022AB6A7E5F0>, <unstructured.documents.elements.NarrativeText object at 0x0000022AB6DEACF0>, <unstructured.documents.elements.Title object at 0x0000022AB6DEADD0>, <unstructured.documents.elements.NarrativeText object at 0x0000022AB6DEAEB0>, <unstructured.documents.elements.NarrativeText object at 0x0000022AB6DEAF90>, <unstructured.documents.elements.NarrativeText object at 0x0000022AB6DEB070>, <unstructured.documents.elements.NarrativeText object at 0x0000022AB6DEB150>, <unstructured.documents.elements.NarrativeText object at 0x0000022AB6DEB230>, <unstructured.documents.elements.Text object at 0x0000022AB6A7E820>, <unstructured.documents.elements.Text object at 0x0000022AB6A7E890>, <unstructured.documents.elements.Title object at 0x0000022AB6DEB2A0>, <unstructured.documents.elements.ListItem object at 0x0000022AB6DE97F0>, <unstructured.documents.elements.ListItem object at 0x0000022AB6DE9E80>, <unstructured.documents.elements.ListItem object at 0x0000022AB6DEA5F0>, <unstructured.documents.elements.ListItem object at 0x0000022AB6DEA9E0>, <unstructured.documents.elements.ListItem object at 0x0000022AB6DEABA0>, <unstructured.documents.elements.ListItem object at 0x0000022AB6DEAD60>, <unstructured.documents.elements.ListItem object at 0x0000022AB6DEAF20>, <unstructured.documents.elements.ListItem object at 0x0000022AB6DEB0E0>, <unstructured.documents.elements.ListItem object at 0x0000022AB6DEB380>, <unstructured.documents.elements.ListItem object at 0x0000022AB6DEB460>, <unstructured.documents.elements.ListItem object at 0x0000022AB6DEB540>, <unstructured.documents.elements.ListItem object at 0x0000022AB6DEB620>, <unstructured.documents.elements.ListItem object at 0x0000022AB6DEB700>, <unstructured.documents.elements.ListItem object at 0x0000022AB6DEB7E0>, <unstructured.documents.elements.ListItem object at 0x0000022AB6DEB8C0>, <unstructured.documents.elements.ListItem object at 0x0000022AB6DEB9A0>, <unstructured.documents.elements.ListItem object at 0x0000022AB6DEBA80>, <unstructured.documents.elements.ListItem object at 0x0000022AB6DEBB60>, <unstructured.documents.elements.ListItem object at 0x0000022AB6DEBC40>, <unstructured.documents.elements.ListItem object at 0x0000022AB6DEBD20>, <unstructured.documents.elements.Footer object at 0x0000022AB6DEBE00>, <unstructured.documents.elements.ListItem object at 0x0000022AB6DEBD90>, <unstructured.documents.elements.ListItem object at 0x0000022AB6DE98D0>, <unstructured.documents.elements.ListItem object at 0x0000022AB6DEA7B0>, <unstructured.documents.elements.ListItem object at 0x0000022AB6DEAC80>, <unstructured.documents.elements.ListItem object at 0x0000022AB6DEB000>, <unstructured.documents.elements.ListItem object at 0x0000022AB6DEB3F0>, <unstructured.documents.elements.ListItem object at 0x0000022AB6DEB5B0>, <unstructured.documents.elements.ListItem object at 0x0000022AB6DEB770>, <unstructured.documents.elements.ListItem object at 0x0000022AB6DEB930>, <unstructured.documents.elements.ListItem object at 0x0000022AB6DEBAF0>, <unstructured.documents.elements.ListItem object at 0x0000022AB6DEBCB0>, <unstructured.documents.elements.ListItem object at 0x0000022AB6DEBEE0>, <unstructured.documents.elements.Header object at 0x0000022AB3B5A360>]\n"
     ]
    }
   ],
   "source": [
    "# Each CompositeElement containes a bunch of related elements.\n",
    "# This makes it easy to use these elements together in a RAG pipeline.\n",
    "print(len(chunks))\n",
    "set([str(type(el)) for el in chunks])\n",
    "for chunk in chunks:\n",
    "    print(chunk.metadata.orig_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d740624d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'Image',\n",
       " 'element_id': 'da3fb822-a782-4183-8e2a-c63c64331de9',\n",
       " 'text': '',\n",
       " 'metadata': {'coordinates': {'points': ((np.float64(486.0),\n",
       "     np.float64(261.1805555555558)),\n",
       "    (np.float64(486.0), np.float64(614.7805555555556)),\n",
       "    (np.float64(664.0), np.float64(614.7805555555556)),\n",
       "    (np.float64(664.0), np.float64(261.1805555555558))),\n",
       "   'system': 'PixelSpace',\n",
       "   'layout_width': 1700,\n",
       "   'layout_height': 2200},\n",
       "  'last_modified': '2025-06-07T12:39:51',\n",
       "  'filetype': 'PPM',\n",
       "  'languages': ['eng'],\n",
       "  'page_number': 4,\n",
       "  'image_base64': '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',\n",
       "  'image_mime_type': 'image/jpeg'}}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is what an extracted image looks like.\n",
    "# It contains the base64 representation only because we set the param extract_image_block_to_payload=True\n",
    "\n",
    "elements = chunks[3].metadata.orig_elements\n",
    "chunk_images = [el for el in elements if 'Image' in str(type(el))]\n",
    "chunk_images[0].to_dict()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c4addba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate tables from texts\n",
    "tables = []\n",
    "texts = []\n",
    "texts_c = []\n",
    "elements1=[]\n",
    "image_c=[]\n",
    "\n",
    "for chunk in chunks:\n",
    "    if \"CompositeElement\" in str(type((chunk))):\n",
    "        texts_c.append(chunk)\n",
    "    elements = chunk.metadata.orig_elements\n",
    "    for element in elements:\n",
    "        elements1.append(element.category)\n",
    "        #print(element.category)\n",
    "        if (element.category == 'Table'):\n",
    "            tables.append(chunk)\n",
    "        elif(element.category != 'Image' and  element.category != 'Table'):\n",
    "            texts.append(chunk)\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a531e054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the images from the CompositeElement objects\n",
    "def get_images_base64(chunks):\n",
    "    images_b64 = []\n",
    "    for chunk in chunks:\n",
    "        if \"CompositeElement\" in str(type(chunk)):\n",
    "            chunk_els = chunk.metadata.orig_elements\n",
    "            for el in chunk_els:\n",
    "                if \"Image\" in str(type(el)):\n",
    "                    images_b64.append(el.metadata.image_base64)\n",
    "    return images_b64\n",
    "\n",
    "images = get_images_base64(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "0557cd4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import base64\n",
    "from IPython.display import Image, display\n",
    "\n",
    "def display_base64_image(base64_code):\n",
    "    # Decode the base64 string to binary\n",
    "    image_data = base64.b64decode(base64_code)\n",
    "    # Display the image\n",
    "    display(Image(data=image_data))\n",
    "\n",
    "display_base64_image(images[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f2d8de73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'CompositeElement',\n",
       " 'element_id': 'd0c135ce9138d50900d8097fa16c6a69',\n",
       " 'text': '3.5 Positional Encoding\\n\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\n\\n5\\n\\nTable 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types. n is the sequence length, d is the representation dimension, k is the kernel size of convolutions and r the size of the neighborhood in restricted self-attention.\\n\\nLayer Type Complexity per Layer Sequential Maximum Path Length Operations Self-Attention O(n2  d) O(1) O(1) Recurrent O(n  d2) O(n) O(n) Convolutional O(k  n  d2) O(1) O(logk(n)) Self-Attention (restricted) O(r  n  d) O(1) O(n/r)\\n\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and xed [8].\\n\\nIn this work, we use sine and cosine functions of different frequencies:\\n\\nPE(pos,2i) = sin(pos/100002i/dmodel) PE(pos,2i+1) = cos(pos/100002i/dmodel)\\n\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from 2 to 10000  2. We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any xed offset k, PEpos+k can be represented as a linear function of PEpos.\\n\\nWe also experimented with using learned positional embeddings [8] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.\\n\\n4 Why Self-Attention\\n\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu- tional layers commonly used for mapping one variable-length sequence of symbol representations (x1,...,xn) to another sequence of equal length (z1,...,zn), with xi,zi  Rd, such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata.\\n\\nOne is the total computational complexity per layer. Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required.\\n\\nThe third is the path length between long-range dependencies in the network. Learning long-range dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network. The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies [11]. Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types.\\n\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [31] and byte-pair [25] representations. To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\n\\n6\\n\\nthe input sequence centered around the respective output position. This would increase the maximum path length to O(n/r). We plan to investigate this approach further in future work.\\n\\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output positions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels, or O(logk(n)) in the case of dilated convolutions [15], increasing the length of the longest paths between any two positions in the network. Convolutional layers are generally more expensive than recurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity considerably, to O(k  n  d + n  d2). Even with k = n, however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model.\\n\\nAs side benet, self-attention could yield more interpretable models. We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences.',\n",
       " 'metadata': {'filetype': 'application/pdf',\n",
       "  'languages': ['eng'],\n",
       "  'last_modified': '2025-06-07T12:39:51',\n",
       "  'page_number': 5,\n",
       "  'text_as_html': '<table><thead><tr><th>Layer Type</th><th>Complexity per Layer</th><th>Sequential Operations</th><th>Maximum Path Length</th></tr></thead><tbody><tr><td>Self-Attention</td><td>O(n? - d)</td><td>O(1)</td><td>O(1)</td></tr><tr><td>Recurrent</td><td>O(n-d?)</td><td>O(n)</td><td>O(n)</td></tr><tr><td>Convolutional</td><td>O(k-n-d?)</td><td>olny</td><td>O(logx(n))</td></tr><tr><td>Self-Attention (restricted)</td><td>O(r-n-d)</td><td>ol)</td><td>O(n/r)</td></tr></tbody></table>',\n",
       "  'orig_elements': 'eJzsvOmyq1pyLvoqK/Zfam/6zv9AIBrRCRAgfE446Fsh+u7Effc7NNdeVeXyqghvH9vh6+sZoZiT0eTIkSPzyy+ZoH/8P79kbfbKuvmfqvSXf/j2CxqTKYIg+K85iiK/EhGT/hpTGfIrQ5NJTsUMnhHML3/69ssrm6M0miMw5//8krzfY1p10ZxNX9dtdLyX+Z/KrCrKGbRgGIKAOb83b1U6l6AVpb9a+3fVzZ95//iPGEv/hv7pG8pQ2G/I//7Tt780MOxv9KeBQtDfyJ82fE0BLb9MxzRnr89erGrPWqePkuyX/wd0pNmcJXP17v4paaNp+qd+fMdgGPIbQ1I4BgbkVZvNR599zbX0X75U7oolKr729Y+/ZF3xy//+ap3mf3q90yqvsi+rYQhG/opQvyK0i2L/gLP/QKKf2T2Y+U/d8oqzEYwiP0rM2f6xyC/4b+Q36z1VH32i9pvYJUAcEP8Z8rsKbjW3X5r/7RnlLIWiGZb9GmUsOCOCSH9lGTb/lc4wlM7SiKDo6D/wjNjfQBPKovT3E/i9AUynvh8aSiDgcH7W8vukf9spsQSJoP/Jp+RUXZJ9ey/jNyAna78l726Oqm761r2/jVmyjGP2GRB16acF9K7vdvko/6dvVfcNGDwbv+Xv8dtcZr9LmN/fXlGTfVsmIDf/6vg+7PeLKRuWj8w/fdvAlGWagaAaGOTb9H5l4G8g7RV9VvgWxeCgvuaMWQua1o8k0Dp9VMi+9b871w/B87vJgOJArb9e5rdv7hs0VNO3rEu/1ozS9Nv/+qX/i2tmv7vm9L9++Wj/mV11PVg6AxZLv3q+RV+K/LXzGtE4finlfkz5EydOCRInPgeSsEn0K0Gl2a8xHRG/AofDGJykEDwj/8OcmCGw34iPh1Lob/jHQ//cAHDly2UZkv6N+mnD15R/mwvTKE3/ZwMN+dencn2/ZzDkZ8eRUDRKseyvNMsyvxJYyvwas3H8K5UgWY6RFMPG+H8o7n/BA0D7H7D/ARCG/H46AD+w3/CfNHyf8W/EExz/z8AT6q8Pw43iNvuG/sM3Pdqr1/L61kdz+a0FK8zl9KdvfTb+CowF0CB5v/o226v5+AKXV9V9Df8u9RPS3wN4rkCAvsG0L0iYvrAG6JdnAJfmb99FffY2/fat+waC/K9D//dl//Qt/dEzZv2YTWDmd4BJK+Af0xeYNT+GNNnYARSbqvMLvv4K8aYvRb9D3Y/uz9/dxxni91i+3+kHfcAK81glc5YCTdr812ieP/t4d7/9Ifig0SiK8Aj/NUUz6lciQ5Nf2ZSifsVpCkUSnIyZOP4P81ccI35j/vQNR/HfiI87/n5Noszv+Q7g18eB/0XD9xn/Rn/FSIb+ub9Wr4/LxdGUUcSnGWZrmOC4u9OEql1wPHfnwBX4zQkwtvECxxUSXyQSfy9kfkvUy724CfzdELhJE/jGETfmIdvvXC6RCAwo1AvRKRdlUvYtuQlJrwncbgh3RD4S0nKLw7oQpyFo32XfGyBvA3IKQ7CLQ7G3Uq+V4//uo8Ebx9kXTjB4rhA4R+G4UuS5XeRh5s7J22d/D7BP8fPh/vIjXrjtfgV7VS73t/YZO9uguRAFblNEbntcvxvnynJ3BdjlbvO2UuoPURLRa8kf6n4VbnwTiYqC3PbN9hwk5xodU4+iaG7XMnlK9rtVq+R9c96I4T5xU2ioO2Jf7SZVnEfvelfP99Ay9F9GE/phF0ntkOA2mnYpkQWDUCq+VBKVWvU3t1V9vyWr8DXc6v7mv3qq6ofxds6aj810RS7geHY9kHamVo9Zc1Ej8FG2DrFFrgjIbcklf1PZuDOrhbB5jENWR323D7DJJv6wz/ct/137iB/7XMAggeeS7/a5q/jX2F0UeUfc+VLlH/e0TGy90t+ichVv9juW+Yujn4+V856dZl/KZ2OYYsvYXzZ6AYv1t/qBGWdBmo0h2o9QdsT29kBtz2vTp+/1dSh5rxAr+6gzkDQI8UxuqeJtiuUzkivldWt689GGUfBSX9F7MJs+irrh9hqm0UTmOMIX7UVtkyXuSSwfendDZuuBpnGAGV1ELFZDZnFHmd3wz+zDvjmB00FsKDbYH2dtAgKcLc2F2oYrni3gOSLXdz5Zryxgm/R6rhh5Lt5KF9STRnd4E7azJix4uxTt2MGcDJegrfay7fHgMtAuCKZVwPBbjUgYhzklBx/UBmvoBCTAnPV7RwlDhkfVo/8+KLc39nz3YzxvFKTe7WWYPE41dUZk0tsO81at5hLU5lgfUWHiz40filmKqklBmFxvycFIPbjE+Okaj5z5atyEv16Y5u07T8AM972T4b46Nx4mcoYjPup+bZK6+EvOgr3K4PPnWf8J6/hX4sU48ka1hTLrC+ssMUVC5x1jxxm/zqPupDvw5wdpRpxlWQxXdvKWw4g4JzAN9APnwEVvmAh3RgZ63h30hFcYKBCN73ElGtyoiXbKBXQnHujVcNc31sxEPhtLehS6y2w5B1FZT3tzpBFDh2oB3dkjfh9phx4k/LpZs7AvU3Wd12OqRflVCAUMSUkOAIr70mK7wAzQi/mzFgz3XQuG28Df+7A+bzfO+rT/63XHLlUhfHzwe+cmfHVyAC7B3/tQw99FbTyvBRb08UXwkQa2eL85BrRfvjSEfxfJcTDM8T9mMRz7fRZjwX/RjrO+i+SMTyfDfRThLGmwvm8W5sTvanO6BT5/1hD4/JeGP0Ryn7P/8ywQc79vFob/rB3D/Y/9/rvaLw9b2zlF/q43DcN5O9sRjLkGOpVdWWkx/DlOT1nNjdt0XXz5oZyhBhd6BhucrPwMV36Azd/iSfDE1rrvGN8PyvFhcKjgsvelBaTx8Ua029BWj9Q36Ul/SjqDoiwn10DGzzAIrMB9Afkm/N5B5PCfUZ7h4K9OhvvrTvl7I8zdPo0Ml0PAfuZfYJPTv5SGvhIH6C/B/M8sjvsoAM7veweYBXYyXZMFr3WVuSvWJXm1gf6Y0IU92lOhYslO6lvpuMc9WwQaYL/1n4HJ9vAa8VdHzb3RxZSV4JNBK4cb6LZ0oCChhvATRqzpwmXx3zmfvyObYpBKOW7nMJrpwHX8k1A3KyhEuHSHj4tuws/96u/g/p+D6Cdx+cNF/zYef4Q4Z/40Lt9/J/Z/BNHP4tL6eTwC7RZ1zSsRbQlyhi9aJKTrcD1lctJGTSZjG9ZTzMmsxdhr68rJtz/IKaBefYg1vx+W0/7n7enL4uCMf4o19d/Bsx9W/wnW/ACgv8WYH9r9cZ8YXkPURpHVp+fgsTdl9LFan8ZuestUhwE9hj+ixzLGO57vkQzHCF5qD5hW9QBwzcfknhdLutN2XNz1hBEbYbhRnZPeXW+WptSpH72cYkxCDW7HpfdNphAu8/7YGf8ewpvwR7Bxul7/C+Pd+G5tAsuU6n7z5UOUVV5RoOVus9GZC68xyGZKu+bB0uBm6y4nP/qEGQS8wsOylLWc/Op+KvcH4nDm34DrOgB0jVxiR+KN1nrvcAH6TAYW3G1YWciJE+e9ERphvPPbf1WbDeZElUbvtyeOeA2LkxdDYR7xMMb3i1tbvCU7U7BpBI3gO9vrLNsD9+SLlhl8KZbYjLtnlwz4lgjyrPiH8mwvV/Uuh5AICzM6GowcSTgHpzuBm7vCPTJSHXpCZ7x6795JNt2j1Ir8ZH/eZ51qO2zy1K03+UZMIHB0/M/tKP/Mfj/KLM76qR1/KPw39ss2r4nCgSXz6Bzm2nqzaHT33fdkoDd7WortcDoElkcM1FF/Tl7/2jz9FVWgkPyXfvbD+f5dasV/Y8wbyTTF1yuuc871rFVWB2hzh/+IDCcCv8Uq99IMde1Y2+ZBR+m+juOWt7i3bPLFRPCnPxMZk0QBUaAOjRR24JUD+zYQtsz2UvWCmiyLRjft45K92Ygzr3+Hl/79XNGPhyrP+DLQ9RyoewMVLMVZSv7HeO9CotpevV6QwcqMfkRxcmVdsayXirpYF8U3uYuvEhzJUSxFZX+YR6RtiFTpG2H27eK5prGzOjsQ9IYydwsSX4wddW9EKvFEXNoj8zLHdXb6iR32XjQzi7CWUwjNf7Pci+59yzd2Yt2qspZ6iebPesggobb6wMrEVRLz6nA9xHVJWp31VZpIsUudPc3XPFU5kiDUN0e4PzuH6kunS1UBG1go4DbqFX+A/Y1IC7OckWjMxYKcBybDQQb0eka0tdtJhTE6VeXKEuIZ3tSOic34K33dOu7yvuQ75ZEko85+vO0EN883NLfnfB6Pm++9XGgfe6Z8qTzv6/V656kxORzbn6bmYb9a+3LYXHE9+E3v+q4PXq8F0i3RKust7eFK8+jqIjzCEIGQSKeUq+has3KvSRbY8mxqbPTy3QYhdLh+i1h26kzEqLm9GztRGc5cPFDPfGGLRCA9rLMAjifCSOIDgcp+XuxGuGXSHOf+xjxe/Ykbw9a0acj5pg9FLsbxlxrZgco8c4dFc86u6R3x4XIVK0QiFmSe72rpdTPYdiKyy5IUBODh1xjr8RnCK9wxdMBRQ5F5tGc2PNGYRRZuuWRl+AKgGCM9NkZxBJkWn55P3FiteKNK7j7gun84Cus9h6zkuZiHpSWHRHYekq6con6MbyyrDNgjLkg4uV+boFvbWsEyiHhw9yMzwjSOvZw20SSK2i5xfHSG3wIud1MHViqKm39w/FmE51l4MqTI1S0g7oFVlalVTik4mj65B4lQ+sqtTopDKMoiS2rTPj1towd/zIXcY5+I9lgFbMShV2CwakrlWZRGr0Q0E1riKLmmyBPbIsySJ2ykZkabQydw4j0e4IcW+aIouMd7IZMsnHgOVDCWyemCqph3wWd0J9NGKHdZD5UYhIaOeHSRJkLqSmmdR+kICyVbqnUXTE6ojdRT4+04beewB6RR1fpZtxe7aPLCuR3rRe8bRo0e4z0Qxcbrr1dVTu6WSE0DJ159zgpWZIxAaLeMlzPdFbqA2ByPF4j/15hDKguPQYnbV8ahlOfMvmw5vTFcuiphudy6sRG1HaHZgVx3S00dWHY3dEDMJtq1Hi3jVUgCPXudUzVtjcxfkppsSck+gwE9Yx8aotiIcDF4+SgqPkkFf9RJLfA8T8r8myNhj/WXcHQ35lVS5SvwVARzJSMQtvhCH+Jzs1pZ0gVNqKYbxNiQplpVrxHuNNauMoazYeCyyS6nIV2IQBNShhZ4ifNk00V2Lye8SB787sTZdXhs3cxe2hZZ3dAka62yMQtujDyObC9/GHaNnOw7cU87EBubsUAsi5rTcuG6Xj3eNHHO1KGXe+fflEAMKO3kcnsSNuO72Iwus5Bh2ftuZ956D3G+Pe4X3UDghWWqQYRr/DF7jHZG7Bt6X/DFGA/s9ercrhWPmzt7gIuRhQ7f7nwjdJLb79QNrvAdNoiymUk6d+jYlREIizJlX02mNNlt5PQrF3MH3zB1NbmbRVTouHn9LRPThNEiPMGNirax+Xz662sPxPAJSPLBmWvCCFIiA373xsvaV42cMXxpcP1k7ozEjnst8xypTmRB6DrpnmTYDR166omy6tM+olqWr6kThu/nuYh3lQispHjyVkLuu3cU3Bv8cLfbDwD+A9eWtdbe4O19t82oPYYvmy52jNbXx+3kDOh5WPuYQDLWbVHIWJbAzvEw9GxZNXGMhPhD3+HrS4WlMajgjeSJmblF5buhIX9G5VLVQ8y8SN66PS+LzkiZwtqMEmkxJK1XqKFKXntqpHiFY39qz/jc61IHLnW/6wIv34V3BhS806M7RvSq3ehXfVqhY6gXghp8u7EIUc+sZ9Ym41taoo7Aa9SEEAwO3Nmc6Z72kdumKczuSqGMJ7Hjbu8tM1dLwp8RaQFvkjTZHXuNC/UZz2yDXwhCjHSONJNQ5SSzNJKAEOaxIzKH2okMdZDl7OmgDvwOMLpZ1Zsm5oRnWphoOcs8RZo68+B8VmJiagcYm6m075vnDItBSSwT9MSyy5P3b8sV5wpjtiSgTX4N9byK9xlwGK6vmecWtT06q3xsJWuchwaMFbcdj1+aEG5CAQAC+FJgWIhDT7S8o0S70DfSIVBEn+ew55Bmq3yjrY8BsDwsaVJgeHKo+8QgeehctV5/tdRrecNPdYxpJb7EZ64/1yJOlySpu55GOD/NGC/T4hzkQoKatWUrZlsQks4545Zl4tYue+eeELF1qWW3G/2SwJiAonsDJR6xbYdGHJ3ZfbuqCODwmFAnlzt3T2TSsbHxig43ZsmxlEjVClk9CdVoPsZW7O6L+OtiX17ENV3upnAiLDVCpvbeMXk7OmNMaSJ9zDMi3PVm8bxknJ6AkoA0OkUrUTE+dMlFrJ7Hln5S3Tsb8PssRRdUF/ypztOdOcNgRFWTy1oTdeGGCO6Am00XB4bK1et2g8tlrI/4Hm3jIKLyJ1W83sF8Jv7U3Aq7oJ+tWBXTeV2SlDNzZ8j6FxETdOQOFyoeYl+XXo6PtMhc6s+20fS7sQU8ndR1B2rXpGUiY7zSA3OtBxygPXY92F4kJn8RDKWQyJd54V4EfeePRARA7q5CVwm7RSwM5+KJriDjLsQL4x8RrlSv9H4RyuU+hrKZCd1YX59ZHZUDvuGMG8F9C2oGNH4YMirBzUpLNEJhOoQc6eMp9AjHJkUnCa/lWHIxG7CuhEjUD2ztPEl3nlLT4fvQxvgXM+ni4TZQVS8gFM2EkbMbmzDP3b+uQur4LRlmTyNlDixgUUW6jWKhQAygLAyXMI03ZPoqLnN0JSZG84032rMF6rPX1VgqteUknTCTp5m7RoJmkTd0gD23Pdi1+04dNNO8lhmD55ihTatZXUoS3j1dLm6ZbRKPmhq+C00HwicoMmJiDdQhytm4X/fksKNc36raxNtJ3bJVfjytKweobRGNGpPkgjcKIJ/rGubtqgC8cppsTJdMgSU6+cGaaBRUQQD2aWYmLLRzglM5o8BD02GZExxXYT8JyEvxO1/XM5yxtyHT9gSM8fJLXmnQOUIkgmpRhrcZOndtZLwedHoMem0S6Rog4zuX85LNr3BhMhPujRGp35c4OUT+HbYSmTRikW6OgACyPa8ZzsB1b1hzPMZPH21JlijrB5lit/0mUbXi3ztKQ04fTy+EMSqaTTSLsdJz72mKQBeiWDMRGRPJDuQAguPP/pa6aPhY+oOvOew2cFl33EDymyKKKoQj3/qx2zkmyS4rHU6pg7y2xWsB8ZEQrUqG7KpqubLG750mh7PPdz9iGYFKB50FEDiw6KcWezn21Q+V3j5o2mb2h8ZgSEIqUsaG+RYzNga14BiUMLYABW8X9VU5+ESqttEu1fS+ked+2KAyMaNiWbm8km2Lwxqqf/oDLleXHU3ncNFuekHPoPahDMfsL6W9o1uSDpxZuns9oDVAxNfmZIE/q6j4ukWPKGM0LOm4R0NwN7FbD13u0DrYrwSKBh8gBd3G2XsTW8a7cw/U4lmmhCAomth0ps7yRcdT3kxZxLmu4rhtvgyAg7XyREhOqoszV768rZNPOm3GOG7TQTp4mjIRR4RLoNkthlw3detpxPCZoDOymdSmThv7vuHXROgslSDeHCsiaIwa2uyhN0dyIPFWEZkoKsjLet0Khjhfq8bGMEhRjJmzRQg4JUaM6LDKabpobV6Tvblqb2w+qCCMMnQ0kj0KTKo8ydW9hM/cXJXzztPXwAAxO0541dPqPTWRm3QwLOtI28QstAQOqbxfD8fpoTdNQktmYXNqNvAUbzSoii5wEj+fZ3/dxeLJXmiz2N40MV4WGvdoi3hDLlEg64N9IIu5V1JnRFjflMZepvT0hh7Wgj9ViLLugBBJJ2EyDsjeC2NSuCynGBXVPQtbzw1vHs/IDMKiV2dSulz0Qqaew0pcmSfuD9Al9VBh59PMHfz2cPeAUqUBDk+5gW96wmRLdo8G+IIEaDvAFXWgghNDSNuhr4z1yGdiX+W94KFapsJhHK0yo0tISxKvOGonFxPuhToyYkyU7u+U1T4sCEKBtgl0Yx1Eu6/CPA8IkrNLtOXdcxP7uFDXhL6ScaRr9eJ2ytVLTZAXZ4FgZkMCSWuk7NzWvTWfhG2OjlrFloS7ukVCnCedHLl4umnoQIwJ3dLL1K10d6cru9cciwZQXJUBVi+JIHMqqIBGD8n3NMvU8OlNBofGLnFVMnIx3HlVL4M5KIsTPFkTVD+dmXCFbPgySCgoUaLwxgWR+gyyxp+g5rj3vIkuu0sW5F40nfaUXAANhWzaVBxfQPpTFt8eC2V9zZJ+8/eztmIjiyGuTkD1Wu9r4oL8l64xBRqsyHUxEYkc/poYzAmyxpv6OK0b/c49R4nxKK6r6m0czMrSn3a6tbzCdxHuufWiF4k3oeda+xAauJawoYI9Rg61gIApYlqT9FcqKMPUkXqWYTo7cJbB8pAiTbCw0+62M/fZTDXvKhr9gCj4vhSERKOcnOQ6X99Yih59WPRKwkGC+yg753VaOwed/A2ZElCdqs22auWD2zhQmTbCaxr3dQTDG0aMyv4WaUOLDh7OPYopNMzWI8jKap+Thw1XUwBuTVCvyQWJQK6su8XhxUIjEEeVA1wO7hmOvk7qlcGyVzYRAHmKe8BesLUWvNjd0R5roVQbpZoqxht6aDwRzKqN4DLHP5g0sykmoaCOwnPggjlaocjBQBsV9sbO1llmGkHbddxBSIx4G0iOEht2dwBabWsuIpPod5A8+wh736LfvcQ37pn3EMm0RghCShrmaamZNI6PtWRBONqUN0glNYMKfEghln3iaR0bZq+xXizfic3M711fIFwclOOujcuIr7Xlxgc1AxLk2884yi6CRJfnEjrOxS1CbjMX3ePfa3X6bABdNOiGLe/3/KhAVUGqjniVmb7nrrduur8p7wHJ1gsHg4m6cq5j5kRMLbfL3Xl5eUGoUh7wFmo813LJvVggrKgnbi1jUJsRzLTDls8XppRKnm1haNbgFwfoeiKsdx4dICuk880l/DVu9hzrx2ofJKZ/OdaN7hpMO+6ctdSDY9roIKHMMxqjzJxNZD379Inkj2voL/bTurD9zXwRIJ109XhYQd11Y2evYpJvLPCXvcdq3EaDDaNegUs22OHs9H2k7SNLhFrK3B2zkj92QApfYdIKafOTySAVFvSU7efp1anck+5j+3XjUTxPNFjfQY06F09KtkQcJER1fC2QvIpQh7P49Qp8DH+gHTvPxJ2DkMQULc7i2aUFnotjlUbsmT5CsTs71URjJn5dE8fXj/2QTJ2fdG9HZXnEQNHIzOMMiuy0t0Ci8Av0c0uN9ujhbCvfMeZN4Ka3nCRpxMkwTxpBB13RMu0Hx3HcC6En6S4sl0Mick3flHDqVIqlFuhiSdICKXnZpxSshhv+GhUG0LsnDwItoIdahcfXEGzNSrIyw2FQRdW9NPoo2pev6+ZP/WXVR91YPrcEcEax3I7bO4GRNaJFRkKm5iiK+9cb66qLSgZl1c2qgm9n0fTLg39h7cSF5JWT7WgkBrTbDrx03MdIXSIajehijmHXt6+6/KrbN7ueT2+xXlrI2i4gdrDqW+UOo+iaw3bkHcviuAFe4Ftv9GuZ++i1HFiwr4KZ33hnUkV5FrClOC5LS2ZnRQsoAd3gSjOOxbWw0JujEftaIOCPispvWylWyhW/esq5JYVIgGotez8ehXm/6Az03h2Eu4If7ioEFmTnqAaPaPu5fywE1VLkDHddrUoANJSHaCTYvX4SkxWQwXvFwCwryhbs2quzPvOrQbI1Y0TFNtMD0XzuztYgTFOyS6t+05WLfeKWvAY8xRJpvo/MNfZPwG0OHGQiGjAX6UbMXdn4mcFdCoEDNQ15t/emKoQ7k9YQHoIozVffLykvCXT8jaKgrq8DuLjFL5iQykEmgBnOKOdOd6zO2U3xx+OKMMYiYVBEvNrb2M0NcfWlScNtq9QG3YOvWbAsUReeTqDMdEh2zRI+D2MR+Y1nhtDkS9s3CC0OWAPixVcXpf3y9tdKOooOu+PXTpUwsg+eV4Mq9HQMOSkDhY8hA8mvJZMMizECaAgaIcDe+EM02N5VKsiZCpPYCSgmhSJUnzI2wIxtPJk7JCmx67PWwkG1fb6Xfqv4oKkqldMrGaMLkqrn1yKgPd098nJ9B5iw7WOOvRD0gi6p24KMy2C+fb/TCWzdnJ0iNh6XndUBsKda4nyGkNxMhl1MMaYcVGlo0WGPpMlDJE5sUg6q6OYmycITVBKQFOOPOWyW+Rkx55vdoNQodcV7VZuF1/JY1s1gCvPmNl19Y7RMTWPU2kliQLRHN8Ea0j8txVjFo4CefayFz6aTTDiaViI/qI5w0e6x8i0dRgALjPnmI+PDDY9ZJuPkpXYYQZxb1gXd++ZuBbrLjL9KKWv0wChYCs0r++pAudY47oFs2AUgDJWtUXFxN3YE+RHkl6DqHHswX1iILLR4CR7pKzBBAVlrrFurKsi8XLqdWwuGa3v9Xt53bTA7Ks2oIHOwB4ppj+OV2SZxb3i+qD8c6kKQAZHBkRbsJUEu8QgAbya0AxLZVTWerqI+HkYztlneo2YUMze8QzB590H8p485N9EqYyuGdslWb9w4ud490pLvCeCJYiLXp5/a0GU+ib7OJBzNDmG8XWvBXg5rtnhafFo8Yc1et/fEhK72WrpPUCRcqMphowuWAUe9H2bILZn5Jq0xMKFrKoDyGeLxSri7fcvGUaewG5Nc0di6BMf9IDSOI/eaQgWiW9gH4+a8L9xHQKLG5zpGElp2TnSGul56zvU8YAAiOheqCiVydLDDxAulN3boypfmYNOrQK3Hk3YcMzIKzFfgGmTFrIQeTTq8qXWEK8AjDcg0RGTJWIR+PI1UnBInD2tuHpWCPxt9DxPdvCRd16RRCSkjnr1xuMgiHKm9ADqjmc+CWw/ogXTRxVC7cnaHna8OMs8kkPcrc8HTFXgIfp2fp3sEjwl4LLVrysAnjUbLClXL5o3T0G5cK8GGK1DxSHOIXjBUQtHbkRFFW825El0R9ViM/Vi4rD8R1szg8bSCdRZgjp9BTDNZF7Vq6M+dcbWSHdNP+30OGHItFsulQnJEelxMYnN7lncnHdoa4mQVKt4c4zMqdfJveG+YbLVinMFB7VvaIfaYt0xnbanfIfOGpgqS+2kIeD1LUA1SkVN8wyY3xtI5IepuwTDi8OASCGXhCYJucGGMCw5pQUVBS6qgGT6xbnP3593oEUqB0ClpemYNLPPGGijL2jWonhqoUWRUjf07NnSyviB0tsoByGNcRBPbB44sHusgZQ4WRTHstnNpsM138bB1CCkvVodzomw6VtR8nrHk+5l5rNw8x1Sv8nziBJoqKtvxpuyaPASWaEz7ROjUDRRqMWy0I3TXT+Ry9HF0sq7qkLBnNKlq4N+vdbu+Grbg7m9W91BA7jPqxbgQt14ffkO92TI6A25R0FmKItG5vR+g0pjM0nItgnlwNmS0KwFXK2Mhc6+EI3HHq8pJrg8t84rpvhKf9N4DH0NnDaV3oQ92I9jGMYXjqI3xtogPbFHG+JDq0FduItcphJVbzkTvmpUGIA8yItXJb2iEOck560VIjsvWJnYWyqBq9Rwdep87vj+wIY98yJLq4IjSwcV06XV83fKrb8+2eU53A8I/Wdz93PKjgqBZtwkmbzHjfP73A9cwK7xGNAPk+Nm1T927XyvxPuUbdzj50yNw4gIYjcBkmcHeUdk28nI2HgvagDLgzR5b6kVud3k7psR/6MnEZrGTmVodaICddA7rS12Fv9pyIuBJOKuk35d0el6mzeSWC3+4ZxExaw3KITlIDWS0WRFtQRYUAgNOqVjjhau4nrp+3KxXLCFJvmfLjbS2fniNuZCRqLydAB1OX6N2CD1cgK2f/8ylgX12EUhdGUx0c9FwEzUz91x0YH5fr821E3oMUIbJQ2Pcm0hQAPWLqwkyIEWK1WMnQEAZ1FceJPmI7NMDM6mzsdKHhCyGVc+HIE6Fil8enQYg8yn0u5tXMRybE3ypH4SEvGiDzZ1dwqY44ntSaXxb2si7yE83JwOkTiSOCN41CI06QmMM2zdbakxOdbjOiWN/bhYGtX196r6E3whV5kkrKoQhF63KJ2ZY8svsOt7sOIsf6OvVLk7x3Pd9cEsuM3aF6hROvqMtpQLfUfqcqEEuBGVFtb1m7aZqvdZgo6ZpImZevOQm8aa1MIR19azKzbAIEMx4UWcVrew31izPEXYIAz1j4gJrU/weQ4lOLqb+PJLqc5/kAZL4uQuWLL5GO8IHTqJiL37slik+CNvhRIlv6DuPW5bBSTFjPdAsJ8rMeC1G2LygE0f0pMiuOf65lzBYdMERxFAT4gyomOEz2Z0yBmPsh3oXknasfJxhD5C5HzeH46mGZHVh9MH2MpqxcnHdW/5z39btB+8UEh9ws+Hc9ZeEi4vJg1LQdIuC6zhtLRt0f1ItgbRraqAm6lCfzFnoaKl3r113C5BiXTrNgd1zSMKrDGK87Oovs+uoMYQufewQurLVeimfXDoVNRPd9rXAryn2Kqg1xjFQ1rQ6NJz4RU2xpjEpY1uQYuIExJKDzzMJSPU3//kyXcahQuJABV8REdcfbxuSOfJBv62L33mV277PWkrE52EXtWWk6hPz1n4uY8wYJYXO8Nq/qTx8GWpOJi8j4Ny8bdt37gF+uGuCE+NsXdGDrfIL1kdpOIwGO1zJS4DfkueN4EnZkqWkfi0N0OhkRWSw7FQppkeTPfu2VzuI1ZRdM0kWezZwHagzETL3mpgMYvQt53zCT2viKgNXVGOB0xVGfR3CABNf/w3/BvzXXY/lKndlHeSiP0Ky1jkoQtwpN++zle0X2e4vvXwUmrTJ7tgxN/pVFXxjN7rzCkXnXpOtPW88W911uLnlf7HVp17hA4mi30GF+2nWzuVotDzhgYRQdwVgHpGzWN3DqDk8qc2My/jutRxAt6jSpOpdlY/H07vr4LD79kC3eVbQXr6qJs9UznG9qix2mWzIHnzgB1VelkQAarqUeEoVNbJFgzmr0qCjSYe4bF5GZTHhBMXQvznRr6pK9pXZfiEZVSMVqEfZe6RXePtyk8rAVBkC1peZG4AC48UsPaSOEL6m4av25TRWvKqsHk3pHZzzqX6LkJCfHoL+y1XAdaBCpiVVmUhn+zxGKIRR+5CAUuyC4PGdbor4ZYV2E0Jv6M2Z03tsc1pnovy5WjDhmBs8C/ECncBFWAnP8YRe9R3DwxPfsd7i4Nqzru2aYSBBvw60dsM6xtm3j/Myr+C3jOr4nRVYh0sDNwRFWbIPo8PJ0WjMvPUUCVBe8D1QXBayyyqYayQTbxBM3RBRMSlNweV8z3Oy0HacbpfGkN4WL5s2Qnm48X17jknt73PZPE9ydhNAC8Jptg8q8tGHAY/jB7MBnCosFUX1A9LxnuOy45LdUY+upARfAyYAnJ1e8IV1HwaJoMtrl6AnsenCXKT8ne+Rv7bm31r3//6a300QAevVWImjZexMjmk3LBMfkIG5G57JrXg0xb68HL0hlR12pb2HughNTwxFiaKbUnYim3IerxJ2Ix/jEuKmv0kxt73J89zr7N83bv8LXa+yJflkjlUwHwcWLL/jR2VZ7mnhK0Xe8j6Y0mQlUOa6ZdroIKnWjppnWK/r/sQHbdG1omBqUDLqNQb87+ERM/EoIAyeucxU3kHrnBlycVZeeSjNeUWeSUECjn9DccBnByaBn+sEs22ksjNG53OHUe6RwPiqMWsKb7LFcNutA07NAx4L6AcekRGLug+1TgKWomjXUBx3M49a3awE4bIMFuKtFwgckg7cnKl+QFgE3XUlLLPj9vTxPOf47QzJA8vdwgsQax9HZi1Ge5YXtVdZp0VRVm0Xy9DjY7cL/a6a7yd5iZdF78bTgwnjgJQGkgabCqIo1WtcxFs3uG5MWMXpuTyUdKMZMaSSNMINF7pGTsbRsj3r6PVt+JM20jk/CGOdJ6Sq6TzCqyzzlANZ4VBLyCsHYSLKHZbaF/2Xz6k16mDmW/K0UqgFPnnidUNSXY2PymyIcbBWO3EgEoTGno+gmoPf9guoPx1qwDgsKbimk+9MePDVsiDZwST4ue2QSneOlqHlUyeQ8xTUpHUbvsK4oSnEQlCphHsNWAVJuWiUjGsAsozr2St/zg/gPcwC6prI0e1hKJidAwCdhDL65jTcpgM3zZEKRwWv13BXMUwI0Pz6GMI+rkJHP3WehS8F15ZlQhKnzxPBq0HdDZDTVFnRV3ROI0bVM5193kMU5Fm7WIpK8LKryoQ3UzXxZFxs3c18t9/T8gJVaOrfsGTyGHp4chehNd4mAOF9P1w2CsFoKWJ7s4duxHxf4kvgztGr6RhI0dX0HO72ZF2KEF7lnibf3ABfRjt2XMKmckMmkeueYEjRdMUrBGw4ly7CjsjOpG04MaPCfcWS0BK5VwNKrccxNBN9xfDVfAhiZybxxEBtChIKIu8hoAny+PaQV+PAT6Sp55eT3bN0eU6XHcsVvqk5IWSnBIUv68YudEAEJ3RZxQCziU6squh8ASXNQHiG182LeTkAJbZqPBjNBvEhIDjaprgTkcITjojZ9dPVbNpLDL/MzHsysbuB/BzIkGqJV5ioIDkvrJ3TDnqiMRb18bmHlTZ85IIpqTVE5a+al+652wcx9ioh7ZnzOHzRXIdKtusSG6knjjRwxkjS7QD2+QyDpSxhB2DllbvCaSavgl9C8nV60q4NKjwKDxp6OCFfkqzWXtxFVdkM5aR8XDqVcKCbVVC5M77YAEL7kM5gP6Ka9roBd7wWu6VoIbGZPAISWgTHQPuKz+u9e55HxOf7fafivHQYb7gv8oMg7WpXcYGcuBS4BzExOqQYCahkrrM/K6zuyDYZvFrHpxrBuW5vJRBtiZ8tlSKpy+yQkxG3ixHN47Aj+IO9rcEDY3R2O1qedG6HeSklPOmBjzHsjQMVLXu98kSKUNIFgvB7H7+HwIdz8igSY6FJ2YyQYbN4q5wgMeehjpLimr6j0xj35v1y6r6ps4BOydMNp/7DoV6HH2yK5LZ1g2dQo0gbQ1stHNGlMkN6uC/KWStU9h4/z/GL9PfnpVHq8z6HIMkwMSyfx6JJGC5N+OtZayP7PLNf7CUM3d4w4a5fzz0LwfvzrL7+zOFqoC4LQPGLyDA3KENsFDuCCNoOQ60UUOAyG6x12Hnn6D8iE4tvzDzb0zs+zpcfYd3dkpXykB7s5dZ9FNyEn86zvm+K/Wr8rLAJ1TD3xAtx3MB+J1mPIa+eeL4wAM1UmS20jOf77gGa+5hYrbbpwWv18ra/m9gxSU5VvCsfTpzICG9g3PWP7GGltHF5Zcps+jDyAvlzfEwPvG+ez/R+T0QZMyKDkz/vclzSz/sW5Yh8nheX9M9z7Z4Nb13+9U6ItN5hmK8/z5hfvckJ2P6ab77e7tKMmvVh2yKsU2Od2KyxweZ4/cMyket/3TlvH1qoEGTZlDL4zF4H410dLkaci+1Ldxeja2an0PeDSz7v5av41zsoH8lAJpAM2n5IBnzkIxlwjecGw3/WhuG+tNmEd8R+3mH4LMxw10fOfAnbLl9qcvpH2O6ln7dQTiCL+4hkuC9lN+FLJMd7MvwljOG+hG3ClzAvuD/+R7//0e//D/q59W48XefY/QsuvY3L7tujqSzVZA40dNWEhUtqZDh7f6by0XGl46U5d4tMGL26qk+fle3iss5NUjfhFZAHIPezCshV38Hi+fVOz9e7aD/df2SO0XjtjVmi38yEIEU5Ewq8no78fsOc2LAc52S8R4RYnXACzpkRpwt/LNdNIKvGj/H0tQteC1rPOw883gO+aoQ6Jp98QE55dhb1fGIKz/9Bm78gcYRYII7okATXUbLPumOrsO4VZ4dJxb1SkWQeJWnShJsT0bdC6D7fsfDZwOc9xuzzrt4/38DXrqBPcufM2/j+3ILL/07ur79bAv5qJMCGGY4YZsFPw2nmEAzU/Hedwqa72mQS3rwwjmua8qoCrk+qhDx6GcY4foqqfa3I135olHai2/Y2Ko6BbQUz7AFZdgOH93c++F07zviYF6zzz9L3vwcnqT6m+S5uE77EceY/3+Y/t9fvpvnvrtdwI5kIeFmFNVXNhJQVL9umPuf7fBwZgYs1s0XU547duW5pAvE5b75kN7WYFPP2kH6lT3p8jN48ysPdu3TMiY/95723z1Kb8Bdn2y5/1pwzf8LJ/s6c4VSJeuIzB0tHEmXTeydOaS+Glma9zzx9QLtizY/XC1R80OieGCLWfYpBljmHxClrcvd2xoSwwrKRp4i7BT/Fwx/B99XIcJ9Gzrp6Q9QP6dpfCxZP2qpqboOirwppXm0bfzYFl7G+IjXZxSDf8K0J70GNx3O/HzrLPLhn/scw2V9Dqg6jSq3D4GFsiuoKt2cn8DLH35p60evqndNe7forrXV1dPMd64zTI29fHMRwOX2Qkk2JYkhuXIh/X4Gzvm9I+vp+g88uf449P9LBH8TW31MPwKqf8LTPu+A/zRM/0sgm/I+O/1/S0Q1Vsaojx0ZtkQ9fT4LjqOZVCPzPce2ndVfj7t4TF/lFu+HriKGyhVXY7KeSPPQ7Q7AI7r2pcGjr0ahaukePdo/Xhkesez1czLdCFwwAotuntqS/v6P7IzGBdX/A5d/g23hfU6+eNeicxDqK6KVypgje7LHOlZ2+6UVGKnI2pnty99tVTQIdSXQ9arumed6GGlU/LyCLP8ud77+jx7xgixIfaLnEmR0+1UZskqpTQkcvakwZxv417FTNvis9vuPeqJLezpHknb3b1GAXBbtlO89xOg1kfRZluL8YkeH+bNm/yRU/IP5/5vy9ObdPqv1+Y+TIr9c2i2rigWY7wrItmfjJ9BDCvcTuRcIuLkp75MgYsISVgVxCwiYT9Swi+IP2LVpWQ7zJMJOQ3XVfrEQe3CYcP19UN1aQtArzOQq75N+wyOg9+WG3qNBkxKMh2IHMiRKS88tay5uVRnuyxvIR2z0dPAhM8raLzeCM0zCxSzFPg5PgSHtCvHURr6tl0tjcVZJ8IVP2JOqao+djgEj21uNWnVDuCL2icHw1B5W9BvM5OgRDqg+9k8Rcxn3TrgcrW/XZXoywgzi8DDz2WXbLOkn9PUiRkcQPbFiIx0yRvNshW0SXPiSxNP/EvVroX29crwe05S9EFvbme1uSwUZVRcJ5da8ASx8KGRbuWNl97t8/mxWADz70wzJQS16PfcMh25HZvX7etzzyuKeh3EOdnUp+R4kBHb6+5NIQKHGxO2hXMwviV3sQRnhDwv5Glx20yZIOzty1KRBxIziaeXxebkg1u4qrqbL45vD52E2zP1ErG43kHDuAA9w9agd1MQwmwP0WgzaN7eqJCPkmdduye70tcXnjeUlvOR92j3w252DcqTzC9WqoHfeQIxmrizhpF4ueObnr6lf+eM7ODFd5uCrs2Etv+EkQV+tYI+5l2h/seZRJyF76EcturEMju0bt3PJoSxGR0OlyF8OAt1zrmpQqYwoqEZuFvHtjwHYSDp0Xro/a9KU/YdTM97PDT1BdMFcjfpl1VBI2Kt3lxyqwBwtsRSM+UYjZW+BL1pi560kkkGaJVmU5rL+Op+u9ngFD2WVcer4+DOWzuOgEwOsRV/dHt+2MT61EjVwo1PUV0RGBk0sEQ/BK+6gvfPbmZBBL8urJhBak65XI0Qhpusoh4fkgg8NP+CO5JU+K5VyIZ2/bCo6WZARYlBp/OnKsvaGwIMflpkp55M4cRSpvju48q+IMHXGsXYQq/NGVbFUaxDiSRf0sNJUMgmdSrx0vX91MsSr+SQkJJUuxxCyE8UwB43zhl6kUnhbH4yKhKFzQWf4CfI+fWcY0Aob39VcPY8HV4mP1mvkseuO25vF5TiDpVMkuEpKNWiaBQ4J1IMnx2SG+4dKwVxKWxK5UYOXdxoh9/3wNSur1cN2SOYimcvVSYWFvy/i69rTR90+S1wf2npaKV3BurVK5a7o9Xp07n+XXzk+vsYsa7dNH7o4aohJH8gy6PhoxEXoovUM3P4DLDC/0UMonlt5h7Ijes7LQ2QFZQYWOS3bjA4TCUvdGPK73oeg65M1RrpNfpS6Aa9ozhArJIhSVsLlpF23wjas+kNyDGxxfkogTYW+zRrwzcO539HIilnsOQpTBbaCP7uWmO8TxRJ8F3XHKuXF6EY7tLIUuhGhE89D12lEya3dh0bsySqga+rJeP18pp/eDNw9Ti3l5cdgOn441ymPk5zsy7kDB2F3ltBxeBQhBbw2DcjG0EFlnC3urRBCYhHgtekrmeXLfz91X8mo4IfGdi+xMPNFb7iTX8fV07JG80dXKqQdkQnwRUowsBf31hXRVTnQUGTNRaFy0zNhZzPFNsbrX6IbZHaoCcKSc4/MdH5peHnqOpYnA5Hry4LLV93BhuNYxPk8QdCEq3zdWz5fu+WG46AkX9wIW8A0tSJDnVS+CoaoFFcy/1zMc50oU0Z5SJVEhwSb37cauxrDrM2tJiSVHYzNdrec8P0N2SilWHE/N0wDKM3l6QyNm07THrEfHTL4gaiLUOOwmx+zGpgBbqzCjjqjwufleyzXXkSucm8oXtzfIaniCf2oI2+tiLNxm/F1C2LDa87g8nhCx8R1ipPCAjZhZUrE2SG/8sgG/JcOlt8K72asUJzTonhfaBg8itFIzQaAYHmGo4N0V2kdyGh1jQUJ9VjJRx8Z26vOwuTuHwoDv95WFdbPy9cfD6XoSITUTRzGlgEVojqxK270Q+n9pO49tB5nmXF8QA3IaEkQSSWSYkXPOXP1h//byOSP7s5fPRHuwtSTRXVXv89JNtUjmMbaPau2LaO3HePz5FkqjJr/rtgblKXkF8I7rPvhSCy6Viq/jhA5EzJicyG0X7BvPBr4fzSMjwhn/YgfZifdiV5wBolFGtAYstQIwxZJPj5dtu4jIwmlO4SEDEZta0wLBWL2Xwu9CZgg2tc0gpZ9I0hXsOIqPvsW/M5docTjQPqxzX8SrOhZMeng4szdDTJYUmklfnZ8zF1bPDfOt1CKqt4YiNWaYHVWjTBsViYJw+7Oql/5bR3JXfP8bvFdZXGqLhfklcmb4rzsDLErPimQmwVt2gHMRhc/nowZuSt67fbVZ/xsG8lBwRdEzx7K1HRcDJZDf/1g/ZvPeSDZuKHEi2oI5zEY3WUBtW4cYcLZzhr3t1HMoPRbRCrXkzShcEnXtRkORmm7RW1KU0yF4PT2o3xi9SvxWeE7X0zZOzFcaUbHYV8pOMNhc2mo3gC/zPS6B+gU6in0TR6b1Y/m+0WF/r00cvKKnrlNkudWekKQpMVD7eFBRk3Znll96W1QaVkPk7Nde8g37k2C3ebk8ES0c5QXwVoCf48Sntb3BaJPrXfePPP98ZaF5yCb3L+KTb1m0CERJSShN6yL05c62z5ScK6m1rY40IxjVqIm6X2NhAqar4YJuk1ImL7MfYsIzHbzQkUSkc7JYXrbsp44zRmSJEvC1+qXLf5jgOwcoQ/0F1nBTKJ1QtTALYAIDHutZ4A3bTa2VdsZqKAcWMjLog6ZGjaEQktPBmn3lbQzp1ikSAVJQ5IblCkdJFvKVIHpEGsQvYychoon61RsmnCysSvsj7ZepmGm/c1QYOH5wTRQvPonwpFegCRALSmz5YYDxVcocWyCbPpbASb4+dSS1gW6j90bFrW+eRfY7S31jK88VU9JhDIBttN3I9AdGurhN3neX8/ReeRraWowltSKEmEQA5cwGlJc/Y1A/pq4EJu2IpiC5x9siv0px6ZlaxFkUv/KiAlC1j78zufGWX9qMkpojUH1MM6/u+PO/NxULHFVAdnS+rrR42eXvtuZbLy1rEqajrg1dsD6Uhdbz3DCM+DDMEgLylEOUcscwY9LfVwoNsDbXpjAOKH7FCoPbgtPh/KS8xH8t62xTJvtq4b0tWHsvPKUfHysBvlfbMLQfJj/zeSGFyI3XwLXtD7dO0QVG3Zi3CeL9vfe7HUXH0f2B7pf1+v9J/Xf8ZANF9DKkktIKkodqITm8D4vtkVNel07MWlUbAMdDQt7BeIf9tmcsroaK/I6uos33RXodErBbxB0zBKAKmQZ5XiygNe9XVx7DlSHDYH/fdOqZdlwJVcRFvIGXCnflgPDk/LyZJtP3LwXJLeuwgbL/mh/ZyC69Tobq7m6ILuoO3rkUGByYSL5fAyx5vddywiMNZ8RK3DE+w+9cgpHTfvH7V5qp2oK1g1w/Vses1phjEuvhOeiqYnBCjSLDkS3LvI/EVgG5vvgs9mbuBYMc6Ep3yY26Rv/U39jWtLuQHWoyHerrYClzEcnSrX/7t9UiSGCJviMdM7H5NDE4faefA/niku0fC0YQ9tev7YQY//jrpXfLQ0WCq+ij4DjFTxZ1Rgt+ghPMJmrN5+uJsxRoBE9bLWCU29dQElDBKnzOgPzEshLd+6gMErjqe9gdkBY2UoI/g8RJjBQoBYnEyg4gyaBR8uv8j0OAE4ktyYEtUmOismPulwRNG3xX81evtXMGWNjtrAFWoGNjSfHW3vzG1HwStQ8A+pvk6XYNDQ6eEPk3t0XFfgBLDMIv2w4MTnqPV1FOwSNTnGbW5sK+fbqtnilTS15Ls7Tz9Me0OeWmrWMZ9SbepG9V7SevpMmPpqEafpkwtAN0WAt3960ovVUgkxGZ1srMjKdNgg/H/IiDMostuupovKecrayltHPGaK55fEBMlA7V8os2C9zEwNL1HHh2ANpiq3fMZb/6/s4FkIdgKiGC36blb/4m1WJr9wfYSTGG53eoVtLVGSjx8HHa/nogHFG4y+ieRIr4unyfvfzmVbrnDhsWl7oJ99EIz6Ek27XwIsJFW0EOrUiHhIKuG40OgSENG6oVg9KIADW23+JXZo0tY6H8hg0MNmBof6pjacwh0NLvIgO505XaYBbwuNIMVDjbb1eB+33XRt64okGU+52p3mlxWXReo/b+HLblzwXLxRlEZp2HSADSEZB06N55dJQms8YoisVFxhcjBDh+a75uQ/V5KCNIB0qK3LY2QCrTeiPkvmP4puRvmQEb2HUBInyyYTjn9mdd3o1elG/NBQiIt0jUk3Vn39QVJwFUlJfeFvzWkeTzep0O/G8fZb7TmhSuSf5KyHHCBTWKjfPb+I7efMF43Y6r0W+JTZVe8uuRDlZFTVFUbtFGsvw0srT51Xt5bgsdDp7p4ErY5xas2f6QGvTt1YRI0vAiw3k031HHMzNxrnfQ6zva8K2KLsD36XdmIpr4m9sv3YR+3vszslXx7y2pBxqO8iWWQMlc3v/iPpglgKuIGGIzDuiFTxESzOvup00gA0cvT9SWdSgyMYmlqj0S4/KzHTPajbzabcXBJ7ukcISoAJi7XuliE2Qk+1ICSvNmuPSH2tuCRgNqIjFzf6AEhFR5QQX3D6idILLtEF3HA5RcUkwcGdo1uAu/pqn1hCd6Shbiiyz8fKrODMsBPfN3VIwNWYgAqzfzQ32S47MqK/2z2Bb4+agqsyiiDyZwk9JWb4h/r+SMb8vto1U3bPT1+gFXGOnztCcykiTB0yM4JGI1RjesMmcBV3Bv45zyU0ZRb0YeoyhAKSnM4ojZiH4+9M2+pWkpYi+Mexnc4IFNAdd2KZhBUPlMX5A8C395leU02P+7I9cj72vLfiVVUxs2YduZRPIYstXNaTRnXPMVSEFQP65u73QGSVam7GMWr2oWCTGRy9Z2GIIc+QUr85+kQnApLvlRpEzCIHxim7Le227nw3PH/aHTNLsZh2i9DvhikZ42zvkwEoL24jFZUL2toKaB6HC6DM1dYLV84d4n/3pD1st5sVOHZ4856zeko5GD8s8M/X7d9EbAN+EbcmdS7AqcAFj0qZ/91HvEcV2uipUMnr5THJnRFf1JsGhi1Y+d5nkcvZ/g8gwjMYL6z7cwW4G+vM5zU5LA/aTUZyYjefL470lz/Pm6ZQPb5cRG2l3tPTmUO/W2bU3wPMlth7+NR/tbYcZsp5xlgpz+Q47W15HlhhhFhQOreqtXEaYsh5hYeoIGW+2xjdMu9POozHrSLl9oFvEpGbq2vMYyP96wTPs4tt1GInf7ydDYmVmFeUvHwMqyjLPvKyGWXIe1q4ItoFN8vO5Dwc80ZdEYXt0bbScrj1JW/pYyu3uDjiH6/cw9LkS1R7aFeDrullcobK1dKzH5gnSPtuFoSVzVdFRbjxaAuDF3wMOxk9FbJ3mZkdKWLyMZftOxhqgvIGXm7u6J+svUF8QM9+8RKyhyyA8vz9VC72j9mmTH+nhNiFBUhs4qBnhlhV4ngEfl8sm6YCDPZZpvOoCjTcyRXXlL/mDK8oSv1KldL5YTU4spZMQS0cnE3f9ob7r3mnMZAj1sxOjfJi/lG+CN2uAjQnum55D1srl6AM8nZEDCUzyAv+wyBnn2c+95LY+3HwKlwHB/j2VHE3z+9RJMY1RVtzhbj69Hc2SUSFu/cauYBJlVbuLhU0YwbbCIt76vPVURhOJZBExibkO9qfbmrlsC7SisOkuV1NAuoNetQjIotZIp8i83XsKGUCagkjXJrZkOj1ftzoasTtbGmqMicMsAUCN4eAvRjEZD7MQx5y2iEwQfL4H+Oiw3L1EGFpJ4hemle6JXWbbED+5AP50Lnxi5CWDZLdcc+Rm203gjM8bZmLfxLzD6OAgRK2WMJEZtMzWLmO3XxiGzLaa8Aytbr4r8lPf7oapetxeK/uuSkXEedkEhtC8Rr1vmx9LFZN9DhhuNiroV98zPvTT5REuBbBiYYUEGw7v/XES+/gpSjMzDPLgg7d96gxM0w9lRGSCBvHS+1Tdxk/1v5Vcb620cpbbtw+2cGZ5VbgPMD//4wGAcJ1gMi9iR9F+0Fg5Zp5bq52wGo9F2YR/SUW2j2OTGUe4Fv13pWQPVrkThvTA38uCARx/BW4CIsn2hLaCkg8+Xt6bH5CzOfhLn+4xOu5pfIXVODyyFQ8o2CzXOdJ6beDK8yAJLlrJJmtBD8yTkgY/4BHXISczS0Kes7BSnGd7/6zt9XHBaxMrBHjXgJa9sTSQnSF3BHB+wiuzi0BtpP2VzaQaQCerkV7D+8AA8+NeTxp9elLY50Y7h6EqvFQ+1iWtfjJRmLSQxxiBWsNbDuMFMaPFJA8ltHLZDf1vC7hQ1ACtn/zpnTFFDgjeM33c9lqJWnePzesukIDwpQ4yRveh1dQthJhpD0fCbRxI8B4W0wagO5r4XpQNaIllTu9IyzE7oCjfI3SMhI1otWJmswd9//csHySck6kcLlOf8TkAK+pc/yme4g8ZGnD2IqsYMv1o7IPePsQARFPyRgF+X5MZC+vNXd290rk2p11AreL3+7MkU9RF7CAoWsIB6ze4CN1gT0frW5LFZ3Ih9WY/g9d/M2WVDo+gZHyw6hchzoEJTXP3d6R9jKiIl0fbm41SuLAeM8Yef/CCbk13wgbfYcHKBSr7DPb/oT6olnV1aosF2zckPjZhCTHxUzfVgM+UT1uQvRIIlm7lq9EllUKQe8S2LMqq0hcYw2jA7p0DCQgiYh9Af+rKKNSCqOvnYCYasbQqTlc8N7ImwXf1zEEaaU64HmcNVNZC7n0qAeXxbKXxwHrbX8vC6T5JnWXw+KCVzV7W5FIyoqyPJLXxLur72IsaRKzWMCve1Jj8W2mDsdZGw5Ca8TS62rOhmajl23KSJlwex+P2OZ8rLbAJjZyqu+wUoVD+B/OAen22YUifou5XK8OhISc+dtXbGXLIqy6bMrqHec2gINoViAR0UxDZXMWEjC1f/tfD3NRNLurf1HSpByuSDtLUS8hy5SkeremB+rrx+dIPWzRg2v94UJQSVGA/n/FcxBn8KtzawhhYqzN/tD3moTcfkxD5X8Y1fC1Lo6g3d2xf+6ZFpudceMkiVbidn25yG4SeIVfG2tFIjnUEUJylC+DqddXWTHGgzMtYhgxUQSgz51xb6DWndXRUTiTKztqkytu17Db2fzbR7VatLA4t8m82wKe0jkr7Ogy+CYA9B8lvtyKdDlqgJ0rr8rIZWfItbf+cRPnAQyU3SwYuBtA6K+T7bQ0b45W4/P0PZAmkzODGCshXdIrb2N4fpBjHTtJ7+f21bXx7zIvN3LA/JLwEmHCL1i7soMidOUzCV4mvAI9g8bQVTl8cEGG9YBmhGD6jqIkM1uYoQkGMy4CMtGnnDy3zckvKmY/w9JdRa1/RGQBp/25j5zrPG/oy/r/ySu9iCsbmGy2+g4dFR86Qgd7oeTazjAXnZtuDN+WjSV0k796X/FYQZ+wd5xyld1k0LBqMxyQtHcbmQoM7yuOBnP0h+AVZD7C1+95FaQuXOLBjmLsIsjWTWnJY3futuadfhSziRT0+caU3pfUtEy2lF5TVXgu1mVd+HuiV6Un/Pysb2DdrPKijC0SnxEAsxwyjp/NuX3yTBhlyTI0PcR/JLNg/4ObpNtM0xfL1dPIOfqSCt4kSxzMtQeiaWTwRfNMFJ2gLefslb8HJlWayMRIymsT8TYTS2ch4uKatIHHuoCMbepwFtgxSw4pfYdxaQkyUjUMyBaiKTM70/+iPZc6uckyTLiNHOzV6RXr90x6R/7pG6X/WuhGIZOyqnHwebgmqbNQ0rk1Kz/dhQnSVOqHZZeQ29ZcNtYFaLox6XGeVyNAbG5LvOee4KJ3DbYHiENL/s1MXZqTiJXTOl3fGrQrBrWiw/tjwBRaE92AA3nV+xpVcd3FReoH6L1NWHvFI73GpbbdKcbJvlafpgn0PdqfUQhhpAoVcid9qCWXhLgudjTTLmfdK46w0oJdfBXOC5wTTYMM+LEh+xip1gxJYNV62i3/m0GhRUgTHsLn7RMoLVGCbBWU15h/S3LyKrrCgRVKRT397kRFvtBySxPx4SdGxNfBknybNQk6CDCDUjBr04be4rS2kINV79O1yG37NLMfVMxaDVJG1KiK+EISNzXW5LM2Zb1m/NTLV5oL5ofL8UgnTV334HsjU+1TpNXDFW+iv3/D4N/sEbHTH+GBiPLWdlEVXmLCBanIV//axDZJgIiU/nLio/UZwxkfoqFvDj4/8UivMQ+Cbd8SKnAx2nazuT/rfH7u8syCIOy+bGXjC+XUUqMtIjsregX0lByRsVT1txPOsfLLCJGzipwhvTj5WScQzV04GVv+7t/t9RjhqTzdQJ3djr8cfHdze7+NaQr6uL9LuT9ovT9FI8ywvBlOf7LQLc6N+6XjW79fu38YX5R61PtrM0XHhvRPgBxlAyalYEetAOnL/4VMB58qaQrdAw/2xNAEaB8jIhzLiMQalGZlNBrupeMdzJbjpySrv2Vw9Cnji+f2eEwZ6CdVTMn9jVxzcFPNU+qj6WBWbP4mnIyYKBVb5Xa42cvLWHIXoIHq4utAsC1uMYv8D1BYh1An9g5wVXedAzYBYsUs172O6R7u0aP+HfsFFATDwlzGXifQK+6mNNtPwqntbUW+YcIykw27V3Tf6TQj9INqL5689O4HrSz3HBz2/t7IeAYd+fF1TpJ8LCjwRkqYNtu4yDtYOlW4IfXwh0kjzxfDVTpp8liXWzn2p0Gk7roZp+YhBh4Id8JbtsKVpPpvLTqLxZSH8tEQTqb9MmPJz8CJPpX+9dFncXaX6udrzAz+wENnxL51HmPJYQWiR8+1gnWwZo1+Yt/6KQFf/9Z4c9vMLEXaQSwNhSykMD8n6gO9t0qNeoRWQfRWbBPBTyfBflBQIO7aiKz1HSW0jJxEggNCoVr5cZoQqpiTJVK0gBKOsdPeJvPblvS14x2KAzzwyW7oMNasctucxsYJ8cts4EGf7hZlTaYFDSE7AG/WTxl29xwpibJY37otffoQNGmIgapT278Wo+5hY6el1+F+cfMrGKilaxCIKkUP3arG+yGeLiIYArysgklClLDcUSTup89BSFTITIAPJ2pEdUQJK4AMGzMLxNLjaK/b7rQ6JLzBRRLRtX5YVhJ22pBNnAJAXDTqcUacoku+TA8lyaPPKNDPGkRY95a8IZXwd/g8hEeeAHmJMCNXofEjufDDFc6m+m+HUv2W1IUBsZKFBDtYObPC6jzxwYLiuNXDnF+h2+8++XFq601bN0KS1w5uKrjj+sh2pgIzcHXIHwkxa/kazX5gSzzfw7pwGZ7ata4OVW7cRHotVoPyFv2wU5od1Kcz8WFWL0soMHZAQy+mo3Cm1HbH2NRaNutaEdwGOk8L/1ZHqg2blBy4k70eYb1zQc49uzDYGzpPfCFdaAuS849iU/EbFr9V7v2TeYKZK89At/kWMHLpT4quLNe9PvKSK1h9o687huu1Re1PiZi4m8ZKZ5nHnuK4gpxlzO/H0LpBB14czdNGfd35RhIfHfYWneR6Jip6xS84DpNdfueqoXcP8C95krXje/arXBSJ7P/D0ae8aZ72EJ9UPMevklhZ/p1Il0Z/CJuiC+kN1VLtrT441hncBSBGQfo+LX6MYiv1QgRARvTquOSumOjKiNrGZ7CeeyQWpejgxM/L9AtzBwczWTXxHPtHdBuR+bPz4qm5BJtlVhy5M4EzTyU7Z/bVzB/6WuAJ4Hq6882SrnAzCHN9viy1Fu/zCGZsmWZtg7ZlRPvmei811t+lcT23rV9RTmsO3YvAnLD6m8di+aPlXXSi8d5uaIu8D+3Qhp2Sqsgii0q+TLi8S6a1p3ptya0po5bxh6fj97meguJI+1m95LPY7cEz8PnXKLELn7ogHHRVjRtShyGw47zhVd2Ri/kUbCEpNCpqXLeKjJy5OwHzS+zidOcrQ7Dk5Lghtso56bOTGz024tvvwfJ+jpC7qSt6jXq8iqjepz8ws+HJgPGyKrTL58T56HgbC4vIP5AjB9GFrZps1fb/sTl5x36Hjfkq7D88/0bzaAzfgPFhdrncFTecyqcmta6A1KdbPeH8to7vbXIsQEZJIrAjzUP9TUQTjix2BvMrnu37JH25f05HUJolXw8x5kzr8Q9Fzm1JfHlBEUMglXEnC9efdfdU/Mh7zKrOhef88Y1bg8xfWyfF/lzEjn/U7W/GU0V0EY3Q1Bb5k2BKq1ikyWTRFiGtjgHGRf486c/L5xixXtfz/QzMZDnaQG7wOA8og2O/lmYOS/bhT31zdyQRkHD0Pdd/bSqsx/v/ifqAli60qINK0VWFXVhREr7d/MejOt3BIRWr0v0i1sMqPKxw1/EjUPC7uBHyKeFHmvkHQfX+12Y9Lrmo0+iSowVdD7WSQ+Af3rM3P6hQtr6dF+zbyqoTAEQni89dM8bL62GJjS/ILGyFTHO/QO+IH8w3dVezI/2zR/vkX+ZesQ/xn4bz3lN8gNw2PMX+KhycpnjZim2GLCi9dC+e271+uwHfvNSZ8YeMfeaPiXo+kbhFwO209AiT3ySYElyJaYYoM2/1kXSaWl0NCv0Pm9JUZ+5Cku7rbZO/AKNmzL4SVyrMSL3GYU18EZX/7O4iolAHgtA7+1w/bEkPNG61AbHD5AMy5cfBLwPtRUNovXfwsSRINXj20TKzwtkDG8LmQScgYM8VcXMqsbEVo8uZP+iUSS//h3FpptlDHZ5Sxr/lgC8R3Ft34R/w33uQgDZSMvm8Xt3hFF8sRN8cMoWi2SdaNoU8tqwQVJU80ol0k8IWQoZ4qJIyaCmR6UiaIxe1u+T22LWal1zEnozcAzERktfwUpSexM31WAahGf4gRUFh2aMXDzx+/O7/VkHLGSPt18MjT5PSM/JN8yjIBq5qA9gWkFgFGv9na7aoXpc3cX6mltlSEaQ5URieFzhfmUcwLfRSRekvwSG5sEaTfewV/bPkfEmvBhGkV+hjVm9PkrGpwSjq+BaILy+15+Uf6r+w/7ILX/AUpiyuO0VfUpe770axnUW5ZCcpJ7KRTqwKXNXsZOjWYE71Un0WkxyYXecsD/avbGvB8ucojh+C+RfIAtVxaLrJWR1qEJNJziosfd3gVZomHmwsm1oIdhjO8ejqbjw0D90/i/06p9qgup34xkdQNn6wcjmch0I0b3/226FrWDX1MSjJm5Y7D9pVjgekpKIxrV2gi9FCqi1CGEuuOncIhoIoNVP+ixKQ0g14ujl98syRd+aY6ymd8LH11PStCPjiZWoB/yun5uzjYsp30F4s3McLb/sxgyJEDd99cgkFhMTJnvNhaqjTb6agP2TRBGYUkpGVg63gvesZbiB1bo2B3JVTlqfKHaJXmTIvAvWYCv+DLfgSpykcfO18k1463NziOLZbSXrVt1D/lpWLqO8eJkMWLNdIh7KB344h0U+JteCyvlAdZnnVVqXpuNWiJG+oHT87KQR4/ZVMTPlM/7jpAQjvPii3geFIkugMom7HpNnGEdLF26MjuANJJ6Yly7uHm7uq7F9pnBHKmKfdTKZE8pKt+H6J1f3ltwtbAlcOPU9gm8PYM+P4bIIxrulVOf2zZz6p0DRA8WMEVcgQ/40S8pchCaVFJk3W7MUnt2LbRGL/WyxGkNuEaGkGxTWvs1l7Z4lzfNN/1MiDASdfCkiZm7HPCAG75DnEpSQsWv97YKQXgoMyxEfRqNAQxDKPaxutv7TzNL3+E+mR/DGIuknEVHRBWUQz7S/N2uQ/DQ34d45bYRupB36g29o95swRO8CKj4or+wcTp0h3jiITPYOo3c1IqWHEnnbrp/JzWGMY7RB63vBwc/CbLN2Qemx9YnOjRZsokRXY1JJdmUZEKClpEEXfrXUF6cTN4gJSZv8oqZbsqOXnLlRqLB7yrDm6PzNEJa63xJxSCTOJu+RSGeVNl3exhC49ddxmBZtCDD2syphf+aAmIHxfg0H1z6DCn1sQDiV5ZRL8RKCyKTif+CUJh3jNA4x2cfAWXS7eKkpWiJEP2E5+3KuoYeu4BEIJWG2ZbPnmFo4AjLqeACvpsA96IPxTfsOH67LQQUbWi3M7mX7NOZSoieXpS2zOpp6xGlAK+DNe5hIxEb36KkO+tA40MgEhMmYdmHiOjGBasesZ/DlIoaILPEcTKvZbZgMLvaNnRR6lO6VwuVAJsfa1NMfGS5BCIZ6fiqYYi+fdd71Ej6wM8K/lpmQkLxZTQ/UjO42RcQtgF7NtKAWxXZjtTx1+pvq9QqCmH+BrMR4+rpsvNB6jo8wTWBCkTXvajySdiBF22r5P6OnxPNQI/2A/hmCvzWrEiXISBbanAjxde9yyGGKGWkyGeGX1XunWYgXz8WRi8eHoJwC6jPGRcmy8dE9QpU2yRZh7I8o1kgexvs7dBfxBSGhvcKbqDLSZ22JS2QrrB8tyWczmuiB/k1CsaSTb+HgCyAyXWC/BEoYmw6SeFifT+0VvcJZUllGlm6h4VwC2EJaOVFqzPTheIU+OUdgEh9XZpNlpEmDRG4Zfzwtwl/8Y/PtR4fcXjzzaxsDPYBa/opInKdvwVuC0nBMUZy6Que60Kg8ZeqxIsyAFmNWWu214QDttp6w5sOEW/tfCLEZcMKzPxrAxUoFD9TbKRH54a1mWg4HbPbB+xHJj8py/nBMJ3xCnzlN3eDpjmbV+agDOYnD0oAag8l/jpnfbK4E0M1i0aXFxFe/kiCa8Fq2LyKBazmG7LPvwcswnqFNuSq3/cmvexk0BdzVQKXZxyzqQQVt1hd0r8OgLgxxcobEDSaINMLxbEWPRJqpB8mH5fB5IyZKBQqNSXLp7dbCZfgvhW2otsfj9GptBBen/PHx0bsQuBAqBsThKoPnJyRytrQvkxWjTH8wJzgt5r1JJXFy1RUqG1+8pHIWCEkqtSybD+X/BzxHdmrLyxsGhlZ6L+7tCDF4OKAFe/fMAmALpWwEQbtLAcEf4gHeuth04P3JE5ctcKQhPxe5rNAyLXsBjP/NXfOncD7Ow+CnYTn72kjiPJHBH+D1lPuK1Vr84ulOPEt3NeS8vBe9yNxy8t4RiLzXaebM12X+N+4lc+hfzdbQNGgNvwYoOSc5052aMSmCkMKM3oXGZrbczofYHzAokoqSazPv2ryWif9WNY72Kr0Lr7t8tcZFJWanEM5OuV1Rpq6PCz4GosxQDGvT44qwFnTlcsD0vOTvNssZBzk9hfugK+4jYpuUOafYwa1HeHQWo5COze1zrQ+UjAsDTOhpfOGH6ZR+jSBPRirt/eYeaAQj6PTGOiMLMmV3EaZkg5O/hRj7yfNcZfauOla1x6GMFD6WjF4PFY5V5Af+Sf7dO3f0TrUO9/rdY711qpDRjCrB36X4TxdBCzPOP07m3PhgJ303bxRDRxsf8W1yB2UI1QICoMv7aDxEStEExaGchfZHVqlU2cLZRA6baa34EAoyFlEmeXQ/gS/ZER0EJ8EuAR3z0qriW24pClZ+9Xu8hMuI4G94QUo+g9OTnzS/EnbrnnSCHTCvPG1thkTuHxV/8brGvAG0bPtx9T7UnY+z3wa2cgteDHgxJ3Uz+/yITfdHn3/1v5vrMcZjxCZNdjTGLyKjmvKzMVNo1Rfg4fJVa/fE/AoX+Ecrm6NqQDl8gWUfDymV4zgPmftpPd5UuUE6RFwW74a9EaK87U9EHotOYFihlQ7Di15qusvzrqhqaitHqt++49hndZumFO9N5DjhVkKGSfsoxZUHWqmQB4wVyVfAqCeJhi0RQcPMpqM0dyqfaHWO6ag/ztnON7fqouYYajgtBO8SYqWYr/EiBaKAxL6yBYxiPWt5RHlEfY+ju1vxy0sZu4bBGqLiU6j0F9/bYKD919f564tbCi4sC6TukuJ7F7ynn4wonlVG2JCfwRkUIEJ/pcZW5URg/JJassvIumdmut4GZgayi9jF2EKP+Lw/qC9EL94u5OhBVIyPZXVVwbowhySbHcHiW3c7GekoN8DYlBCxUVi2PEq2KfaJ7JmTiBOBTJVqP7MfFwJGLKR/B7kUZfX531D3VYtkGruV5Ttq5SM1BitnLCn8+sWRJM+kPEF8iLX1cHFVSA4BVNNrwSGwCdztkPd1rI0N15Sh71wyhB/CGd7pK3go7+1yfhvsfeWTrz/EmxOuD7KW7S9Z+ZvAIe+wRsJk++G+hLAbgBSTZLBxRdfE/XIZUPVD0rjXmjhEnb1+/UPKhGFAV9/DrInIwTsCwERFm3FznWPKqZat+6mdpHtBywnKT8sgkKp17jsfW6C3NHoI9pCpPOor9R4Ntoq8vJVbBbkuOvOUYGRRr2LfGTwNaGLzolPfrKMjZDIzGXICHkCaDhiXgvF+vRoq9NnH/oLfK4oTQH0PoIUBevkRbfV2/EEEKBWIjtZ/DQvespwoKQDJMb4cFZYcpDDZRaAfnw0s42b/I2kLmrL/tkqiC/IiBgkR6FthgmS6ilAoYe2AK+gww72v23MH7hvqVUiqk9aULyepZ63Jm8S4C876mn2mbajUVLy2NG9utbKTC9HEyXc/r4VaE5IV1Ud9W/REO2O+BdxQGR+1/gF9RYbGSzB7Ld2SQ02gyd3dUeBT/5G2V5unnoRV+yVEnFIAuxBLpchge4rodx7WZhFkJNqAfLixkNcxMlwsdnz+/XRj5r17bvO+H0a/G3af5jqou3fSvbJ2NAL6Rzzdwzaxy1q/50E5+coqU+5VHS3NQsDlWi53c9/LV+7qjEC4tilSsCPmfN/qF4Yio35dwI5s8yJIzHBBhyyxIewp7YoCnRfcCNLbwZ4kF+DqjifOI6Hx6kKbZ/qdNM1VN3foBpgMlyGo8ZHgV6KSkR7iGCXGoduoD6uZNQ2bY74L4sa4tMgV+D4aD38yMWCYU93N2+9d+y27Xk/QdRnTZ4ff3kKMalFpWQ1HizQWwhuIoDjLQWpbAClsLssgw/64q/wvMRhA59NSEA30Aka1UzyRqgo+xVJ7pqY1/1iK8uNQhI6rIE5tN1oBmoUaEeH30g/3zQaIueeGwYpqYF56dymnDk06zeJDh+rqJvaODTUFU50iwa3/nU6CznXsYIVsOQpWpIYSDvIZhFPhnldvkQZmWnUID/Yn93n7iMBGXHGsqfBcS4SVQlBi8kzsaV+Dclrx9RO3qyD9J+hJ7ivjnuo5iCSqAG01PxQ5Cg4MsBooUxSyrn8sTYc/TMx4cwiIXtq0a2FBMTEXof1x5ZAQODSH+g23nokAs8dfPA6JhDarkm+sq3TvwaIwacbeA2HWc785UcKGzrXDMWsdmu4AVmVBScZtZvB0fRvbMIItB4VGBMFFrfNoVv9KmybsYUib+4I+3t4QPCI1Sl5kX8Ck+gwKznRETwpiMiDR1ehwWWV4n61hrtbocsOq9AU0YHQP67x4w7bD5qvodnxdIEbPHX/PlVd7j/yum2pNLufIfCf9tXhi4QhPj4mIt6kyQnS34F/tNrHzUcb37owBXzcvAmFjYBqVqiz6KC56cr2ylrfBr2OmORsxnl12qiMPReim4UfKkUj5L8ZctCsm6BsSH62zf6CLnki8SG/cnVyec7CJ5NXf+tzWIqi1Zz4kxHRf73WyRQkzV74SBAnly9/VVBfuGp9q2+mv7WU6bLfvIu8LMoewKVJVWJ3Gi09WIVTCUhbFZmXHt4O0s7DvFiPMUkE89o/BgCpVDzhx1lU4AmGEhZzbEu8MdLuTeHMXvk1TJCKYoaKjHPm6gByduAt2AyRv81ewxDXrbLvynbDH1nQcmhdmPmI1hKsSLj5Hm+aiuOQ9Ohfp0++IVf6RAqKgdh4fulHpXw6OJTwhJDupbBqgtcn1KgWJa9BKpiT8a9TCOnXpm7gpwsa6u9Uml8aGH1vgGUCEJRTgTATFr9+H8v+dcQYjgFu+zMOSH4CDfzMyV1Ux4IxSvx6bJZ5HYrDfc1L5fZmTd4kD/9O+H6u2jepkn1VFD2pTvMH4hiBooUGb6mhSpo+tvIO/d9WoyoIvQstlULfCyGjWKp7lhVrkvW+5jYvAKCN/wsRnY1X+3eBe87rLiWZn8ULyH7xNOvHJHZBPgWJkMRQleduGRnwz075Pf/if1v1ihr0Cp4KavGBHEdJfKymAO4E4H07oQgPcHpyUutpRAOJGQP1fvPfw0pNyfCTbYptyWRCyOBMGOHLl1KJS3ltXuNsXldSb3hX/G2l4I4QQl78ynr7K+5LExXCPp2gwEHU9/GFuZtSP0HtgHM0cmVshtNHbpCJQqdjSA+o3/0DFJU/0QmvxFfsP90dKZA68eIg2qOsftaUiaTfsJWlZLkl4/7tMMiY733w3bV01xSZtXhviQ/6HvRF1h7QYRbwrqGNaP4Bv1tcsss1yW9x9BnjJ4p5nAUysWD0un0BOW9zQfFEAKyyvlew5o01PFpeHuVOFF8IGIYZ1rHEGj/rRkElhhjg+YscSUzFmI/0totQU9Uhg3G9wjjRpygIS47AJI4icaRY/BfSqh/O7cYMuhE+vnkntO5MVJ3palo9/GtlQ7c+TRDX3xmoX/8Dv/KQJGRBlxRubkGXp2FZ9Fx07eaoUUVZ2CWX12sN7XpaXOr8EeAQgfJGkcTExu1Aa5u2MMRgZd5rUZ7rhVuN6mWrM1UnzO0dCnXz3yNsJI4Znsa+vwMYezjdFryPiONSiHmClNU2w5IQE8zfBXFQtxvbjhQk3kCV2tpOB2LNFJgIL/yPYB1VhH4L8DolN9XbIloFKdUMjOuZv/fFhQ2bfqsp2UIIZ+SALzTJZW+bh2vFl8vPx449pXhiJ7bjiRUSlLS+Sf1VtnsEjB+PigXVlxxuWS57JGFcbmXUAvD2iSphwLdSZhMikiWQmBnl31dBdweQj7r/6+t0VF8M3q91iYkP69pc7LUI2XNlyUjZjcr3Fmf0ksSBEAcbLpYQEcorRAzshoYYGxYoZ7MD8MEUTPmW4vOfPUzwIJQOc2FSi0as7LP9FVmQgZUXuIUE+Vv85r5IWVeNRH1i1gJ+8+sjKOYj1bVpqwgEzc/2FDFcRMDjx9vGuFmkiGdW8/Bp5yWrm+0KvrTofPsF+R2Mtgh0Y770jDoUAiJoJC1/27ckPnQIHZv2pEeq2HE2Pwp2wWnPgHsRHgP3fXQ/f9vqfm0zvqz8pvpnUGTJX6WzwMaa8ogvjMh8EniHQS8ZciVpGLVvWJlpxvGE2Nj0b4CfQEWwqoteLQW8xda9lz+DOkxtzQEzF//xACUqhFoyqGd5quAxqvR+iQsRf3VtEj1fFBn5Gdq/xUxOm/fJ6/ivVZaNrP+lOXOyEDPF/14I2eXvBCJSnqJi/4txu+O+YlSOvA7LkjLF1nZZZcX8PQN4M8X0Y7aof104EaehrtZqAJ/Ft25lwOZlvE0QN8XIgSFxSQNE2qXeKrQE16fdyOi6iWJWka5LEXiHYoMKRWxvDMRTDOjvDA9+YCUnl+jQCTKxOrpa0yrVek4t7UVP/O4V0DAN+rKPTZiTt415g/S9Xyav+GrW7Bs2YkzKC6eKucfHP1pfl/z6HRyii94P3K7ZGAPOp6dkDLquGEdwoQ2a4RBg8PVJhrot8imtARmpb2rPLubfd4UfvwuQPZfJKKGFwKLNTiTQpqupG29/tJQoyIUTZKrlvW285I7WNgU9V5cAop7EorcQAr6ZR9giebICoEhEhsTgN182K/9jC8oigvSNBXGJ87JwN62N7a+2bR8o4RPl40AiFsoKxn/ferGedp2jTc8ZZetqf9XwvI2KJSRkkSgrXsxYD/9WjL77FHdyOIpRdlaWtbedTJIW9NalkYSLoX/LyFl/9CVDSXV74X2t2qQhL3UdyXQw8/KFU5NY3jQgl6yL70vrO+QzB+mynWod5SHEkPN04qYjuymo1+aZYFNxBfELy3SdaIg/3ojZ9T/DwcNAeatr2tIa6MLq3I8OIb0faurfh4jmz4ys9fbQQ29+EuY1CsZHBg/B1qJEZo//xuaWv/O/EokM6AW4fBAZFhzt1ANkDGCNZvhv21QLyLQIbo8tvuBi8Zllv7ZrvcTi2//INkFFq2x2zvI3P1v2/QXjFKoT98VMakbqCxL9rDkFN0MUjHmlCAKw7qAYJ0QPVV/iqPT6ssW51eCYvFEJade8lDk1fgpMn8/lzd+qDFWv2JRkHfJmuVK1kszDy4Ak2OUbgIqX2MS63aN8NKgcv/rbV9hIyVs+4Ivj5uDFbXeI7Wx5c7xRHlg0OkZyVbHWnm89P+n60z6gvxpkT1k+WBePvMQGAf2QHg7HbOyFayfWIGTUjdbovFMWcXRQcfOHijg7zaWoaE/mrDh+yEvd0vhs1GiGbAMzSzEyr40HsWIni4D23jiboYAe41LG2qoK125j+mxma3ZX/s7NYx5o3kxM8DHo8JLcBp4URRZ1DgctAJZIskboGhTG172bM96YhmmBEfJ7w5MYfbWSd+6VNJ42FG37pRdDEN9Xgj/gRVxJrrDMD4zQBwf1fTyDw7HiwIPKLltbYJZ+jfx5g2iX4icPPkOtWpm84n2jNUceOHJeyI2ov0ExSY20M3DyJj7rsQrzqt4g+S2i4D0Y76CoI0b6DpEqKuHQqNqY2bubsGLxoxtffw2incfYQUswSwMjkRIVru/ogw1iiKd8GZ2nYej31jQC88ln5APsBBr56+ooU1Z9O/RLphzWv96FMYg5NRzoFiTzCSE93lWCd5D3l5ovnja4q7wFV29AESe/410UIntb/3pwdSBeShZLQonjKFkfCyESm3xr1iQq9zOg0etBU3j8L84ccWHHSQpfVw5d32HVh1GO0kb2zBamxMzlCZqPgGIXwM01coz8gpZ0cEmmcKnx7KC0cfxAdRit/vBM0dvmDsbcw4V+AwB/qN1HzjTKFCFlgAvBzNXxrRlZYroAxORwIpOlEWQKvBVinWf0t+1RyzVQcYTCT+8hXeY14xi8ktLpTP0ESHpbE1f+5mYPpaEB44ESbdNFnSfIgjUB90F2YfNafPoGNWp/P1CvU3Q8n864waPf1TY0Npj3ajhlKDkvazVYQFQ8JWc6ZfgeKb25Bk8rgVIfxP4AENsnb0jw+CuF4YcmUF2Gk+tZtNTrZrOaTUsU2Sy4jk/HSlTQybuzm2GLBUbRX2UpJvMAKuf+1ysl/HQgzecr6NIK9dAjd5bgxoHB86Jshy5m1YL5Qc2DxAFw/AZTsLTH1UNoMG78HkAzvEqbNHwg5r6itzr1TrbhUmJuTkI6WV5gj//6E/WnTvR3o/X4jkPnZLr60f/ea6BtDr5AA3skNlIGwX+7as4PRN9SRTzuIIPq+5P1hjO80b+xmbsG0EhMUnbePlNrKFJJvOwyt89Kbijo+yZCO9BTBAkhz66kIXqfXzKktdZ7gyWHYBt7CQxSrY1rd90chtoHtE+r2RcyhqIl8MZun6IWXuXQl/yjXt4sVWBQVGQawxJWQbd5GQkUX6yo7m9C3Borr9HTheV4gbl/hCaH7CegFWzXzau6Ca8LoezcTXVBoeGTR+RQkjBvXYUYVvTdvwdN4XBTQvPdTJb6bx9sMg51cHZzvhJORgjpL5fImszzs4vxMj3LtuG/a2V+q7zPf4VfcjymER7+GjcPNd6Kt2fOPtrC8JoAFNhBikHmGeRyrIFUzCc6rB72ZPdUwybbmLlxiz5MB6DSrMg3EIsJ77WAHe1BPgBl3ebD35xo3nLEQ4gpAvousGqyNuOPLem0YcecO7h9849K9Q/WXwibEiZTSefX8hCC1LQst9ZlVyN5+mFCEsf+dlUl4BXNQSXCcGOzCwoyzF/bRu+uv224omioNMgHQxI7/YD6qQ39EjSm/9r9IwdYf1uIwFEybx/H5L5jXbIk2W0/ZdK8jNtk5avx3P9p70t73EaStP+K4E/dkFW8r8HsLqj7viVKmm00kmSSoniKNzXY/76RlFQu2+WesXfdL/bFNNBlMZkZzMyIeOKJICV6pbxedpueRp2vkgxJC6/PrVGq7Jpc6I4vw47nFQd+sFBnrmp04x13OazyXl+6cnYpbbeCsqOSa1MTc907jY42N7ql+6CtbICsFjNanfdvV+oc8BkahxKSLORFzVSjpc2ai1bNw3gQ6YbP7DuXy0CdGQFbrunptjuW8bKPfCovBTPWK9uxYndiRhSLMawKLCs/TG98gvuKDrwqG5tzmh2YY+E873T9fScsIgPykwFeRr5/2aQWbfG5GMX7OJQObCDGp3LdOZ3o9fGwVZvW+Ha5LNWxMFCROhSH1yyZyyfTpDWJ2WhiMrW2vBCO/HW/03cyK7NPYuDKzJjfMwtetvIYh5uDwiDR0g8SOw/aaNPUVr2OWyWFcPKnZlcmz4/Eu4VL4OBQXhjrECnd7JhOM2myYC7BTG6PTma22ffOi0WXHSvXQ8GEWozV1GAqLlUWPjNcpZtsi1lJi8vmSa/mRTWLk2xldCGQZZflAZIIccp7TKUtS48/5crGQHE4GWxYpj+iuarsd6q+N/HMopjJhpQf/GTTEeQtOsv9Ud6X8ERZOeSH2oSpMx9B3mHpI9aatdtT14bouoCZBVdpW9yu87TP50wTV+igzc3taXgC7uVP5kfFRX1/dDC5o80rM16opl3gGU6a28sxf7BWESUBrau04twv27bEHc8p5x8LdS01GXUYWmdud5of5RnaXo3r+TStbudsoIpqZh+pW3zJ+yatSPIBvMfJA2naA/rfjVi03h3XAWWP20ueYYY2y+bTaXEKmYvFul5sap7X1atyIc7latQft1dur3e5+JBqImQoHs/KEzzdD3Rc+fhG7r8NR4LHTgRFjYcuzBgQty0fFI2JV3l3Hltoft2y7uWQ7HoOJFGL+EQ5o/EhXHhj/pzNT3JXmytX2s8Pi+tWDJZtpHnxXA03nG8mfVcd8aVLYslpsDGE65ZP0TxeOt1CQMgKM2/DJRWX7IuyR+/Q9XqqOofjcDkSO2m3FHgd6Exue9KG6bKDaDzybsgxvcq9Dkfd/qYQBni1vYxCtZhvsmXfvMQDl/NtlnMGkjPjt7tY3y2nM8Ndi1m+rGZLJVCVa4JufJoq5LcIlgaaAxOO442y8NLwEu1k47IqNwujeTwYVqlKK5Wr7K5rXOa3DXk1rpjqJeSIEEenayFpT4PdBY/JV+7DkKnMUjot2tzt0k7F8lShfHltegiw02WWa84cJFFvth1S++tg0ClOWldP5lLgHgV1sWpOvQteUx1puwbYWBqH3lzqXKqwyZzkxJgVarG1VUi5tEDe6bFmdfJzh87mAyYDdInFxWB7c+J0FAs85xkxPdxy8ZwhP6FlD4t1SiqBMJMYbWNjzUobm99t+ufBYNke7ajtRZX2odXbVPIWT/N2GuiadXb4jdRky0P7jCW7csbBgveKmRvY3eVwZ8uqyswPdFB2Q+zgsXnYlibeVNSRzUZHNuWVpBteZit7NGID+1gCmrqBn4iMfELno9U5FNK1ewPbOjEmP10M0PE0DDbJOj+ahdoxMDsR83IxpXZWexCxShTtb2PMecFU9ZLTyDtp5mDXdw4sX63pcewc5C5iYsjj6Hx1mprpjOE2/sVPL9PxoPK1WdDmRho/GlM9x+5ep5Q6WzZHYBvzUab0GRZtK+m8vu5Mapxt+POSc65nA+y2TDvgOLtkPptx0mqRj8xVzpSRYWT7oMCHg7BUbmwcpm3gV9yVP6dgIIsVmuxGTZrj2HK84oe79ToO2svr1j7dKjwJuYWHlOhQQjC2knSo76bd3XyuMXI5pi+9mZN05SYvsPzhqm5vBflFABQ4AwJcaUxPYK2aFhxirTemYxTJpTqXknKxMcuiTReLlcFszvIJT6jusrjl+q0cHlNKn82r6abE5v4gbfNRZ8HvlCSAlNrCwRrpfC538SxFhCCtIRAsNd0GEn7J3LObCmxTUSTbDhR1sb4Ea1nDPcWQ9WZbAuDh7CTduu4lSKTTeC6b2U6c3Pjm6eIlSds+DVTyJcsz1adW0rVs9hRqPo7ninG7HpfpIVh03OZpJ44vnSOkJ9xw274dQkg5djjCHYAw72B4ZSSVrkV++E2VtRIvlgOr4DOz48vjFIku5TpoLVXDCnJXx/WX3Z6SyLw6wBs8kaaassnmo+bQYwyOydy9mPpHN+Wj5YzbzubOKAJ7TvbDbb5q75t9Lb6JgjxTehDcK5ly047UE6ZYk0YdyzivkDTIDLmNh5w8xe10RCN9GLDy4gTo0Fk7h6Nl33p72vyjZB9rVCdlScEp7wPzF5tY05oL4AGARVV1vhRm2AwnJ6U8RDJNm3l7vJzxm8l8I2Uc+UEjlymCM7WWbkU/1pJizc4ULlxEh1N+HE/3psTSF7VNm9BiHye2skBpczSd7qlBmpjDyzmgg/XJaePDLFKD4CKVZdgxw+UqYbuXCyJ3Nzl5dIryZu8g7/BkPKpcLUKewSWTZHHtjm9NzFvnw8HqomFlncsQMmaMaI03L8x0c3KPlKONY8Om/SqdDw4bXWsf9fHIGm3mCye6DCbF8pxIMd+jm2VQTIOtqZwXgcLw/sS6ql2lQ35SRd/nvS1eKBMaDWQ55JjAOJhSdzy4WKeRLYLWbMsSr2Y5qgx+pPZNSGlT00wPTN+EfdD9cez6JdsVmHMx02+pyvWEJbAzxTBnt3U6yIH3no+cMz3EYvfUofv5CeHy2Et7IVcNAdgyzqLjGKmzuOzyDn0r0mgqRlelOE7Xw32HXwfCkR92bD9da90JZLWdmU2z2mRx6rgJqkZGj5S1JtaqXVjOTQM+dtPmo8NWWCWMcq00ox+M+nK24XqnmXvpqUlnIPDLVffiCgFj7cwdvS1PPMRbZqHJ+n7qaAseqP2l48+PVWmaVztOFwsYssCpBljWQxTkH6G4vc6SwXxKfhTKdzyu6/PecbQvGX51uknMntluzKPFXy7aKN7dVqW40w9brsqqjsktFK69wP2OohqX0XCuDnXG03Jnu5EAQSzUbhseuYNHe4HDztRroq4hxPEjcZhrwSDajHsn1riGbTM56dxisgLIm9DhZbeZqQOj69rmzLfbnmAWWt9b4sPOJq8ZnEenw3K9353G1GhfKF6cxeSV1PlysFPkZsDytgd8Ebau4xRhZz+LRuNj1nb5UdGZHZdzc8lH9Fmx5AmKo97Odtx8qm36w+Vqth8NLH2pZXPmBmyY2fLamD8l86Gbxs7iZqdCOZ4O1P0WDTej88V1VX5czEyZfLnAKkpa3G6BmoGHVLovOHNhVoqesDl1A6557MqTizya6VZ55lmawaOj1ByIOyzTnRlHXSrybuKVuEgZYXa7lXO98IqpxumDY+jt9+piJrvnC8O3VdFm7GbHnXNXzom18lLF0u7W7xTizO1Mu9kwzfZUoV7VRf7PPfEnp83xNI/zC1ec+IXuV5l+jlnpIm3iXqfYkQdemkdlomziy3JwOPsK+QGoabkNZbRFQShfTilrjJXQNnoLe8x3j+MyAHsMVWmRT5c9KY2tkpXH6ISoOXXq53PMbadUdesC3jSjvdVsng9oi0fTJmJZ8RTOlTGezdrTdN0eDqw9NxZUqmjbMxQ3e3ln0hz6ebrK9Yukx360wTrmfE4dSyObOq+xPYS+R1W9lqlybc52/FHQSnqu2kt5urnNtKWwjg5HppRpSwkotW2ksUd5Z+aw0VJ3GU+747xUNpfEaqqDLMhjX6NTan5OdEexRkNvNR8MmvktCSnN7p4hP6VKvjnJm4+nZ0p5E1qLJnTpDbsFG2W9kXilTz0nkDvh/DQsxAT2boZyHUU8ziD7P1+V0hrPxpJJXzbbgcqO5yxeFHOUl2OTkzvi+Vjw89tVqwr70JTluL2k2+OmDX620ealEtNNNjOOsjg+zbC/6u6dkB95dsrzu6Y0CHfWCrjud37JVuddM+8dykpeovZchjRwj9Kr657E5dQ9d6zQPA+ZOY725dIZFpKl5Zdgaw2wq5F3w3rnlE0k8ba346pwhe5aKOaVKxSbmX0Em0D5tGemBoWmMy+96eIwWnqH7iUSwJmmAVNQ1CWKE53T0n0m7QqxHyqwSnfQXlqRAGahaO3mKcGOLShrBrLy4XkCxiXq04574/aKufGxkp/WslJOcDykh+GCYyoAZKqoODYuKf6W+Yttf4pY50Sd96OVMKSs3WVwJb8znyImWM8n2ZTaRr7jOm4a6pMpOveZ+XiiVqnbk8xcLS9XLpxpZu9CJRLv4bG02Qrr62yeXnvDJq3m3sZy9+QdUGq5BRvTQ4aKLWdZRP1hO6OrW+7ncXYw2gmaL7lVCVh9VJYbuxPxKTPUrG56jkw5i9KxddCAwcak9rocG5v5OWSaB5OuVqwhDKPyYOxLwIANuMOyT53jXS8RprdtClhxK8pZOM46oTYKjHUxS2/sUkunWtxs0tvCms5o5wamiSB36nZyo+qNbLVUOFxQ6k5dNQdUc+zFWopkbrLZWkd9RymzY2dVDhlvljOqluiH9CivtMC5reLBLVv359dUMMoO46+1y4Z1VoLLD/g0gNBk4Y6fFbS+HQFL1CqaVd2STSZl+8LoaqEts21lxZo6H+QQ369pvL+1U1qs6ERTQuE4tKajpsCqN/e4Xy0s8jsXNL4xIsMEu9t1Ea4Pmj+H+LhaLRcQNWdTdxIOIC0ZHs2sucWJvuVzcSMqU2ozdQZbTr6uhzvqnOvkV2mz1WS4Wzq06xzRdGOOJ9VsQSeyPTqI0tGxmu2O3Y3Ff/jMtEzeT6HLW6vZN8AHPKro9kIq7UqWkFPTm6DdTgm0nxOBa3KbnLzz5RzF1JW8G6aKLPI8ld0V1TAm775YDPy8GZF3EEw8kIuuy+ajRb4eKPa1Z9H9vP1C/Uvmz5cZufRsX7bH7eXAdXeON1yXnraTez2zPNgHqrs9ULsQZHnk2cq0fh8PzYDuNxdK2OVUuleHed9D4yPraLThxmVUBH6eHafsYFsZ5sIIuGt5IO8DnM3nFoQiqujsp5REXggEgpjJkgLRRZcIottr6Fl05UdPuICsvu2pjh+XvPf8vyBzv7C2OhZzCUtBdzPX5SIsm+uB0Da6TcXmK2yLx8po9qmnMHUBly06DNET2X21f790k9mp1qrftyA/Iu8lmcN50Ezz80nKKtGKOt2v2oq628uwnuU/o8P7ktTlV9e0vprXYxbkXUxfXHP91byesyi6X11z+JVtPWchqz9hL/ZuIW4Uj7cDpnNz9zpW1LmgchgYQkKeF2bstjY02ttifePjf+0v5OciiiA93p/HOjsv0nQ+PXcXa7e84niyOOvD7VJdjiH1sSLlrKN9xORHzVhd0hjP1mW4MFRMGU0K5x3yHj0AHnXiDQmP/wyKqAmJLeiqLsOOkwfknckIUSL0BIgi92RgjiNoKToExkAEpVxDtWlPJk3yfaR/yf0pcpWMyTd9xqk2s43fEW1Xw0zWH2y7k3ydmGW1WnPtkcRge1am/M1ot+fp0O/M3OugvNlBMEByl/on51+d0jFW5NUKnfFmwuQn1b/Zh3RUjNA+V+L2xDZtbTleDp38n4pF//f87F+Y/oN7Ie3jhZtbG9ZTTtW0e927Auqsz3I84bplSiUJGP0qEkonRWD579jflzZK6FA8ZpabyNww1mbsuyvN9CZKIZ9uGT7oKUxeD5ZMhyER4n97n/IkcntTf7xHB2TaBt9e6uPC6MqTm9KjyRs2Nv7mIK0xt4uCTLroqcyu7K5teqq66yb94SE+OP8cX/v/3C6+6SN9dUhRpw8fGx8cH9n4d9/x8e9pFeEPf3k0Qa6ObdLBQ4GdQUMCp/72AQf2h9/q1iT93Q9Nx3KwSQaxNCu0aLFFS1uG/Qun/EVgyOiISA8yX8cx9BKhJcVl+jtKfj+nvkcG/jVFuof//a/pGSMT/onJx3+fogrHjS3M6K8UHJKmTuhHHgYbrhoRnKt7vJ7c4GuGg9RBXmMBZ1HqhEHyenaGSsfP/MYSpefGFNaQnu/nKHI56nlpPTSrxwxMkOhZLTVNidQwgE5m3bz4JfiPRqth/vqmhXnvoBb9kLXGRhbHIOmtmJb5H2/HBe8dvBXSCYM89DIyG+S96eu2PhcVekH15rQX2iXIe0fg5yts/BLjJI0dI8WfLS4m4t9K/2yeVPxWMPXYQuqu0w//9dA30fMnjTbe02TjjQbfUdcbrTa+mDfMgm38Z0bTugRqaRAN3P+87jrp8tqDJWeDx5/P9hSa3Ge3L/rXImEnXbKTX07g7cZBt/hrGa8iyH4Rt3i62pbsE9mmv3/AHvZB4O9O7U6Y5ziZN4SWgXmxxUuC1dJN1mgximjxOmdKkmUSQT5OkYlSBGP+/sEIw9h0ApTWzvp3cNIqzNLfz9ixz0QHLEvTte/WzYVjpmdoZaS6NQqdIK2d/G+swr+AqwqK9KKAsz+PRVl8kckxw9PKC/NOw30EwYekSlLsk5UsnRJ7mwgZ9To/mDjFBtm23w3AkOT3KA516Ea/KJzCEXywHA8/t2e5nP3PIchzArce+fcPSYpi2OHAxCXZDkZ4Y58y6ZvFNSYZTopfxmATSR/mPccpSzPSh//67WtAe2vhepimoZ80QqsBiNLAgRGaYNgoMBsmvn+GCRhu8tLYwvkoTJyH4dVdncBOGmeU43p0gnzcMAGWg4TYmAkLxV4DJXfRcHmzHvCxkYTQhNK6PS3ChoGChg7jM9/HZn2lGDcQ/O+joGoY59AxcD3H967/seFhFAfYrGf9n5ml00wJR3+Tf3t5a7dzFBNvzPGWrP0d++VYXpdpTLcs0aBbvGEJLZnWrZYsMKZk8JaAWeun2S9H0y/QJNH8i0TM83nMsS98ba60zL8o7zXUI37Qfmle4n6G/f6BxY0CULuTNIowdj82CtzIEtC8E+BafUZYf7SywLgDJygdrmbhGhOtuAZcw8HJX75LtZIoSZxMcy3ZVOQWr1hGS5ckvWXwgiKIhomwxP801YqsWCtKkV84ornHsSzTd6hiaIl/kd9puI/4MdVKvMD+2apd9n4BB/3IOr82/o2olBxRDA3/sQ51R4NfG6+dmgzpBgp/r9tb7fbD2M889J5eTU4UOYtB4KgC0wL3FUGv2IQ5QyN5ryzPyj8x5EgkxCiM/CLcQ059zNCi8tAjT8Op91ruY37QaXmGl/9kzRY1IoOeGs4dzZ9AXDut82x9xX6C4YDvDoAzRsb5TVB4BJp3gBxMAZhPEoWBCdLCBiImlCWhc48IjQLCjFcTq6RhgUlABxuHoFmgMQ3YHRsG15ew4tBvsMBjOIMmgmrTevKaR/tLQ8MksCT4jkZPwIEwZCCCSABM5yoKYa6Jc4No4qSAWJkHQcbzwqJewz28wQUwShyvuoeheuaEZpkNvWrE2Ktx6XW9JPg5gYHJChokuH2KV6FlJThtACgue9C96T7DYowjWBoYPQlxCSwb2AFc6tOcYU/rId8X7iQTGYousy1TlIQWz4lKS+Z4MAzWEDiJMSyZ/5l0TSQIB0FMePK1ewML4Yx/5Wf8uy33QT/qPTz751I2kfs+xkY09bkERmDfiODeiKhzlhfuH9I8MHbkAevCJaQkxBeJLRUO5ChZQlzvyaDeuuUrXyNMquEEsNPI/Fg7vBVm8PczCpfjOKkDdkReI2WALGKj4BWOSVIOAySCEWdemjR+STBu1FlEg2vE4Ey/9H799TOHxK+uD8Mekl89EzzRR9W7flimMYpCcDlMDpM7WcCNJ2p4YWADoYV5B/XAMABWSeAng/2IYcpmFpPdAClOAB++z59YXRBZxmRagqTX/sS1FEaxWrxgIdmQGBPr7E/0J7mOPawg3EnGa4P8iE4iC2febaiH/KA3MaL4Z7MMvqGdqy/y2c/yUyd9Pz/laZ3hDRYDRQAmCMzAaiFFoVuihGlDYQUkCz+PBBJ8g2yT4YD8iU/AIw2QJ9/1UXMD6d2W+6AfBTxaYf8fcfzkPiUSTI3Qj0hGl6PYCTOIYkkEJ2t+nxBVotfShEdKKzUDID4avxZE7ulBXftoNR4g9egLwv0wALABfDDr0OqjKCKuDC5eX5KgTeuOA59wgVy78vXQ+xRiH+WaX0rm48vLy8cy+LUO6AEhAvFnI+EjmcBd5C+3e/9b8OvHO66WzsebA9Ed7ENurAE3kwx4UB2+z44JmHifO+AqtIBeaoR8lQ8IFCQAo/WGPJNyWNYjJ39pzEKAIpgsWWIW1znU1ztZbzvQLrMGvRhQ18T1ERj392EbUE5BkWSlxUsK8GxAspZimZA/cVjgLZE1fi5X4OuwzyvcE9vuDYLA3BtemcHXLfdBP+o6rPBnM+0FGOyDTadhCjZBHCe72+Xj6E0Zsrahl4b6sM/HQOSTiEYM4s3ge7h+UEpwRYif2CPU9iOxSh84bEZCIJDWOqZCACTlzPsk77b1WusMP9U1SULuwLjvMyeFERDPcWxLtxgIlbJktBTOkluWyHA8w0iMgJWfaE513Y8RIClj7uZ0b5Ak+Q7NYDxinbd93XIf9KPmJP0cc/om9eRF8Y1pMcyX5PMcGgAL8CmGvWPsGJkOKOkfskmSjQG+x+ZrNkiK3g8o1HFaYAJvQLZaAGM2QZ0I8qF73YYAHhkSQLcwdl8aU0I9aw76rQEEM11M6oDEZsl5kFFXBt/HyxQlpF5JPIkMs5CRkmTLsohaCL0jLqI7HnEiAPd72lbD85fXJT0fy3rmreiRfhYovhccdWS49UHi2OChz3JoSOZEqCv+asVk+5JzGKc1LEPYeUh97lxd8wx9ndj6I717TR+fwpwA/Lq+Phg7+fjcCkgw62orpKQEENJ6Gc9FfmuL/8Ywv700hvVWFo9U4RmwazR43Nx4T89ktiQB+GpGn835sfg6VMOJOt19lAyexb17PCSO8Z2JLId0LOkG25J1DrV4mqdbCoshOBmKwhhYFnke/1Q0oQlUAJ0TnmgCDTBcfKEf2CEQfPm65THoR9FEru8F/HlowshvweSrLJR5L3XlFeVt6voVAhUZJL2iHYb2k7N/Pl747KKs8OV44GxB7BhnkCIEOIOo9g/RS00aEC5JSSd4ZKGgCvQuCyX0KahpKiDPG4OuGR6qyVWKwHTfi5LARnEJ5JVc6VPABHZISmk1D/xEbe8Xe8TS5H6b7914+9Ig1BrH9zs236IGH79BqYk7W2AFzxz4iwkkZG53ePkidW4EBEYSn+Dvm/z5c9b8qciHamg1yVpBNWSoHyaEj6QP8QYiFTan5uKQepMLfcnAayYPZAQ2OMWt0GrBuBZYxj3bT+4hwDiTmwU1+HvP/X3ybIAbsxU5GGT/jWN+u2N1BbIi5MTgpsJvX14TkBlgzAcPy/EXewsKICVH9Kzc1SEG5gDJSE5CCgB9VaPrWxz+QglGXT6s63nPW68EmZ8M/Z6tgNUgAEsAIT2Mz2FY4ySpQDZIqvBdsIg5zJmcAOmuYAFxRxbsn8mwLSQKChZpmrOEn8fZZZ67Y574IOSvDbJyB0FZqO+CvNNQD/lDUPwzCbn4dtN3gQFbY4cxIc7f3HjJUGTG0ltI0jlIlhBEJlGByKRDIs8JApJY/WcnS+wz1jyPRf419nyjoR7xw7ckWObn60V6q5dPLOgVrAz8qOOh+FGgrJ2NFBpIDf4LXkKIWH0fkrilExgElb9Nd8BT749C1IXKyEN1pR8QAJzZseuKI5GGItgUcsfDyuJ7ShbAxxRSq/qG5/cRG9niFEHh+ZYpmAaYj260EAN/GAbJMrI4rNA/N00iPEZmXtg3vEZgmdeUqK4wftVwH/GDhiSwyp/KaViBf0tqvuIXc+S1IWgE3y7O84LyJVq8FVAaOCIrJTTnPWrymU2rpO4NTOhZ53pb5brHSxfHAfYatdoabuOvEJnNENeM5klW7lwFglzNEb5Ny18a3ZAEHeD6r8wD3Z/2IAOJtbu/vjeT1ySkDuM1D4EIZ2ekrHefHwQ/CJJvnj76YoDpkBq9+VY4SUIIf3444jNP+zz7qgv4wCS+zpdIBvJVkvSacXXeWwRhQzZoNq65mh/CIbk9AtG4fpzlHX70kRAS9EwoYU4geoNJLUX/XGewGOIS57DAwAw+krLdJ3R5U8J5xn4YD6Sthph3nuZqND97tOul0cth1XdzaPxbI3hzoS/kwxQJsX1nhoSU3SuYjzrrFwnnN/gwsSPUqFGiVTiwIAtjs/VMh+s+90m84iCkkyly6ySYFCpr9vZ9KKgjSzBYrLSwYZL0jtVbui4BmREQLViWzBum9FPL9uDfAqfc70E+jkVBfkZPhn1h32m4j/jRcCrSPyW1+yPoAX4PpghOFeD7Pen0Gwy2cjD8rf3FIfEWOHSdAD6YeR0fnaCOuo1PY02HkF794R71LfpXe0hqu3pw8fuzZ05iZAmYaImIMb96NJgVDkynfGnMAfBqumySKGw6uWMSa/50QfJ8bNIwvPtdyNd78w8m/6buUHP5j/eCEpGP4vu9xLOjOynsxxnlThjfb+XfKXudIVWQNoD2jHq+CYbh5AAWmRl1uH9A1jPB+cOaxm//DYBUeew=',\n",
       "  'filename': 'attention.pdf'}}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tables[0].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "54aa6828",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "prompt_template = \"\"\"Describe the image in detail. For context,\n",
    "                  the image is part of a research paper explaining the transformers\n",
    "                  architecture. Be specific about graphs, such as bar plots.\"\"\"\n",
    "messages = [\n",
    "    (\n",
    "        \"user\",\n",
    "        [\n",
    "            {\"type\": \"text\", \"text\": prompt_template},\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": \"data:image/jpeg;base64,{image}\"},\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "chain = prompt | ChatOpenAI(model=\"gpt-4o-mini\") | StrOutputParser()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "10086981",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_summaries = chain.batch(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "7a2be71f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image depicts a detailed schematic of the transformer architecture, a foundational model in natural language processing.\n",
      "\n",
      "### Key Components:\n",
      "1. **Inputs and Outputs**:\n",
      "   - **Input Embedding**: Located at the bottom left, it represents the input sequence.\n",
      "   - **Output Embedding**: Positioned at the bottom right, indicative of the shift to the right for decoding.\n",
      "\n",
      "2. **Positional Encoding**:\n",
      "   - Found at the bottom, it connects the input and output embeddings to the rest of the model, suggesting the incorporation of positional information in the sequence.\n",
      "\n",
      "3. **Layers**:\n",
      "   - The structure contains two main sections, indicative of the encoder (left) and decoder (right), both consisting of several identical blocks (labeled Nx for layers).\n",
      "   - Each block in the encoder consists of:\n",
      "     - **Multi-Head Attention**: Focuses on various parts of the input simultaneously.\n",
      "     - **Add & Norm**: A residual connection followed by layer normalization.\n",
      "     - **Feed Forward**: A fully connected layer applied to each position separately.\n",
      "\n",
      "4. **Decoder Block**:\n",
      "   - Mirrors the encoder but includes a **Masked Multi-Head Attention** layer to prevent attending to future tokens.\n",
      "   - It similarly incorporates the **Add & Norm** and **Feed Forward** layers.\n",
      "\n",
      "5. **Output Layer**:\n",
      "   - At the top, the final output probabilities are computed through:\n",
      "     - **Linear**: Transforms the output into logits.\n",
      "     - **Softmax**: Converts logits into probabilities for predictions.\n",
      "\n",
      "### Arrangements and Arrows:\n",
      "- The arrows show the flow of data through embeddings, attention mechanisms, and the output layer.\n",
      "- Feedback loops illustrate the connections between the various components, emphasizing the roles of the attention mechanisms and normalization.\n",
      "\n",
      "Overall, this image provides a comprehensive overview of the transformer architecture, highlighting the layered approach and attention mechanisms that allow the model to process sequences effectively.\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(image_summaries[0])\n",
    "\n",
    "display_base64_image(images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "4d2483dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "# Prompt\n",
    "prompt_text = \"\"\"\n",
    "You are an assistant tasked with summarizing tables and text.\n",
    "Give a concise summary of the table or text.\n",
    "\n",
    "Respond only with the summary, no additionnal comment.\n",
    "Do not start your message by saying \"Here is a summary\" or anything like that.\n",
    "Just give the summary as it is.\n",
    "\n",
    "Table or text chunk: {element}\n",
    "\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "\n",
    "# Summary chain\n",
    "model = ChatGroq(temperature=0.5, model=\"llama-3.3-70b-versatile\")\n",
    "summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "b92bdb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize text\n",
    "#text_summaries = summarize_chain.batch(texts, {\"max_concurrency\": 3})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "47ffa59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_html = [table.metadata.text_as_html for table in tables]\n",
    "table_summaries = summarize_chain.batch(tables_html, {\"max_concurrency\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "08300b73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The table compares four neural network layer types: Self-Attention, Recurrent, Convolutional, and Self-Attention (restricted). \\nSelf-Attention has O(n^2 - d) complexity, O(1) sequential operations, and O(1) maximum path length.\\nRecurrent has O(n-d) complexity, O(n) sequential operations, and O(n) maximum path length.\\nConvolutional has O(k-n-d) complexity, only sequential operations, and O(log(n)) maximum path length.\\nSelf-Attention (restricted) has O(r-n-d) complexity, few sequential operations, and O(n/r) maximum path length.',\n",
       " 'The table compares various machine translation models, including ByteNet, Deep-Att, GNMT, ConvS2S, MoE, and Transformer models, across different metrics such as EN-DE translation quality, BLEU EN-FR score, and training and cost (FLOPs) for EN-FR translation. The Transformer (big) model achieves the highest EN-DE translation quality score of 28.4, while the GNMT + RL Ensemble model achieves the highest BLEU EN-FR score of 41.16.',\n",
       " 'The table compares various models with different parameters and settings. The models are labeled as (A), (B), (C), (), and (E), with some entries labeled as \"base\" and \"big\". The parameters and settings include model size, hidden dimensions, positional encoding, and training steps. The results show varying perplexity (PPL) and BLEU scores for each model, indicating differences in performance.']"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "82008726",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "# Prompt\n",
    "prompt_text = \"\"\"\n",
    "You are an assistant tasked with summarizing tables and text.\n",
    "Give a concise summary of the table or text.\n",
    "\n",
    "Respond only with the summary, no additionnal comment.\n",
    "Do not start your message by saying \"Here is a summary\" or anything like that.\n",
    "Just give the summary as it is.\n",
    "\n",
    "Table or text chunk: {element}\n",
    "\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "\n",
    "chain = prompt | ChatOpenAI(model=\"gpt-4o-mini\") | StrOutputParser()\n",
    "\n",
    "# Summary chain\n",
    "model = ChatOpenAI(temperature=0.5,model=\"gpt-4o-mini\") \n",
    "summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "a643b4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize text\n",
    "text_summaries = summarize_chain.batch(texts, {\"max_concurrency\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "f74e9c8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The paper introduces the Transformer, a novel neural network architecture that relies solely on attention mechanisms, eliminating the need for recurrent or convolutional structures. This model outperforms traditional sequence transduction models in machine translation tasks, achieving a BLEU score of 28.4 for English-to-German and 41.0 for English-to-French translations, representing significant improvements over previous state-of-the-art results. The Transformer allows for greater parallelization and faster training times, demonstrating its efficiency in handling longer sequences without the limitations of sequential computation.',\n",
       " 'The paper introduces the Transformer, a novel neural network architecture that relies solely on attention mechanisms, eliminating the need for recurrent or convolutional structures. This approach enhances parallelization and training efficiency, achieving superior results in machine translation tasks. Specifically, the Transformer reached a BLEU score of 28.4 on the WMT 2014 English-to-German task and 41.0 on the English-to-French task, outperforming previous state-of-the-art models while requiring significantly less training time. The authors highlight the limitations of traditional recurrent models and emphasize the advantages of attention-based architectures for sequence modeling.',\n",
       " 'The paper introduces the Transformer, a novel neural network architecture that relies solely on attention mechanisms, eliminating the need for recurrent or convolutional structures. This approach enables greater parallelization and significantly faster training times. The Transformer achieves state-of-the-art performance in machine translation, scoring 28.4 BLEU on the WMT 2014 English-to-German task and 41.0 BLEU on the English-to-French task, outperforming previous models. The authors highlight the limitations of traditional recurrent models and emphasize the advantages of using attention mechanisms for better modeling of dependencies in sequences.',\n",
       " 'The paper introduces the Transformer, a novel neural network architecture that solely relies on attention mechanisms, eliminating the need for recurrent or convolutional layers. This approach enhances parallelization and reduces training time significantly. The Transformer outperforms existing models in machine translation tasks, achieving a BLEU score of 28.4 for English-to-German and 41.0 for English-to-French translations, marking a substantial improvement over previous best results. The authors emphasize the advantages of attention mechanisms in capturing dependencies without sequential constraints, allowing for more efficient training and better performance.',\n",
       " 'The paper introduces the Transformer, a novel neural network architecture for sequence transduction that relies solely on attention mechanisms, eliminating the need for recurrent or convolutional structures. It demonstrates superior performance in machine translation tasks, achieving a BLEU score of 28.4 for English-to-German and 41.0 for English-to-French, significantly improving upon previous models while requiring less training time. The Transformer enhances parallelization and efficiency in training, addressing limitations of traditional recurrent models.',\n",
       " 'The paper introduces the Transformer, a novel neural network architecture that relies solely on attention mechanisms, eliminating the need for recurrent or convolutional layers. This approach enhances parallelization and reduces training time significantly. The Transformer achieves superior performance in machine translation tasks, setting new state-of-the-art BLEU scores of 28.4 for English-to-German and 41.0 for English-to-French translations, outperforming previous models while requiring less computational resources. The authors emphasize the limitations of traditional recurrent models and highlight the advantages of using attention mechanisms for sequence modeling.',\n",
       " 'The paper introduces the Transformer, a novel neural network architecture that relies entirely on attention mechanisms, eliminating the need for recurrent or convolutional layers. This design enables greater parallelization and significantly reduces training time. The Transformer achieves state-of-the-art results in machine translation, with a BLEU score of 28.4 for English-to-German and 41.0 for English-to-French, surpassing previous models while requiring less computational resources. The authors highlight the limitations of traditional recurrent models and emphasize the advantages of using attention mechanisms for sequence modeling.',\n",
       " 'The paper introduces the Transformer, a novel neural network architecture that relies solely on attention mechanisms, eliminating the need for recurrent or convolutional structures. It demonstrates superior performance in machine translation tasks, achieving a BLEU score of 28.4 for English-to-German and 41.0 for English-to-French, outperforming previous models while requiring significantly less training time. The Transformer allows for enhanced parallelization, addressing the limitations of traditional recurrent models in sequence processing.',\n",
       " 'The paper \"Attention Is All You Need\" introduces the Transformer, a new neural network architecture that relies solely on attention mechanisms, eliminating the need for recurrent or convolutional networks. This model demonstrates superior performance in machine translation tasks, achieving state-of-the-art BLEU scores of 28.4 for English-to-German and 41.0 for English-to-French translations, while significantly reducing training time and enhancing parallelization. The authors highlight the limitations of traditional recurrent models and emphasize the advantages of using attention mechanisms for better dependency modeling across sequences.',\n",
       " 'The paper \"Attention Is All You Need\" introduces the Transformer, a novel network architecture that relies solely on attention mechanisms, eliminating the need for recurrence and convolutions. This model demonstrates superior performance in machine translation tasks, achieving a BLEU score of 28.4 on the WMT 2014 English-to-German translation and 41.0 on English-to-French translation, outperforming existing models while being more parallelizable and requiring less training time. The authors highlight the limitations of traditional recurrent neural networks and emphasize the benefits of the attention-based approach for improved computational efficiency and translation quality.',\n",
       " 'The paper \"Attention Is All You Need\" introduces the Transformer, a novel neural network architecture that relies solely on attention mechanisms, eliminating the need for recurrent or convolutional structures. This architecture enhances parallelization and reduces training time significantly. The Transformer outperforms existing models in machine translation tasks, achieving a BLEU score of 28.4 for English-to-German and 41.0 for English-to-French translations, setting new state-of-the-art records. The authors highlight the limitations of recurrent neural networks and emphasize the advantages of using attention mechanisms for modeling dependencies in sequences.',\n",
       " 'The paper introduces the Transformer model, a novel architecture for sequence transduction that relies solely on attention mechanisms, eliminating the need for recurrent or convolutional networks. This approach enhances parallelization and reduces training time. The Transformer achieves superior translation quality, scoring 28.4 BLEU on the WMT 2014 English-to-German task and 41.0 BLEU on the English-to-French task, outperforming existing models significantly. The authors highlight the limitations of recurrent models in terms of sequential computation and emphasize the benefits of using attention mechanisms for modeling dependencies in sequences.',\n",
       " 'The paper \"Attention Is All You Need\" introduces the Transformer, a novel neural network architecture that relies solely on attention mechanisms, eliminating the need for recurrent or convolutional layers. It demonstrates superior performance in machine translation tasks, achieving a BLEU score of 28.4 for English-to-German and 41.0 for English-to-French, which surpasses previous state-of-the-art results while requiring significantly less training time. The Transformer enhances parallelization in training, addressing the limitations of traditional recurrent models. The authors highlight their collaborative contributions to the design and implementation of the model.',\n",
       " 'The paper introduces the Transformer, a novel neural network architecture that relies solely on attention mechanisms, eliminating the need for recurrent or convolutional structures. It demonstrates superior performance in machine translation tasks, achieving a BLEU score of 28.4 for English-to-German and 41.0 for English-to-French, outperforming existing models while requiring significantly less training time. The Transformer enables greater parallelization and efficiency compared to traditional recurrent models, which are constrained by their sequential computation nature.',\n",
       " 'The paper \"Attention Is All You Need\" introduces the Transformer model, a novel architecture that relies solely on attention mechanisms for sequence transduction, eliminating the need for recurrent or convolutional networks. This approach enables greater parallelization and significantly reduces training time. The Transformer achieves state-of-the-art results in machine translation, with a BLEU score of 28.4 for English-to-German and 41.0 for English-to-French, outperforming existing models while requiring less computational resources. The authors highlight the advantages of attention mechanisms in modeling dependencies without distance constraints, marking a shift in sequence modeling methodologies.',\n",
       " 'The paper introduces the Transformer model, a novel architecture for sequence transduction that relies solely on attention mechanisms, eliminating the need for recurrent or convolutional networks. It demonstrates superior performance in machine translation tasks, achieving a BLEU score of 28.4 on English-to-German and 41.0 on English-to-French translations, outperforming existing models while being more efficient in training time. The authors highlight the limitations of traditional recurrent models and emphasize the advantages of attention mechanisms for capturing dependencies in sequences, enabling greater parallelization and faster training.',\n",
       " 'The paper introduces the Transformer, a novel neural network architecture that relies solely on attention mechanisms, eliminating the need for recurrent or convolutional structures. This approach enhances parallelization, reduces training time, and improves translation quality. The Transformer achieves a BLEU score of 28.4 on the WMT 2014 English-to-German task and a state-of-the-art score of 41.0 on the English-to-French task, outperforming existing models significantly. The authors highlight the limitations of traditional recurrent models and emphasize the benefits of the attention-based architecture in sequence modeling and transduction tasks.',\n",
       " 'The paper introduces the Transformer, a novel neural network architecture that relies solely on attention mechanisms, eliminating the need for recurrent and convolutional networks. It demonstrates superior performance in machine translation tasks, achieving a BLEU score of 28.4 for English-to-German and 41.0 for English-to-French, surpassing previous models while being more efficient in training time and parallelization. The study highlights the limitations of traditional recurrent models and emphasizes the advantages of the attention-based approach in handling dependencies in sequence transduction tasks.',\n",
       " 'The paper introduces the Transformer, a new neural network architecture for sequence transduction that relies solely on attention mechanisms, eliminating the need for recurrent or convolutional structures. It demonstrates superior performance in machine translation tasks, achieving a BLEU score of 28.4 for English-to-German and 41.0 for English-to-French, significantly improving upon previous models while requiring less training time. The Transformer allows for greater parallelization, addressing the limitations of traditional recurrent models.',\n",
       " 'The paper \"Attention Is All You Need\" introduces the Transformer, a novel neural network architecture that relies solely on attention mechanisms, eliminating the need for recurrent or convolutional layers. This model demonstrates superior performance in machine translation tasks, achieving a BLEU score of 28.4 for English-to-German and 41.0 for English-to-French translations, marking significant improvements over existing models while requiring less training time and resources. The authors highlight the limitations of traditional recurrent models in terms of parallelization and propose the Transformer as a more efficient alternative for sequence modeling.',\n",
       " 'The paper introduces the Transformer, a novel neural network architecture that relies solely on attention mechanisms, eliminating the need for recurrent or convolutional structures. It demonstrates superior performance in machine translation tasks, achieving a BLEU score of 28.4 on English-to-German and 41.0 on English-to-French translations, significantly improving upon previous models while requiring less training time and enabling greater parallelization. The authors highlight the limitations of traditional recurrent models and emphasize the advantages of the Transformer in capturing long-range dependencies efficiently.',\n",
       " 'The paper introduces the Transformer, a novel neural network architecture that relies solely on attention mechanisms, eliminating the need for recurrent or convolutional structures. This approach enhances parallelization and reduces training time significantly. Experiments demonstrate that the Transformer achieves superior performance in machine translation tasks, with a BLEU score of 28.4 for English-to-German and 41.0 for English-to-French, surpassing previous state-of-the-art models. The authors highlight the limitations of traditional recurrent models and the advantages of attention mechanisms in capturing dependencies across sequences.',\n",
       " 'The Extended Neural GPU, ByteNet, and ConvS2S aim to reduce sequential computation by utilizing convolutional neural networks for parallel processing of input and output positions. However, these models struggle with learning dependencies over distant positions. The Transformer addresses this by employing self-attention, which computes representations of sequences without relying on recurrent or convolutional structures, achieving a constant number of operations for dependencies. Self-attention has proven effective in various tasks, while the Transformer architecture consists of an encoder-decoder structure, using stacked self-attention and fully connected layers for processing sequences.',\n",
       " 'The Extended Neural GPU, ByteNet, and ConvS2S aim to reduce sequential computation by using convolutional neural networks for parallel processing of inputs and outputs. However, these models struggle with learning dependencies over distant positions, unlike the Transformer, which utilizes self-attention to maintain constant operational complexity. Self-attention effectively relates different positions within a sequence and has been applied successfully in various tasks. The Transformer is unique as it relies solely on self-attention without employing RNNs or convolutions. Its architecture consists of an encoder-decoder structure, where the encoder transforms input sequences into continuous representations and the decoder generates output sequences in an auto-regressive manner, using stacked self-attention and fully connected layers.',\n",
       " 'The text discusses various neural network architectures aimed at reducing sequential computation, highlighting the Extended Neural GPU, ByteNet, and ConvS2S, which use convolutional neural networks for parallel computation. It emphasizes the Transformer model as the first to rely entirely on self-attention for input-output representation without sequence-aligned RNNs or convolutions. Self-attention effectively computes representations across different positions in a sequence, proving successful in various tasks. The Transformer adopts an encoder-decoder structure, utilizing stacked self-attention and fully connected layers for both components.',\n",
       " 'The text discusses advancements in neural network architectures aimed at reducing sequential computation, highlighting models like Extended Neural GPU, ByteNet, and ConvS2S that utilize convolutional networks for parallel computation. It notes the challenges of learning dependencies between distant positions in these models and introduces the Transformer, which uses self-attention to compute representations without relying on recurrent or convolutional structures. The Transformer features an encoder-decoder architecture, where the encoder processes input sequences into continuous representations, and the decoder generates output sequences in an auto-regressive manner, employing stacked self-attention and fully connected layers.',\n",
       " 'The text discusses advancements in reducing sequential computation in neural models, highlighting the Extended Neural GPU, ByteNet, and ConvS2S, which use convolutional networks for parallel computation. These models face challenges in learning dependencies between distant positions, which the Transformer addresses with a constant number of operations using self-attention. Self-attention computes representations of a sequence by relating different positions within it and has been successful in various tasks. The Transformer is unique as it relies entirely on self-attention without recurrent or convolutional structures. It features an encoder-decoder architecture where the encoder transforms input sequences into continuous representations, and the decoder generates output sequences in an auto-regressive manner, employing stacked self-attention and fully connected layers.',\n",
       " 'The Extended Neural GPU, ByteNet, and ConvS2S aim to reduce sequential computation using convolutional neural networks for parallel processing of inputs and outputs. These models face challenges in learning dependencies between distant positions, with ConvS2S operating linearly and ByteNet logarithmically in terms of operations required. The Transformer model addresses this by using self-attention, allowing constant operation counts for dependencies, though it sacrifices some resolution, which is mitigated by Multi-Head Attention. Self-attention has been effective in various tasks, while end-to-end memory networks utilize recurrent attention mechanisms. The Transformer uniquely employs self-attention for input-output representation without relying on RNNs or convolutions. It features an encoder-decoder structure where the encoder transforms input symbol sequences into continuous representations, and the decoder generates output sequences in an auto-regressive manner, utilizing stacked self-attention and fully connected layers.',\n",
       " 'The text discusses advancements in neural network architectures aimed at reducing sequential computation, highlighting models like the Extended Neural GPU, ByteNet, and ConvS2S, which utilize convolutional neural networks for parallel processing. It notes that while these models struggle with learning dependencies over long distances, the Transformer model addresses this issue through self-attention, allowing constant operations across input positions. Self-attention, effective in various tasks, is unique to the Transformer, which relies solely on this mechanism without traditional RNNs or convolutions. The Transformer employs an encoder-decoder structure with stacked self-attention and fully connected layers for both components.',\n",
       " 'The text discusses advancements in neural network architectures, particularly focusing on the Transformer model, which utilizes self-attention for sequence transduction without relying on recurrent neural networks (RNNs) or convolution. Unlike previous models like Extended Neural GPU, ByteNet, and ConvS2S, which struggle with long-distance dependencies, the Transformer maintains a constant operation count for attention across positions, albeit with some loss in resolution. The architecture features an encoder-decoder structure where the encoder transforms input sequences into continuous representations and the decoder generates output sequences in an auto-regressive manner. The use of stacked self-attention and fully connected layers characterizes both the encoder and decoder components of the Transformer.',\n",
       " \"The encoder consists of 6 identical layers, each with a multi-head self-attention mechanism and a fully connected feed-forward network, utilizing residual connections and layer normalization. The decoder also has 6 layers, with an additional sub-layer for multi-head attention over the encoder's output, and includes masking to prevent future position attention. The attention function maps queries and key-value pairs to outputs through a weighted sum of values, where weights are determined by a compatibility function between the query and keys.\",\n",
       " \"The encoder consists of 6 identical layers, each with a multi-head self-attention mechanism and a feed-forward network, utilizing residual connections and layer normalization with an output dimension of 512. The decoder also has 6 layers, adding a third sub-layer for multi-head attention over the encoder's output, employing similar residual connections and layer normalization. It incorporates masking to ensure predictions depend only on known outputs. Attention functions map queries and key-value pairs to outputs, calculated as a weighted sum of values based on query-key compatibility.\",\n",
       " \"The encoder consists of 6 identical layers, each with a multi-head self-attention mechanism and a position-wise fully connected feed-forward network, utilizing residual connections and layer normalization, producing outputs of dimension 512. The decoder also has 6 identical layers, adding a third sub-layer for multi-head attention over the encoder's output and employing masking to prevent future position attention. An attention function maps a query and key-value pairs to an output, calculated as a weighted sum of values based on the compatibility of the query with the keys.\",\n",
       " \"The encoder consists of 6 identical layers, each with a multi-head self-attention mechanism and a position-wise fully connected feed-forward network, utilizing residual connections and layer normalization. The decoder also has 6 layers, adding a third sub-layer for multi-head attention over the encoder's output and incorporating masking to ensure predictions depend only on previous outputs. Attention functions map queries to key-value pairs, producing outputs as weighted sums of values based on compatibility with keys.\",\n",
       " 'The encoder consists of 6 identical layers, each with a multi-head self-attention mechanism and a feed-forward network, utilizing residual connections and layer normalization, producing outputs of dimension 512. The decoder also has 6 identical layers, adding a third sub-layer for multi-head attention over encoder outputs and incorporating masking to ensure predictions depend only on prior outputs. An attention function maps queries and key-value pairs to an output, computed as a weighted sum of values based on compatibility between the query and keys.',\n",
       " \"The encoder consists of 6 identical layers, each with a multi-head self-attention mechanism and a position-wise feed-forward network, utilizing residual connections and layer normalization. The decoder also has 6 layers, adding a third sub-layer for multi-head attention over the encoder's output and incorporating masking to prevent future position attention. An attention function maps a query and key-value pairs to an output, calculated as a weighted sum of the values based on the query-key compatibility.\",\n",
       " \"The encoder consists of 6 identical layers, each with a multi-head self-attention mechanism and a fully connected feed-forward network, incorporating residual connections and layer normalization. The decoder also has 6 layers, adding a third sub-layer for multi-head attention over the encoder's output, with masking to prevent future position attention. An attention function maps a query and key-value pairs to an output through a weighted sum of values based on compatibility between the query and keys.\",\n",
       " \"The encoder consists of 6 identical layers, each with a multi-head self-attention mechanism and a feed-forward network, utilizing residual connections and layer normalization. The decoder also has 6 layers, adding a third sub-layer for multi-head attention over the encoder's output, with masking to ensure predictions depend only on prior outputs. An attention function maps a query and key-value pairs to an output, calculated as a weighted sum of values based on the query-key compatibility.\",\n",
       " 'Scaled Dot-Product Attention computes attention using queries, keys, and values, applying a softmax function after scaling the dot products of queries and keys. This method is faster and more space-efficient than additive attention, especially for larger dimensions, due to its reliance on optimized matrix multiplication. Multi-Head Attention enhances this by applying multiple attention functions in parallel with different learned projections, allowing the model to capture diverse information across different representation subspaces. In this implementation, eight parallel attention heads are used, each with a reduced dimensionality, maintaining computational efficiency comparable to single-head attention.',\n",
       " 'Scaled Dot-Product Attention computes attention using queries, keys, and values, involving a dot product, scaling by the square root of the dimension, and applying a softmax function. It is faster and more space-efficient than additive attention, particularly for larger dimensions. Multi-Head Attention enhances this by using multiple attention heads, allowing the model to focus on different representation subspaces simultaneously. Each head processes projected queries, keys, and values, and the outputs are concatenated and projected to form the final output. The model uses 8 parallel attention heads with each head having a reduced dimensionality.',\n",
       " 'Scaled Dot-Product Attention computes attention by taking the dot products of queries and keys, scaling them, and applying a softmax function to obtain weights for the values. The formula used is Attention(Q,K,V) = softmax(QKT/dk)V. It is faster and more space-efficient than additive attention, especially for large dimensions, due to its reliance on optimized matrix multiplication. Multi-Head Attention enhances this by using multiple attention heads, projecting queries, keys, and values into different subspaces, allowing the model to capture diverse information. In this implementation, 8 parallel attention heads are used, each with dimensions of 64.',\n",
       " 'Scaled Dot-Product Attention computes attention weights using queries, keys, and values, scaling the dot products to maintain gradients. Multi-Head Attention enhances this by projecting queries, keys, and values into multiple subspaces, allowing the model to capture diverse information through parallel attention heads. In this implementation, 8 heads are used, each with reduced dimensions, maintaining computational efficiency similar to single-head attention.',\n",
       " 'Scaled Dot-Product Attention computes attention by taking the dot products of queries and keys, scaling by the square root of the dimension, and applying a softmax function to obtain weights for the values. It utilizes matrices for efficient computation. Multi-Head Attention enhances this by projecting queries, keys, and values into multiple subspaces, allowing the model to attend to different information simultaneously. The process involves concatenating the outputs of parallel attention heads and projecting them to the final output dimensions, maintaining computational efficiency similar to single-head attention.',\n",
       " 'Scaled Dot-Product Attention computes the dot products of queries and keys, scales them by the square root of the key dimension, and applies a softmax to obtain weights for the values. It is faster and more space-efficient than additive attention, especially for larger dimensions. Multi-Head Attention enhances this by projecting queries, keys, and values into multiple subspaces, allowing the model to attend to different information simultaneously. The final output is a concatenation of these attention outputs, maintaining computational efficiency. In this implementation, 8 parallel attention heads are used with a dimension of 64 for each head.',\n",
       " 'Scaled Dot-Product Attention computes attention using queries, keys, and values, with the formula Attention(Q,K,V) = softmax(QKT/dk)V. It is faster and more space-efficient than additive attention, especially for large dimensions. Multi-Head Attention enhances this by linearly projecting queries, keys, and values multiple times, allowing the model to focus on different representation subspaces simultaneously. The implementation uses 8 parallel attention heads, each with reduced dimensions, maintaining similar computational costs to single-head attention.',\n",
       " 'Scaled Dot-Product Attention computes attention by taking the dot products of queries and keys, scaling them, and applying softmax to obtain weights for values. It uses the formula Attention(Q,K,V) = softmax(QKT/dk)V. Multi-Head Attention enhances this by linearly projecting queries, keys, and values multiple times and performing attention in parallel, allowing the model to focus on different representation subspaces. The process involves concatenating outputs from multiple attention heads and projecting them again. In this implementation, 8 parallel heads are used, each with dimensions dk = dv = dmodel/h = 64, maintaining computational efficiency similar to single-head attention.',\n",
       " \"Scaled Dot-Product Attention computes attention weights by taking the dot products of queries and keys, scaling them by the square root of their dimension, and applying a softmax function. This method is faster and more space-efficient than additive attention, particularly for larger dimensions. Multi-Head Attention enhances the model's ability to focus on different representation subspaces by using multiple attention heads, each projecting queries, keys, and values into lower dimensions before performing attention in parallel. In this implementation, eight parallel attention heads are used, with each head having a dimension of 64, maintaining a similar computational cost to single-head attention.\",\n",
       " 'Scaled Dot-Product Attention computes attention weights by taking the dot product of queries and keys, scaling by the square root of the key dimension, and applying softmax to obtain weights for values. It is more efficient than additive attention, especially for larger dimensions, due to its reliance on optimized matrix multiplication. Multi-Head Attention enhances this by projecting queries, keys, and values into multiple subspaces, allowing the model to attend to different information simultaneously. In this implementation, 8 parallel attention heads are used, with each having reduced dimensions, maintaining similar computational costs to single-head attention.',\n",
       " 'Scaled Dot-Product Attention computes attention by taking the dot products of queries and keys, scaling by the square root of their dimension, and applying a softmax function to derive weights for the values. It is more efficient than additive attention, particularly for larger dimensions, due to its implementation via optimized matrix multiplication. Multi-Head Attention enhances this by projecting queries, keys, and values into multiple subspaces, allowing the model to capture diverse information simultaneously. In this implementation, 8 parallel attention heads are used, each with reduced dimensions, maintaining overall computational efficiency comparable to single-head attention.',\n",
       " 'Scaled Dot-Product Attention computes the dot products of queries and keys, scales them, applies softmax to obtain weights, and processes multiple queries simultaneously using matrices. It contrasts with additive attention, which uses a feed-forward network, but is more efficient in practice. Multi-Head Attention enhances this by projecting queries, keys, and values into multiple subspaces, allowing the model to attend to different information concurrently, improving performance while maintaining computational efficiency. In this implementation, 8 parallel attention heads are used, each with reduced dimensions.',\n",
       " 'Scaled Dot-Product Attention computes attention by taking the dot products of queries and keys, scaling by the square root of the key dimension, and applying a softmax to derive weights for the values. The formula is Attention(Q,K,V) = softmax(QKT/dk)V. This method is faster and more efficient than additive attention, especially for larger dimensions. Multi-Head Attention enhances this by projecting queries, keys, and values multiple times (h heads) and performing attention in parallel, allowing the model to capture information from different representation subspaces. In this implementation, h = 8, with each head having a dimension of 64.',\n",
       " 'Scaled Dot-Product Attention computes attention by taking dot products of queries and keys, scaling by the square root of the key dimension, and applying a softmax to derive weights for values. It is more efficient than additive attention, especially for larger dimensions. Multi-Head Attention enhances this by projecting queries, keys, and values into different subspaces and performing attention in parallel across multiple heads, allowing the model to capture diverse information. In this implementation, 8 parallel attention heads are used, each with a reduced dimensionality, maintaining computational efficiency similar to single-head attention.',\n",
       " 'Scaled Dot-Product Attention computes attention weights by taking the dot product of queries and keys, scaling by the square root of the key dimension, and applying a softmax function to the resulting values. This method is efficient due to its use of matrix multiplication. Multi-Head Attention enhances this by projecting queries, keys, and values into multiple subspaces, allowing the model to attend to different information simultaneously. In this implementation, eight parallel attention heads are used, each with reduced dimensions, maintaining computational efficiency similar to single-head attention.',\n",
       " 'Scaled Dot-Product Attention computes attention by taking the dot products of queries and keys, scaling them, and applying a softmax function to derive weights for the values. It is faster and more space-efficient than additive attention, particularly for larger dimensions. Multi-Head Attention enhances this by projecting queries, keys, and values into multiple subspaces, allowing the model to focus on different information simultaneously. In this implementation, eight parallel attention heads are used, each with reduced dimensions, maintaining similar computational costs to single-head attention.',\n",
       " 'Scaled Dot-Product Attention computes attention by taking the dot product of queries and keys, scaling by the square root of the key dimension, applying softmax to obtain weights, and using these weights on values. It is more efficient than additive attention, especially for larger dimensions, due to its reliance on optimized matrix multiplication. Multi-Head Attention enhances this by projecting queries, keys, and values into multiple subspaces, allowing the model to capture diverse information simultaneously. The implementation uses eight parallel attention heads, each with reduced dimensions, maintaining computational efficiency comparable to single-head attention.',\n",
       " 'Scaled Dot-Product Attention computes attention by taking the dot products of queries and keys, scaling by the square root of their dimension, and applying a softmax to obtain weights for values. This method is efficient and faster than additive attention, especially for larger dimensions. Multi-Head Attention enhances this by projecting queries, keys, and values into multiple subspaces and performing attention in parallel, allowing the model to capture diverse information. In this implementation, eight parallel attention heads are used, with each head having reduced dimensions to maintain computational efficiency.',\n",
       " 'Scaled Dot-Product Attention computes attention weights by taking the dot product of queries and keys, scaling by the square root of the key dimension, and applying softmax to obtain output values. It is faster and more space-efficient than additive attention, especially for larger dimensions. Multi-Head Attention enhances this by using multiple parallel attention heads, allowing the model to focus on different representation subspaces, with each head processing reduced dimensions. The final output is obtained by concatenating the results of all heads and projecting them to the desired output dimension. In this implementation, eight heads are used, each with a dimension of 64.',\n",
       " 'Scaled Dot-Product Attention computes the attention function using queries, keys, and values, with a scaling factor applied to the dot products to improve performance and prevent large gradients in the softmax function. Multi-Head Attention enhances this by projecting the queries, keys, and values into multiple subspaces and performing attention in parallel, allowing for richer representation and joint attention to different information. The implementation uses eight parallel attention heads, each with reduced dimensions, maintaining computational efficiency similar to single-head attention.',\n",
       " 'The Transformer model employs multi-head attention in three ways: \\n1. Encoder-decoder attention allows the decoder to access all input positions via queries from the decoder and keys/values from the encoder.\\n2. Self-attention in the encoder lets each position attend to all previous positions within the encoder.\\n3. Self-attention in the decoder enables positions to attend to all prior positions, with masking to prevent leftward information flow.\\n\\nAdditionally, each encoder and decoder layer features a position-wise feed-forward network (FFN) with two linear transformations and a ReLU activation, using different parameters across layers but the same transformations for each position. The model also uses learned embeddings to convert input and output tokens into vectors, sharing weight matrices between embedding layers and the pre-softmax transformation.',\n",
       " \"The Transformer model employs multi-head attention in three key ways: \\n\\n1. **Encoder-Decoder Attention**: Queries originate from the decoder, while keys and values come from the encoder output, allowing the decoder to consider all input positions.\\n2. **Self-Attention in Encoder**: All queries, keys, and values are derived from the encoder's previous layer, enabling each position to attend to all preceding positions.\\n3. **Self-Attention in Decoder**: Similar to the encoder, but with masking to prevent leftward information flow, maintaining the auto-regressive property.\\n\\nAdditionally, each encoder and decoder layer includes a position-wise feed-forward network consisting of two linear transformations with a ReLU activation. Input and output dimensions are 512, while the inner layer's dimension is 2048. The model also uses learned embeddings for input and output tokens, sharing weight matrices between embedding layers and the pre-softmax linear transformation, with embeddings scaled by dmodel.\",\n",
       " 'The Transformer model employs multi-head attention in three ways: \\n\\n1. Encoder-decoder attention allows the decoder to attend to all input positions using queries from the decoder and keys/values from the encoder.\\n2. Self-attention in the encoder enables each position to attend to all previous positions within the encoder.\\n3. Self-attention in the decoder permits each position to attend to all prior positions, with masking to prevent leftward information flow.\\n\\nAdditionally, each encoder and decoder layer contains a position-wise feed-forward network with two linear transformations and a ReLU activation, maintaining identical transformations across positions but varying parameters across layers. The model uses learned embeddings for input and output tokens, sharing weight matrices between embedding layers and the pre-softmax transformation.',\n",
       " \"The Transformer model employs multi-head attention in three key ways: \\n\\n1. **Encoder-Decoder Attention**: Queries from the decoder attend to keys and values from the encoder, allowing the decoder to access all input sequence positions.\\n2. **Self-Attention in the Encoder**: All keys, values, and queries originate from the encoder's previous layer, enabling each position to attend to all prior positions.\\n3. **Self-Attention in the Decoder**: Each position can attend to all previous positions, with masking to prevent leftward information flow, preserving the auto-regressive property.\\n\\nAdditionally, each encoder and decoder layer includes a position-wise feed-forward network (FFN) consisting of two linear transformations with a ReLU activation, applied identically across positions but with different parameters per layer. The model also uses learned embeddings to convert input and output tokens to vectors, sharing weights between embedding layers and the pre-softmax linear transformation, scaled by dmodel.\",\n",
       " 'The Transformer model employs multi-head attention in three key ways: \\n1. Encoder-decoder attention allows the decoder to attend to all encoder positions, using queries from the decoder and keys/values from the encoder.\\n2. Self-attention in the encoder enables each position to attend to all previous encoder positions.\\n3. Self-attention in the decoder allows each position to attend to all previous decoder positions, with masking to prevent leftward information flow.\\n\\nAdditionally, each layer in the encoder and decoder includes a feed-forward network (FFN) with two linear transformations and a ReLU activation, where the input and output dimensions are 512 and 2048, respectively. The model also utilizes learned embeddings to convert tokens to vectors and shares a weight matrix between embedding layers and the pre-softmax transformation.',\n",
       " 'The Transformer model employs multi-head attention in three ways: encoder-decoder attention, where decoder queries attend to encoder outputs; self-attention in the encoder, allowing each position to attend to all previous positions; and self-attention in the decoder, which prevents leftward information flow by masking illegal connections. Additionally, each layer in the encoder and decoder features a position-wise feed-forward network, consisting of two linear transformations with a ReLU activation, applied identically across positions. The model also uses learned embeddings to convert input and output tokens into vectors and shares weight matrices between embedding layers and the pre-softmax linear transformation.',\n",
       " 'The Transformer model employs multi-head attention in three primary ways: \\n\\n1. Encoder-decoder attention layers allow the decoder to attend to all encoder outputs.\\n2. Self-attention layers in the encoder enable each position to attend to all previous positions within the encoder.\\n3. Self-attention layers in the decoder restrict attention to current and previous positions to maintain the auto-regressive property.\\n\\nAdditionally, each encoder and decoder layer includes a position-wise feed-forward network, consisting of two linear transformations with a ReLU activation, applied independently at each position. The model uses learned embeddings to convert input and output tokens into vectors and shares weight matrices between the embedding layers and the pre-softmax linear transformation.',\n",
       " \"The Transformer model employs multi-head attention in three ways: \\n\\n1. **Encoder-Decoder Attention**: Allows decoder positions to attend to all input positions using queries from the decoder and keys/values from the encoder.\\n2. **Self-Attention in Encoder**: All keys, values, and queries come from the encoder's previous layer, enabling each position to attend to all prior positions.\\n3. **Self-Attention in Decoder**: Similar to encoder self-attention but restricts information flow to maintain the auto-regressive property by masking illegal connections.\\n\\nAdditionally, each encoder and decoder layer contains a position-wise feed-forward network (FFN) with two linear transformations and a ReLU activation, using parameters specific to each layer. The model also employs learned embeddings to convert input and output tokens into vectors, sharing weight matrices between embedding layers and the pre-softmax linear transformation.\",\n",
       " \"The Transformer model employs multi-head attention in three key ways: \\n\\n1. **Encoder-Decoder Attention**: Queries from the decoder attend to memory keys and values from the encoder, facilitating cross-sequence attention.\\n2. **Self-Attention in Encoder**: All keys, values, and queries originate from the encoder's previous layer, allowing each position to attend to all previous positions.\\n3. **Self-Attention in Decoder**: Each decoder position attends to all prior positions, with masking to maintain the auto-regressive property.\\n\\nAdditionally, each layer includes a position-wise feed-forward network consisting of two linear transformations with ReLU activation, using different parameters across layers. The model uses learned embeddings for input and output tokens, sharing weight matrices between embedding layers and the pre-softmax transformation. The input and output dimensions are set to 512 and 2048, respectively.\",\n",
       " 'The Transformer model employs multi-head attention in three ways: encoder-decoder attention, where the decoder queries attend to encoder outputs; self-attention in the encoder, allowing positions to attend to previous encoder outputs; and self-attention in the decoder, which maintains the auto-regressive property by masking future positions. Additionally, each layer includes a position-wise feed-forward network with two linear transformations and a ReLU activation, maintaining consistent parameters across positions but varying across layers. The model uses learned embeddings for input and output tokens, sharing weights between embedding layers and the pre-softmax transformation, scaled by dmodel.',\n",
       " 'The Transformer model employs multi-head attention in three main ways: \\n\\n1. **Encoder-Decoder Attention**: Queries are derived from the decoder, while keys and values come from the encoder, enabling the decoder to attend to all input positions.\\n2. **Self-Attention in Encoder**: All keys, values, and queries come from the same encoder layer, allowing each position to attend to all previous positions within the encoder.\\n3. **Self-Attention in Decoder**: Each decoder position can attend to all previous positions, with masking applied to prevent leftward information flow, maintaining the auto-regressive property.\\n\\nAdditionally, each encoder and decoder layer includes a position-wise feed-forward network (FFN) that applies two linear transformations with a ReLU activation, maintaining the same linear transformations across positions but using different parameters for each layer. The model also utilizes learned embeddings for input and output tokens, sharing weight matrices between embedding layers and the pre-softmax linear transformation.',\n",
       " 'Positional encoding is essential for models without recurrence or convolution to understand token order in sequences. It involves adding encodings to input embeddings, using sine and cosine functions of varying frequencies to allow the model to learn relative positions. The sinusoidal approach was preferred over learned embeddings for its ability to generalize to longer sequences. Self-attention layers are compared to recurrent and convolutional layers in terms of computational complexity, parallelization, and learning long-range dependencies. Self-attention is more efficient for shorter sequences and can be adapted for long sequences by restricting attention to a neighborhood size. Convolutional layers require more operations and stack complexity, while self-attention offers potential for more interpretable models through attention distributions.',\n",
       " 'Positional encoding is essential in models without recurrence or convolution to incorporate the order of tokens in a sequence. It is added to input embeddings using sine and cosine functions of varying frequencies, allowing the model to learn relative positioning effectively. The sinusoidal method was preferred over learned embeddings for its ability to generalize to longer sequences. Self-attention layers are compared to recurrent and convolutional layers regarding computational complexity, parallelization, and path length for learning long-range dependencies. Self-attention is faster and more efficient for shorter sequences, while convolutional layers require more operations and longer paths. Overall, self-attention offers advantages in interpretability and task-specific learning.',\n",
       " 'Positional encoding is essential in models without recurrence or convolution to incorporate token order in sequences. The encodings, added to input embeddings, can be learned or fixed, with the sinusoidal method being preferred for its ability to generalize to longer sequences. Self-attention layers are compared to recurrent and convolutional layers in terms of computational complexity, parallelization, and path length for learning long-range dependencies. Self-attention offers constant sequential operations and faster performance for shorter sequences, while convolutional layers require more layers for full connectivity. Overall, self-attention may also lead to more interpretable models.',\n",
       " \"The section discusses the need for positional encodings in models without recurrence or convolution to incorporate token order. It describes the use of sine and cosine functions for positional encodings, which allow the model to generalize to longer sequences. A comparison of self-attention with recurrent and convolutional layers highlights self-attention's advantages in computational complexity, parallelization, and learning long-range dependencies. Self-attention connects all input-output positions with constant operations, while recurrent layers require linear operations, making self-attention faster for shorter sequences. Convolutional layers, while generally more expensive, can be optimized with separable convolutions. The section concludes with the potential interpretability benefits of self-attention models.\",\n",
       " \"The section discusses the necessity of positional encodings in a model that lacks recurrence and convolution to retain the order of sequences. It details the use of sine and cosine functions for these encodings, allowing the model to learn relative positions effectively. A comparison of self-attention layers with recurrent and convolutional layers highlights self-attention's advantages in computational complexity, parallelization, and the ability to learn long-range dependencies. Self-attention layers require fewer sequential operations and are more efficient for shorter sequences. The text also notes the potential for self-attention to yield interpretable models by analyzing attention distributions.\",\n",
       " \"The text discusses the use of positional encodings in models without recurrence or convolution to incorporate token order information. It describes a sinusoidal method for positional encoding, which allows the model to learn relative positions effectively and extrapolate to longer sequences. The comparison of self-attention, recurrent, and convolutional layers highlights self-attention's advantages in computational complexity and path length for learning long-range dependencies. Self-attention connects all positions with constant operations, making it more efficient than recurrent layers for smaller sequences. Convolutional layers require more operations and can be less efficient, especially for longer sequences. Self-attention may also lead to more interpretable models.\",\n",
       " 'Positional encodings are added to input embeddings in models without recurrence or convolution to convey token order. These encodings, using sine and cosine functions of varying frequencies, allow the model to learn relative positions effectively. The choice of sinusoidal encodings is preferred for their potential to generalize to longer sequences, although learned embeddings yield similar results. The text also discusses the advantages of self-attention layers over recurrent and convolutional layers, highlighting their computational efficiency, parallelization capabilities, and shorter path lengths for learning long-range dependencies. Self-attention connects all positions with minimal sequential operations, outperforming recurrent layers in scenarios where the sequence length is less than the representation dimensionality. Convolutional layers, while less efficient, can be improved with separable convolutions, but still do not match the performance of self-attention in terms of path lengths and complexity. Self-attention models may also provide better interpretability through attention distributions.',\n",
       " 'Positional encoding is introduced in models without recurrence or convolution to incorporate token order information. It involves adding sinusoidal functions to input embeddings, allowing the model to learn relative positions effectively. The study compares self-attention, recurrent, and convolutional layers regarding computational complexity, parallelization, and long-range dependency learning. Self-attention layers are more efficient than recurrent layers for shorter sequences, while convolutional layers require more operations and may not connect all input-output pairs directly. Self-attention also offers potential for interpretability in model behavior.',\n",
       " 'Positional encodings are added to input embeddings in models without recurrence or convolution to incorporate token order. The encodings are derived from sine and cosine functions, allowing the model to learn relative positions effectively. Self-attention layers are compared to recurrent and convolutional layers based on computational complexity, parallelization, and path length for learning long-range dependencies. Self-attention has constant sequential operations and is computationally faster than recurrent layers for shorter sequences. Convolutional layers require more operations and increase path lengths, while self-attention may provide more interpretable models through distinct attention distributions.',\n",
       " 'The section discusses the necessity of positional encodings in models lacking recurrence and convolution to utilize token order in sequences. It describes the use of sine and cosine functions for generating positional encodings, which allow the model to learn relative positions effectively. The text compares self-attention layers with recurrent and convolutional layers regarding computational complexity, parallelization, and the ability to learn long-range dependencies. Self-attention is noted to have constant sequential operations, making it faster for shorter sequences, while convolutional layers require more layers for full connectivity, increasing complexity. The authors also highlight the potential for self-attention to create more interpretable models through attention distributions.',\n",
       " 'Positional encoding is essential for models without recurrence or convolution to understand token order, achieved by adding encodings to input embeddings. These encodings, using sine and cosine functions of varying frequencies, help the model learn relative positions. Self-attention layers are compared with recurrent and convolutional layers, highlighting their computational efficiency, parallelization capability, and ability to learn long-range dependencies. Self-attention connects all positions with constant sequential operations, while recurrent layers require linear operations based on sequence length. Convolutional layers connect positions less efficiently, needing multiple layers for full connectivity. Overall, self-attention is faster for shorter sequences and offers potential for more interpretable models.',\n",
       " \"The text discusses the importance of positional encoding in models without recurrence or convolution, emphasizing the need to incorporate position information for sequence processing. It describes the use of sine and cosine functions for positional encodings, which allow the model to learn relative positions effectively. The comparison of self-attention layers with recurrent and convolutional layers highlights self-attention's advantages in computational complexity, parallelization, and learning long-range dependencies. Self-attention requires fewer sequential operations and can be more efficient than recurrent layers, especially with shorter sequences. The text also notes that self-attention layers can provide more interpretable models, as they can reveal how different attention heads learn various tasks related to sentence structure.\",\n",
       " \"Positional encodings are added to input embeddings in models without recurrence or convolution to convey token order. These encodings use sine and cosine functions of varying frequencies, allowing the model to learn relative positions. The sinusoidal method was preferred over learned embeddings for its potential to generalize to longer sequences. The text compares self-attention with recurrent and convolutional layers, highlighting self-attention's lower computational complexity, ability to parallelize operations, and shorter path lengths for learning long-range dependencies. Self-attention connects all positions with constant operations, while recurrent layers require linear operations, making self-attention more efficient for shorter sequences. Convolutional layers have increased path lengths due to their kernel sizes, and self-attention may provide more interpretable models through attention distributions.\",\n",
       " 'The section discusses the necessity of positional encoding in models lacking recurrence and convolution to incorporate token order. It highlights the use of sine and cosine functions for positional encodings, allowing the model to learn relative positions effectively. The text compares self-attention layers with recurrent and convolutional layers in terms of computational complexity, parallelization, and path lengths for learning long-range dependencies. Self-attention layers offer constant sequential operations and are computationally more efficient for shorter sequences, while convolutional layers require more operations and longer paths for connecting input and output positions. The text suggests that self-attention may also lead to more interpretable models.',\n",
       " \"The section discusses the importance of positional encoding in models without recurrence or convolution to utilize token order in sequences. It describes the use of sine and cosine functions for positional encodings, enabling the model to learn relative positions effectively. The comparison of self-attention layers with recurrent and convolutional layers highlights self-attention's advantages in computational complexity, parallelization, and learning long-range dependencies. Self-attention connects all positions with fewer sequential operations, making it faster for shorter sequences, while convolutional layers require more layers to connect all positions and are generally more expensive. The text concludes that self-attention may also improve model interpretability through attention distributions.\",\n",
       " 'The text discusses positional encoding in models without recurrence or convolution, emphasizing the need to incorporate token order. It describes the use of sine and cosine functions for positional encodings, which allows the model to learn relative positions effectively. The comparison of self-attention layers with recurrent and convolutional layers highlights advantages in computational complexity and the ability to learn long-range dependencies. Self-attention layers require fewer sequential operations and are faster for shorter sequences, while convolutional layers can be more complex. The text suggests that self-attention may lead to more interpretable models, with attention heads learning distinct tasks related to sentence structure.',\n",
       " 'The section discusses the importance of positional encoding in models without recurrence or convolution to maintain the order of tokens in sequences. It introduces sine and cosine functions for positional encodings, which allow the model to learn relative positions effectively. The comparison of self-attention layers with recurrent and convolutional layers highlights their computational efficiency, parallelization capability, and ability to learn long-range dependencies. Self-attention layers require fewer sequential operations and are faster when the sequence length is shorter than the representation dimensionality. The text also notes that self-attention can lead to more interpretable models, as attention heads can learn different tasks related to sentence structure.',\n",
       " 'The section discusses the importance of positional encodings in models without recurrence or convolution to capture token order. It describes the use of sine and cosine functions for positional encodings, allowing the model to learn relative positions effectively. The comparison of self-attention layers with recurrent and convolutional layers highlights that self-attention offers lower computational complexity and allows for easier learning of long-range dependencies. Self-attention layers maintain constant sequential operations, while recurrent layers require linear operations, making self-attention preferable for tasks involving shorter sequences. Additionally, it notes that self-attention can provide more interpretable models by analyzing attention distributions.',\n",
       " 'The training regime for the models utilized the WMT 2014 English-German dataset with 4.5 million sentence pairs and the larger English-French dataset with 36 million sentence pairs. Models were trained on 8 NVIDIA P100 GPUs, with base models taking 12 hours for 100,000 steps and big models 3.5 days for 300,000 steps. The Adam optimizer was used with a specific learning rate schedule and warmup steps of 4000. Regularization techniques included residual dropout at a rate of 0.1 and label smoothing at 0.1. The Transformer model outperformed previous state-of-the-art models in BLEU scores for English-to-German and English-to-French translations while requiring less training cost.',\n",
       " 'The training section outlines the training process for models using WMT 2014 English-German and English-French datasets, with batch sizes approximating 25,000 tokens for both source and target. Training was conducted on 8 NVIDIA P100 GPUs, with base models taking 12 hours (100,000 steps) and big models taking 3.5 days (300,000 steps). The Adam optimizer was used with a learning rate schedule involving warmup steps. Regularization techniques included residual dropout and label smoothing. The Transformer model achieved superior BLEU scores compared to previous models at lower training costs.',\n",
       " 'The training section outlines the regimen for models using the WMT 2014 English-German and English-French datasets, with 4.5 million and 36 million sentence pairs respectively. Training involved batching sentences by sequence length, utilizing 8 NVIDIA P100 GPUs, with base models trained for 100,000 steps in 12 hours and big models for 300,000 steps in 3.5 days. The Adam optimizer was employed with a learning rate schedule and warmup steps of 4000. Regularization techniques included residual dropout and label smoothing. The Transformer model achieved superior BLEU scores compared to previous models at a lower training cost, with the base model scoring 27.3 for EN-DE and 38.1 for EN-FR, and the big model scoring 28.4 and 41.0 respectively.',\n",
       " 'The training regime for the models utilized the WMT 2014 English-German dataset with 4.5 million sentence pairs and the larger English-French dataset with 36 million sentences. Training was conducted on 8 NVIDIA P100 GPUs, with base models trained for 100,000 steps in 12 hours and big models for 300,000 steps over 3.5 days. The Adam optimizer was used with a specific learning rate schedule and regularization techniques including residual dropout and label smoothing. The Transformer model achieved superior BLEU scores compared to previous models at a lower training cost.',\n",
       " 'The training regime for the models utilized the WMT 2014 English-German dataset with 4.5 million sentence pairs and the larger WMT 2014 English-French dataset with 36 million sentences. Models were trained on 8 NVIDIA P100 GPUs, with base models taking 12 hours and big models 3.5 days. The Adam optimizer was used with a specific learning rate schedule and regularization techniques including residual dropout and label smoothing. The Transformer model outperformed previous state-of-the-art models in BLEU scores for English-to-German and English-to-French translations while using less training cost (FLOPs).',\n",
       " 'The training regime for the models involved the WMT 2014 English-German dataset with 4.5 million sentence pairs and the larger English-French dataset with 36 million sentence pairs, using byte-pair and word-piece encoding respectively. Training was conducted on 8 NVIDIA P100 GPUs, with base models trained for 100,000 steps in 12 hours, while big models took 300,000 steps over 3.5 days. The Adam optimizer was used with a specific learning rate schedule and warmup steps. Regularization techniques included residual dropout and label smoothing. The Transformer models achieved superior BLEU scores compared to previous state-of-the-art models while requiring less training cost in FLOPs.',\n",
       " 'The training regime for the models involved using the WMT 2014 English-German dataset with 4.5 million sentence pairs and the larger English-French dataset with 36 million sentence pairs. Training utilized 8 NVIDIA P100 GPUs, with base models trained for 100,000 steps in 12 hours and big models for 300,000 steps in 3.5 days. The Adam optimizer was used with a specific learning rate schedule and regularization techniques including residual dropout and label smoothing. The Transformer model achieved superior BLEU scores compared to previous models while requiring less training cost in terms of FLOPs.',\n",
       " 'The training regime for the models utilized the WMT 2014 English-German and English-French datasets, with 4.5 million and 36 million sentence pairs respectively. Models were trained on 8 NVIDIA P100 GPUs, with base models taking 100,000 steps over 12 hours, and big models taking 300,000 steps over 3.5 days. The Adam optimizer was used with a specific learning rate schedule and regularization techniques including residual dropout and label smoothing. The Transformer model outperformed previous state-of-the-art models in BLEU scores for both language pairs while being more cost-effective in terms of training.',\n",
       " 'The training regime for the models utilized the WMT 2014 English-German dataset (4.5 million sentence pairs) and the larger English-French dataset (36 million sentence pairs). Training was conducted on a machine with 8 NVIDIA P100 GPUs, with base models trained for 100,000 steps over 12 hours and big models for 300,000 steps over 3.5 days. The Adam optimizer was used with a variable learning rate, and three types of regularization were applied: residual dropout, label smoothing, and dropout in sub-layers. The Transformer model outperformed previous state-of-the-art models in BLEU scores for both English-to-German and English-to-French translations while requiring less training cost in FLOPs.',\n",
       " 'The training regime for the models involved using the WMT 2014 English-German dataset with 4.5 million sentence pairs and the larger English-French dataset with 36 million sentences. Models were trained on 8 NVIDIA P100 GPUs, with base models taking 100,000 steps (12 hours) and big models 300,000 steps (3.5 days). The Adam optimizer was used with a specific learning rate schedule and warmup steps of 4000. Regularization techniques included residual dropout with a rate of 0.1 and label smoothing of 0.1. The Transformer model outperformed previous state-of-the-art models in BLEU scores for both English-to-German and English-to-French tasks while requiring less training cost.',\n",
       " 'The training regime for the models utilized the WMT 2014 English-German dataset with 4.5 million sentence pairs and the larger English-French dataset with 36 million pairs, using byte-pair and word-piece encoding respectively. Training was conducted on 8 NVIDIA P100 GPUs, with base models trained for 100,000 steps in 12 hours and big models for 300,000 steps in 3.5 days. The Adam optimizer was employed with a specific learning rate schedule and regularization techniques including residual dropout and label smoothing. The Transformer model achieved superior BLEU scores compared to previous models while requiring less training cost in FLOPs.',\n",
       " 'The training regime for the models involved using the WMT 2014 English-German dataset with 4.5 million sentence pairs and the larger English-French dataset with 36 million sentences, both employing specific token encoding methods. Training utilized 8 NVIDIA P100 GPUs, with base models trained for 100,000 steps in 12 hours, while big models were trained for 300,000 steps over 3.5 days. The Adam optimizer was used with a learning rate schedule involving warmup steps. Regularization techniques included residual dropout and label smoothing. The Transformer model achieved superior BLEU scores compared to previous state-of-the-art models at a lower training cost, with the base model scoring 27.3 for English-German and 38.1 for English-French, while the big model scored 28.4 and 41.0, respectively.',\n",
       " 'The training regime for the models utilized the WMT 2014 English-German dataset with 4.5 million sentence pairs and the larger English-French dataset with 36 million sentences. Training was conducted on 8 NVIDIA P100 GPUs, with base models trained for 100,000 steps over 12 hours and big models for 300,000 steps over 3.5 days. The Adam optimizer was employed with a varied learning rate, and three types of regularization were applied: residual dropout, label smoothing, and dropout on embeddings and positional encodings. The Transformer models achieved superior BLEU scores compared to previous state-of-the-art models while utilizing less training cost in terms of FLOPs.',\n",
       " 'The training section outlines the training process for models using the WMT 2014 English-German and English-French datasets, with the former consisting of 4.5 million sentence pairs and the latter 36 million. Models were trained on 8 NVIDIA P100 GPUs, with base models taking 100,000 steps over 12 hours, and big models taking 300,000 steps over 3.5 days. The Adam optimizer was used with a specific learning rate schedule and warmup steps. Regularization techniques included residual dropout and label smoothing. The Transformer model achieved superior BLEU scores compared to previous state-of-the-art models while requiring less training cost in terms of FLOPs.',\n",
       " 'The training section outlines the model training process, utilizing the WMT 2014 English-German and English-French datasets, with 4.5 million and 36 million sentence pairs respectively. The training employed 8 NVIDIA P100 GPUs, with base models trained for 100,000 steps in 12 hours and big models for 300,000 steps over 3.5 days. The Adam optimizer was used with a specific learning rate schedule and warmup steps. Regularization techniques included residual dropout and label smoothing. The Transformer model achieved superior BLEU scores compared to previous models while maintaining lower training costs.',\n",
       " 'The training regime for the models involved using the WMT 2014 English-German and English-French datasets, with approximately 4.5 million and 36 million sentence pairs respectively. The models were trained on 8 NVIDIA P100 GPUs, with base models taking about 12 hours for 100,000 steps and big models taking 3.5 days for 300,000 steps. The Adam optimizer was utilized with a variable learning rate and a warmup period of 4,000 steps. Regularization techniques included residual dropout at a rate of 0.1 and label smoothing at 0.1. The Transformer models achieved superior BLEU scores compared to previous state-of-the-art models while requiring less training cost in FLOPs.',\n",
       " 'The big transformer model significantly outperformed previous models in the WMT 2014 English-to-German and English-to-French translation tasks, achieving BLEU scores of 28.4 and 41.0, respectively. The training for the big model took 3.5 days on 8 P100 GPUs, with a lower training cost compared to prior state-of-the-art models. Variations in the transformer architecture were tested, showing that performance can vary with the number of attention heads and dimensions, with single-head attention resulting in lower BLEU scores. The results and configurations are detailed in Tables 2 and 3.',\n",
       " 'The big transformer model significantly outperforms previous models in the WMT 2014 English-to-German and English-to-French translation tasks, achieving BLEU scores of 28.4 and 41.0, respectively, while requiring less training cost. The base model also surpasses all prior models. Various configurations of the transformer architecture were tested, showing that performance varies with the number of attention heads and dimensions, with optimal settings yielding the best results.',\n",
       " \"The big transformer model significantly outperforms previous models in both English-to-German and English-to-French translation tasks, achieving BLEU scores of 28.4 and 41.0, respectively, while requiring less training time and cost. Variations in the model's architecture were tested, revealing that changes in the number of attention heads and dimensions affect translation quality, with a decrease in performance noted when using too few or too many attention heads. The results of these variations are detailed in Table 3.\",\n",
       " 'The big transformer model achieved a new state-of-the-art BLEU score of 28.4 on the WMT 2014 English-to-German translation task, outperforming previous models by over 2.0 BLEU, with training on 8 P100 GPUs for 3.5 days. For the English-to-French task, it scored 41.0, also surpassing prior models at a significantly lower training cost. The base models used checkpoint averaging for performance evaluation, while variations in model architecture showed that the number of attention heads and dimensions affected translation quality.',\n",
       " 'The big transformer model outperforms previous models in English-to-German and English-to-French translation tasks, achieving BLEU scores of 28.4 and 41.0, respectively. The training for these models was completed in 3.5 days using 8 P100 GPUs, with the big model costing less than 1/4 of the previous state-of-the-art for French. Variations in model architecture were tested, showing that performance varies with the number of attention heads and dimensions, with specific configurations yielding different BLEU scores.',\n",
       " \"The big transformer model significantly outperforms previous models in the WMT 2014 English-to-German and English-to-French translation tasks, achieving BLEU scores of 28.4 and 41.0, respectively. The model's training was efficient, taking 3.5 days on 8 P100 GPUs, and it surpassed prior models at a lower training cost. Various configurations of the transformer were tested, revealing that the number of attention heads and dimensions affected performance, with single-head attention performing worse. The results, including variations on the transformer architecture, are detailed in accompanying tables.\",\n",
       " 'The big transformer model significantly outperforms previous models in the WMT 2014 English-to-German and English-to-French translation tasks, achieving BLEU scores of 28.4 and 41.0, respectively. It does so with reduced training costs, taking 3.5 days on 8 P100 GPUs for the German task. Variations in model components were tested, revealing that single-head attention performed worse, and optimal configurations were established for attention heads and dimensions. The results are detailed in tables comparing translation quality and training costs.',\n",
       " \"The big transformer model significantly outperforms previous models in the WMT 2014 English-to-German and English-to-French translation tasks, achieving BLEU scores of 28.4 and 41.0, respectively. It establishes new state-of-the-art scores while using less training cost and time. Variations in the model's architecture were tested, showing that adjustments to attention heads and dimensions affected performance, with optimal configurations yielding improved BLEU scores.\",\n",
       " 'The big transformer model significantly outperformed previous models in the WMT 2014 English-to-German and English-to-French translation tasks, achieving BLEU scores of 28.4 and 41.0, respectively. It required 3.5 days of training on 8 P100 GPUs and had lower training costs compared to earlier state-of-the-art models. Variations in the model architecture were tested, showing that adjustments to attention heads and dimensions affected performance, with optimal configurations yielding better BLEU scores on the English-to-German translation task.',\n",
       " 'The big transformer model significantly outperforms previous models on the WMT 2014 English-to-German and English-to-French translation tasks, achieving BLEU scores of 28.4 and 41.0, respectively. It also demonstrates lower training costs compared to prior state-of-the-art models. Variations in the model architecture were tested, showing that single-head attention performed worse and that the number of attention heads affects translation quality. Detailed results and comparisons are provided in Tables 2 and 3.',\n",
       " \"The big transformer model significantly outperforms previous models in English-to-German and English-to-French translation tasks, achieving BLEU scores of 28.4 and 41.0, respectively, while using less training cost. The model's configuration and training details are provided, highlighting the efficiency of the architecture. Variations in model components were tested, showing that while single-head attention reduces performance, there is a drop-off with excessive attention heads. Table 3 details the performance metrics for different model variations on the English-to-German translation development set.\",\n",
       " 'The big transformer model achieved state-of-the-art BLEU scores of 28.4 in English-to-German and 41.0 in English-to-French translation, outperforming previous models by over 2.0 and achieving lower training costs. The configurations for these models are detailed, including variations in attention heads and dimensions that were tested for performance changes. The results indicate that while single-head attention performed worse, quality diminishes with excessive heads. The training utilized 8 P100 GPUs over 3.5 days, and various hyperparameters were optimized through experimentation.',\n",
       " 'The table indicates that reducing the attention key size negatively impacts model quality, suggesting a need for a more complex compatibility function. Larger models and dropout techniques improve performance and mitigate overfitting. The Transformer model, built solely on attention mechanisms, outperforms traditional recurrent architectures in translation tasks, achieving state-of-the-art results for English-to-German and English-to-French translations. Future research will explore applications beyond text and investigate efficient attention mechanisms for handling large inputs like images and audio. The code for model training and evaluation is available online.',\n",
       " \"The table indicates that reducing the attention key size negatively impacts model quality, suggesting the need for a more complex compatibility function. Larger models and dropout are beneficial for performance, while learned positional embeddings yield results similar to the base model. The conclusion highlights the Transformer model's efficiency in sequence transduction tasks, achieving state-of-the-art results in translation benchmarks, and outlines future research directions to explore attention-based models in various domains beyond text.\",\n",
       " 'The text discusses the performance of the Transformer model, which relies solely on attention mechanisms instead of recurrent layers, achieving state-of-the-art results in translation tasks. Key findings include that reducing the attention key size adversely affects model quality, larger models perform better, and dropout helps prevent overfitting. The study suggests future research directions, including applying the Transformer to various input and output modalities and improving efficiency for large data inputs. The code used for training and evaluation is available online.',\n",
       " 'The text discusses the performance of the Transformer model, which utilizes attention mechanisms instead of recurrent layers for sequence transduction tasks. It highlights findings from experiments showing that reducing attention key size negatively impacts model quality, while larger models and dropout techniques improve performance. The Transformer achieves state-of-the-art results in English-to-German and English-to-French translation tasks, with plans to extend its application beyond text. The authors express gratitude for contributions from colleagues and provide references for further reading.',\n",
       " 'The table highlights that reducing the attention key size negatively impacts model quality, indicating the need for a more complex compatibility function than the dot product. It also shows that larger models and dropout techniques enhance performance and prevent overfitting. The Transformer model, which relies solely on attention mechanisms instead of recurrent layers, achieves state-of-the-art results in English-to-German and English-to-French translation tasks, with plans to extend its application to other modalities and improve efficiency in handling large inputs. The code for the model is publicly available.',\n",
       " 'The analysis in Table 3 indicates that reducing the attention key size negatively impacts model quality, highlighting the need for a more complex compatibility function than the dot product. Larger models and dropout techniques improve performance and mitigate overfitting. The Transformer model, which relies solely on attention mechanisms instead of recurrent layers, demonstrates significantly faster training and achieves state-of-the-art results in English-to-German and English-to-French translation tasks. Future research will explore applications beyond text, including images, audio, and video, and aims to reduce sequential generation. The training code is available on GitHub.',\n",
       " 'The text discusses the performance and findings related to the Transformer model, which utilizes attention mechanisms instead of recurrent layers for sequence transduction. Key observations include the detrimental effect of reducing attention key size on model quality, the advantages of larger models and dropout in preventing overfitting, and the effectiveness of learned positional embeddings. The Transformer achieves state-of-the-art results in English-to-German and English-to-French translation tasks and is noted for its faster training compared to traditional models. Future research aims to apply attention models to other modalities and improve efficiency in handling large inputs and outputs.',\n",
       " \"The text discusses the performance and architecture of the Transformer model, which utilizes attention mechanisms instead of recurrent layers for sequence transduction. Key findings indicate that reducing attention key size negatively impacts model quality, while larger models and dropout techniques improve performance. The Transformer achieves state-of-the-art results in translation tasks, outperforming previous models. Future research aims to extend the model's application beyond text to other modalities, such as images and audio, and to enhance efficiency in handling large inputs. The code for the model is publicly available.\",\n",
       " 'The table indicates that reducing the attention key size negatively impacts model quality, suggesting a more complex compatibility function may be needed. Larger models and dropout techniques improve performance and prevent overfitting. The Transformer model, which relies solely on attention mechanisms instead of recurrent layers, achieves state-of-the-art results in English-to-German and English-to-French translation tasks, demonstrating faster training times. Future research will explore applying attention-based models to other modalities beyond text and improving efficiency for larger inputs like images and audio. Acknowledgments are given to contributors for their insights.',\n",
       " 'The table indicates that reducing the attention key size negatively impacts model quality, suggesting a need for a more complex compatibility function. It also notes that larger models and dropout techniques enhance performance. The conclusion highlights the Transformer as a novel attention-based sequence transduction model that outperforms traditional architectures in translation tasks, achieving state-of-the-art results. Future work aims to extend the Transformer to other modalities and improve generation efficiency. The acknowledgments express gratitude to contributors for their insights.',\n",
       " 'The table indicates that reducing the attention key size (dk) negatively impacts model quality, highlighting the need for a more complex compatibility function than the dot product. Larger models and dropout techniques improve performance and mitigate overfitting. The Transformer model, introduced as a novel sequence transduction model based solely on attention, demonstrates significantly faster training for translation tasks compared to recurrent or convolutional architectures, achieving state-of-the-art results in English-to-German and English-to-French translation. Future work aims to extend the Transformer to various modalities and improve efficiency in handling large inputs and outputs.',\n",
       " 'The text discusses the performance of the Transformer model, which relies entirely on attention mechanisms, outperforming traditional recurrent and convolutional architectures in translation tasks. Key observations include the detrimental effect of reducing attention key size on model quality, the benefits of larger models and dropout for preventing overfitting, and the effectiveness of learned positional embeddings. The Transformer achieves state-of-the-art results on English-to-German and English-to-French translation tasks and has potential for broader applications beyond text. Future research aims to explore other modalities and improve efficiency in handling large inputs and outputs.',\n",
       " 'The table discusses the impact of reducing attention key size on model quality, indicating that smaller sizes negatively affect performance. It also highlights that larger models and dropout techniques improve outcomes. The Transformer model, which relies solely on attention mechanisms instead of recurrent layers, achieves state-of-the-art results in translation tasks, particularly in English-to-German and English-to-French translations. Future research will focus on applying attention-based models to various input-output modalities and improving efficiency in handling large data types. The accompanying code for model training and evaluation is available online.',\n",
       " 'The table indicates that reducing the attention key size negatively impacts model quality, suggesting the need for a more complex compatibility function than the dot product. It also highlights that larger models perform better and that dropout effectively prevents overfitting. The Transformer model, which relies solely on attention mechanisms instead of recurrent layers, achieves state-of-the-art results in English-to-German and English-to-French translation tasks, and is expected to be extended to other modalities beyond text. The authors express gratitude to contributors and provide a link to their training code.',\n",
       " \"The table discusses the impact of attention key size on model quality, highlighting that smaller sizes can degrade performance and suggesting the need for more advanced compatibility functions. It also notes that larger models and dropout techniques improve performance, while replacing sinusoidal positional encoding with learned embeddings yields similar results. The conclusion emphasizes the Transformer model's efficiency and effectiveness in translation tasks, achieving state-of-the-art results, and outlines future research directions, including applications beyond text.\",\n",
       " 'The text discusses the performance of the Transformer model, which utilizes attention mechanisms instead of recurrent layers for sequence transduction tasks. It highlights that reducing the attention key size negatively impacts model quality, and larger models with dropout techniques improve performance and mitigate overfitting. The Transformer achieves state-of-the-art results in English-to-German and English-to-French translation tasks, significantly faster than traditional architectures. Future research aims to apply attention-based models to various modalities beyond text and explore efficient handling of large inputs and outputs. The code for the model is publicly available.',\n",
       " 'The table indicates that reducing the attention key size negatively impacts model quality, suggesting a need for a more complex compatibility function. Larger models and dropout techniques improve performance and prevent overfitting. The Transformer model, which relies solely on attention mechanisms instead of recurrent layers, achieves state-of-the-art results in translation tasks, significantly outperforming previous architectures. Future research will explore applying attention-based models to various tasks beyond text, including images and audio, and aims to enhance efficiency in handling large inputs and outputs. The code used for model training and evaluation is publicly available.',\n",
       " \"The text discusses the performance and findings related to the Transformer model, a novel sequence transduction model based on attention mechanisms. Key observations include that a reduction in attention key size negatively impacts model quality, larger models yield better results, and dropout effectively mitigates overfitting. The Transformer achieves state-of-the-art results in translation tasks, outperforming previous models, and the authors express interest in extending the model's application beyond text to other modalities. The code for their implementation is publicly available. The conclusion emphasizes the potential of attention-based models for future research.\",\n",
       " 'The table discusses the impact of attention key size on model quality, indicating that smaller sizes negatively affect performance. It emphasizes that larger models perform better and that dropout is effective in preventing overfitting. The Transformer model, which relies solely on attention mechanisms, achieves state-of-the-art results in translation tasks, surpassing previous models. Future research will explore applying attention-based models to other domains and improving efficiency for large inputs and outputs. The code for the models is available online.',\n",
       " 'The text discusses findings related to the Transformer model, emphasizing that reducing attention key size negatively impacts model quality, while larger models and dropout help mitigate overfitting. The Transformer, a sequence transduction model based solely on attention, outperforms traditional architectures in translation tasks, achieving state-of-the-art results in English-to-German and English-to-French translation. Future research aims to apply attention-based models to diverse tasks and explore efficient handling of large inputs like images and audio. The code for the model is available online.',\n",
       " 'The text discusses the performance of the Transformer model, which utilizes attention mechanisms instead of recurrent layers for sequence transduction tasks. It highlights that reducing the attention key size negatively impacts model quality, while larger models and dropout techniques improve performance. The Transformer achieves state-of-the-art results in English-to-German and English-to-French translation tasks, outperforming previous models, including ensembles. Future research directions include applying attention-based models to various input/output modalities and improving efficiency in handling large data types. The code for the model is available online.',\n",
       " 'The table discusses the impact of attention key size on model quality, indicating that smaller sizes hurt performance. It also notes that larger models and dropout help prevent overfitting. The Transformer model, introduced in the conclusion, outperforms previous architectures in translation tasks and is faster to train. Future research will explore applying attention-based models to various tasks beyond text, including images, audio, and video. The acknowledgements express gratitude to contributors for their insights.',\n",
       " 'The table discusses the impact of attention key size on model quality, indicating that smaller sizes negatively affect performance. It highlights that larger models and dropout techniques improve results. The Transformer model, introduced as a sequence transduction model solely based on attention, outperforms traditional recurrent architectures in translation tasks, achieving state-of-the-art results in English-to-German and English-to-French translations. Future research will explore applying attention-based models to other modalities and improving efficiency for large inputs and outputs. The acknowledgments express gratitude for contributions to the work.',\n",
       " 'The table discusses the impact of various model parameters on performance, indicating that reducing attention key size negatively affects model quality, larger models perform better, and dropout helps prevent overfitting. The Transformer model, which relies solely on attention mechanisms instead of recurrent layers, achieves state-of-the-art results in English-to-German and English-to-French translation tasks, training significantly faster than traditional architectures. Future research will explore applying attention-based models to diverse tasks and improving efficiency for large inputs and outputs. The code for the models is publicly available.',\n",
       " 'The table indicates that reducing the attention key size (dk) negatively impacts model quality, suggesting a need for a more complex compatibility function. Larger models and dropout techniques improve performance and mitigate overfitting. The Transformer model, based solely on attention mechanisms, outperforms traditional recurrent architectures in translation tasks, achieving state-of-the-art results on WMT 2014 English-to-German and English-to-French. Future research will explore applying attention models to diverse tasks and improving efficiency for large inputs and outputs. The code for the models is publicly available.',\n",
       " 'The table indicates that reducing the attention key size negatively impacts model quality, suggesting a need for a more complex compatibility function. Larger models perform better, and dropout is effective in preventing overfitting. The Transformer model, which utilizes multi-headed self-attention instead of recurrent layers, achieves state-of-the-art results in English-to-German and English-to-French translation tasks, demonstrating faster training than traditional architectures. Future work aims to apply attention-based models to various tasks and explore local attention mechanisms for handling large inputs and outputs.',\n",
       " 'The table indicates that reducing the attention key size negatively impacts model quality, suggesting the need for a more advanced compatibility function. Larger models perform better, and dropout is effective in preventing overfitting. The Transformer model, which relies solely on attention mechanisms instead of recurrent layers, achieves state-of-the-art results in English-to-German and English-to-French translation tasks, offering faster training. Future research will explore applying attention-based models to other modalities and improving efficiency in handling large inputs like images and audio. The code for training and evaluating the models is publicly available.',\n",
       " 'The table indicates that reducing the attention key size negatively impacts model quality, suggesting a need for more advanced compatibility functions. Larger models and dropout techniques improve performance and mitigate overfitting. The Transformer model, based solely on attention mechanisms, outperforms traditional recurrent architectures in translation tasks, achieving state-of-the-art results on English-to-German and English-to-French datasets. Future research will explore applications beyond text and address efficiency in handling large inputs like images and audio.',\n",
       " 'The table indicates that reducing the attention key size negatively impacts model quality, highlighting the need for a more complex compatibility function than the dot product. It also shows that larger models and dropout techniques improve performance and mitigate overfitting. The Transformer model, which relies solely on attention mechanisms instead of recurrent layers, achieves state-of-the-art results in translation tasks, significantly outperforming previous models. Future research will explore applying Transformers to various modalities beyond text and improving efficiency in handling large inputs and outputs. The accompanying code is available on GitHub.',\n",
       " 'The text discusses the performance of the Transformer model, which relies on attention mechanisms instead of recurrent layers, achieving state-of-the-art results in translation tasks. Key findings indicate that reducing attention key size negatively impacts model quality, larger models perform better, and dropout helps prevent overfitting. The Transformer can be trained faster than traditional models and has potential applications beyond text, including images and audio. The authors express enthusiasm for future developments in attention-based models and provide a link to their code repository.',\n",
       " 'The table discusses the impact of reducing attention key size on model quality, indicating that smaller sizes may hurt performance and suggesting the need for more complex compatibility functions. It also highlights that larger models and dropout techniques improve performance and that replacing sinusoidal positional encoding with learned embeddings yields similar results. The conclusion presents the Transformer model, emphasizing its efficiency in sequence transduction tasks compared to recurrent or convolutional architectures, achieving state-of-the-art results in English-to-German and English-to-French translations. Future research will explore applying attention-based models to various tasks and improving efficiency in handling large inputs like images and audio.',\n",
       " 'The table indicates that reducing the attention key size negatively impacts model quality, highlighting the need for a more complex compatibility function than the dot product. Larger models perform better, and dropout is effective in preventing overfitting. The study presents the Transformer, a novel sequence transduction model that utilizes attention instead of recurrent layers, achieving state-of-the-art results in English-to-German and English-to-French translation tasks. Future work aims to adapt the Transformer for other modalities and improve efficiency in handling large inputs like images and audio. The code for the models is publicly available.',\n",
       " 'The table discusses the impact of various factors on model quality in attention-based architectures, particularly the Transformer model. It highlights that reducing the attention key size (dk) negatively affects performance, while larger models and dropout help mitigate overfitting. The Transformer, which relies solely on attention mechanisms instead of recurrent layers, achieves state-of-the-art results in translation tasks, significantly outperforming previous models. Future research aims to apply attention-based models to various tasks beyond text and improve efficiency in handling large inputs and outputs.',\n",
       " 'The text discusses the performance of a Transformer model, which utilizes attention mechanisms instead of recurrent layers for sequence transduction tasks. It highlights that reducing the attention key size negatively impacts model quality and emphasizes the benefits of larger models and dropout to prevent overfitting. The Transformer achieves state-of-the-art results in English-to-German and English-to-French translation tasks, outperforming previous models. Future research aims to extend the Transformer to various modalities beyond text and improve efficiency in handling large inputs and outputs. The code for the model is publicly available.',\n",
       " 'The text discusses the performance of the Transformer model, which utilizes attention mechanisms instead of recurrent layers for sequence transduction. It highlights that reducing the attention key size negatively affects model quality and emphasizes the benefits of larger models and dropout in preventing overfitting. The Transformer achieves state-of-the-art results in English-to-German and English-to-French translation tasks and is noted for its faster training compared to traditional architectures. Future research aims to extend the model to other modalities and improve efficiency for large inputs.',\n",
       " 'The text discusses the performance of the Transformer model, which utilizes attention mechanisms instead of recurrent layers for sequence transduction. It highlights that reducing the attention key size negatively impacts model quality, while larger models and dropout techniques improve performance. The Transformer achieves state-of-the-art results in translation tasks, specifically surpassing previous models in English-to-German and English-to-French translations. Future research aims to apply attention-based models to various tasks and improve efficiency in handling large inputs and outputs. The code for the model is available online, and acknowledgments are given to contributors.',\n",
       " \"The table indicates that reducing the attention key size negatively impacts model quality, suggesting the need for a more complex compatibility function. Larger models and dropout techniques improve performance and mitigate overfitting. The Transformer model, which relies solely on attention mechanisms instead of recurrent layers, achieves state-of-the-art results in translation tasks, significantly outpacing previous architectures. Future work aims to extend the model's application to various tasks beyond text and enhance efficiency for larger inputs and outputs. The research code is publicly available.\",\n",
       " 'The text discusses the performance and architecture of the Transformer model, which utilizes attention mechanisms instead of recurrent layers for sequence transduction. It highlights that reducing attention key size negatively impacts model quality, while larger models and dropout techniques improve performance. The Transformer achieves state-of-the-art results in translation tasks, significantly outpacing previous architectures. Future research will explore applying attention-based models to various tasks beyond text, including images and audio, and aims to reduce sequential generation processes. The code used for training the models is publicly available.',\n",
       " 'The table indicates that reducing the attention key size negatively impacts model quality, suggesting a need for a more sophisticated compatibility function. Larger models and dropout techniques improve performance and help prevent overfitting. The Transformer model, which relies solely on attention mechanisms instead of recurrent layers, achieves state-of-the-art results in translation tasks, significantly outperforming previous models. Future research will explore applying attention-based models to various tasks and improving efficiency for large inputs and outputs.',\n",
       " 'The text discusses the performance and findings related to the Transformer model, which utilizes attention mechanisms instead of recurrent layers for sequence transduction. It notes that reducing attention key size negatively impacts model quality, while larger models and dropout techniques enhance performance. The Transformer achieves state-of-the-art results in English-to-German and English-to-French translation tasks, significantly outpacing previous architectures. Future research aims to apply attention-based models to diverse tasks, including those involving non-text modalities, and to improve efficiency in handling large inputs and outputs. The code used for training the models is publicly available.',\n",
       " 'The table indicates that reducing the attention key size negatively impacts model quality, while larger models and dropout help prevent overfitting. The Transformer model, which utilizes multi-headed self-attention instead of recurrent layers, achieves state-of-the-art results in translation tasks, significantly outperforming previous architectures. Future research will explore applying attention-based models to various tasks, including non-text modalities, and improving efficiency in handling large inputs and outputs. The code for the models is publicly available.',\n",
       " 'The table indicates that reducing the attention key size negatively impacts model quality, suggesting the need for a more complex compatibility function. Larger models and dropout techniques improve performance and mitigate overfitting. The Transformer model, which relies solely on attention mechanisms instead of recurrent layers, achieves state-of-the-art results in translation tasks, significantly outperforming prior models. Future research will explore applying attention-based models to different modalities and improving efficiency for large inputs and outputs. The code for training and evaluation is available online.',\n",
       " 'The text discusses the performance and findings related to the Transformer model, emphasizing that reducing the attention key size negatively impacts model quality and suggesting that a more advanced compatibility function may be beneficial. It highlights that larger models and dropout techniques enhance performance and notes that replacing sinusoidal positional encoding with learned embeddings yields similar results. The Transformer, which relies solely on attention mechanisms instead of recurrent layers, achieves state-of-the-art results in translation tasks, outperforming previous models. Future research will explore applying attention-based models to other modalities and improving efficiency for large inputs and outputs. The code for the experiments is publicly available.',\n",
       " 'The table indicates that reducing the attention key size negatively affects model quality, suggesting a need for a more complex compatibility function. Larger models and dropout techniques improve performance and mitigate overfitting. The Transformer model, based solely on attention mechanisms, outperforms traditional architectures in translation tasks, achieving state-of-the-art results in English-to-German and English-to-French translations. Future research aims to extend the Transformer to various tasks beyond text and improve efficiency in handling large inputs like images and audio. The accompanying code for the model is available online.',\n",
       " 'The text discusses the performance of the Transformer model, which utilizes attention mechanisms instead of recurrent layers, achieving state-of-the-art results in translation tasks. Key findings indicate that reducing attention key size negatively impacts model quality, larger models perform better, and dropout helps prevent overfitting. The model was tested on WMT 2014 English-to-German and English-to-French translation tasks, surpassing previous benchmarks. Future research will explore applying attention models to various tasks and optimizing for large inputs like images and audio. The code for the model is available on GitHub.',\n",
       " 'The text discusses findings related to the Transformer model, highlighting that reducing the attention key size negatively affects model quality and suggesting the need for improved compatibility functions. It notes that larger models and dropout help prevent overfitting. The Transformer, a novel sequence transduction model based solely on attention, outperforms traditional architectures in translation tasks, achieving state-of-the-art results on WMT 2014 English-to-German and English-to-French tasks. The authors express interest in applying attention-based models to various tasks and improving efficiency in handling large inputs and outputs. The code for their models is publicly available.',\n",
       " \"The text discusses the performance of the Transformer model, highlighting that reducing the attention key size negatively impacts model quality, suggesting the need for a more complex compatibility function. It notes that larger models and dropout techniques improve performance and that replacing sinusoidal positional encoding with learned embeddings yields similar results. The Transformer, based solely on attention mechanisms, outperforms traditional recurrent models in translation tasks, achieving state-of-the-art results on English-to-German and English-to-French translations. Future research aims to extend the model's application to other modalities and improve efficiency in handling large inputs.\",\n",
       " 'The table indicates that reducing the attention key size negatively impacts model quality, suggesting a need for a more complex compatibility function. Larger models and dropout techniques improve performance, while replacing sinusoidal positional encoding with learned embeddings yields similar results. The conclusion highlights the Transformer as a novel attention-based model for sequence transduction, achieving state-of-the-art results in translation tasks and promising future applications in various modalities. The research acknowledges contributions from collaborators and provides a link to the code used for training and evaluation.',\n",
       " 'The table indicates that reducing the attention key size negatively impacts model quality, highlighting the need for a more advanced compatibility function. Larger models and dropout techniques improve performance and mitigate overfitting. The Transformer model, which relies solely on attention mechanisms instead of recurrent layers, demonstrates superior training speed and achieves state-of-the-art results in English-to-German and English-to-French translation tasks. Future research aims to expand the Transformers applications beyond text and explore local attention mechanisms for handling larger inputs like images and audio. Acknowledgments are given to contributors for their insights.',\n",
       " 'The table highlights that reducing the attention key size negatively impacts model quality, indicating the need for a more advanced compatibility function. Larger models and dropout techniques improve performance and mitigate overfitting. The Transformer model, introduced in the study, achieves state-of-the-art results in English-to-German and English-to-French translation tasks, demonstrating faster training compared to traditional architectures. Future research will explore applications beyond text and investigate local attention mechanisms for handling larger data types. The authors express gratitude for contributions from peers and provide a link to their code repository.',\n",
       " 'The table indicates that reducing the attention key size negatively impacts model quality, suggesting the need for a more complex compatibility function than the dot product. Larger models and dropout techniques improve performance and mitigate overfitting. The Transformer model, which relies solely on attention mechanisms, outperforms traditional recurrent or convolutional architectures in translation tasks, achieving state-of-the-art results on WMT 2014 English-to-German and English-to-French. Future research will explore applying attention-based models to various tasks, including non-text modalities, and enhancing efficiency in handling large inputs and outputs. The authors express gratitude for contributions from colleagues and provide a link to their code repository.',\n",
       " \"The text discusses the performance of the Transformer model, which utilizes attention mechanisms instead of recurrent layers for sequence transduction tasks. It highlights that reducing the attention key size negatively impacts model quality, while larger models and dropout techniques improve performance and prevent overfitting. The Transformer achieves state-of-the-art results in translation tasks, outperforming previous models, and opens avenues for applying attention-based methods to other domains like images and audio. The model's code is available for public use, and acknowledgments are given for contributions to the research.\"]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "24dce07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "\n",
    "# The vectorstore to use to index the child chunks\n",
    "vectorstore = Chroma(collection_name=\"multi_modal_rag\", embedding_function=OpenAIEmbeddings())\n",
    "\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryStore()\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "# The retriever (empty to start)\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    id_key=id_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "ebf21604",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ids = [str(uuid.uuid4()) for _ in texts]\n",
    "summary_texts = [\n",
    "    Document(page_content=summary, metadata={id_key: doc_ids[i]}) for i, summary in enumerate(text_summaries)\n",
    "]\n",
    "retriever.vectorstore.add_documents(summary_texts)\n",
    "retriever.docstore.mset(list(zip(doc_ids, texts)))\n",
    "\n",
    "# Add tables\n",
    "table_ids = [str(uuid.uuid4()) for _ in tables]\n",
    "summary_tables = [\n",
    "    Document(page_content=summary, metadata={id_key: table_ids[i]}) for i, summary in enumerate(table_summaries)\n",
    "]\n",
    "retriever.vectorstore.add_documents(summary_tables)\n",
    "retriever.docstore.mset(list(zip(table_ids, tables)))\n",
    "\n",
    "# Add image summaries\n",
    "img_ids = [str(uuid.uuid4()) for _ in images]\n",
    "summary_img = [\n",
    "    Document(page_content=summary, metadata={id_key: img_ids[i]}) for i, summary in enumerate(image_summaries)\n",
    "]\n",
    "retriever.vectorstore.add_documents(summary_img)\n",
    "retriever.docstore.mset(list(zip(img_ids, images)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "077097d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2.1 Scaled Dot-Product Attention\n",
      "\n",
      "We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\n",
      "\n",
      "3\n",
      "\n",
      "Scaled Dot-Product Attention\n",
      "\n",
      "Multi-Head Attention\n",
      "\n",
      "Linear\n",
      "\n",
      "Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel.\n",
      "\n",
      "\n",
      "\n",
      "query with all keys, divide each by dk, and apply a softmax function to obtain the weights on the values.\n",
      "\n",
      "In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q. The keys and values are also packed together into matrices K and V . We compute the matrix of outputs as:\n",
      "\n",
      "Attention(Q,K,V ) = softmax( QKT  dk )V (1)\n",
      "\n",
      "The two most commonly used attention functions are additive attention [2], and dot-product (multi- plicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor 1 of . Additive attention computes the compatibility function using a feed-forward network with dk a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efcient in practice, since it can be implemented using highly optimized matrix multiplication code.\n",
      "\n",
      "While for small values of dk the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dk [3]. We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients 4. To counteract this effect, we scale the dot products by 1 . dk\n",
      "\n",
      "3.2.2 Multi-Head Attention\n",
      "\n",
      "Instead of performing a single attention function with dmodel-dimensional keys, values and queries, we found it benecial to linearly project the queries, keys and values h times with different, learned linear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional output values. These are concatenated and once again projected, resulting in the nal values, as depicted in Figure 2.\n",
      "\n",
      "Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.\n",
      "\n",
      "To illustrate why the dot products get large, assume that the components of q and k are independent random variables with mean 0 and variance 1. Then their dot product, g -k = ves, qiki, has mean 0 and variance dx.\n",
      "\n",
      "4\n",
      "\n",
      "MultiHead(Q,K,V ) = Concat(head1,...,headh)W O where headi = Attention(QW Q i ,KW K i ,V W V i )\n",
      "\n",
      "Where the projections are parameter matrices W Q and W O  Rhdvdmodel. i  Rdmodeldk, W K i  Rdmodeldk, W V i  Rdmodeldv\n",
      "\n",
      "In this work we employ h = 8 parallel attention layers, or heads. For each of these we use dk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "3.2.1 Scaled Dot-Product Attention\n",
      "\n",
      "We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\n",
      "\n",
      "3\n",
      "\n",
      "Scaled Dot-Product Attention\n",
      "\n",
      "Multi-Head Attention\n",
      "\n",
      "Linear\n",
      "\n",
      "Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel.\n",
      "\n",
      "\n",
      "\n",
      "query with all keys, divide each by dk, and apply a softmax function to obtain the weights on the values.\n",
      "\n",
      "In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q. The keys and values are also packed together into matrices K and V . We compute the matrix of outputs as:\n",
      "\n",
      "Attention(Q,K,V ) = softmax( QKT  dk )V (1)\n",
      "\n",
      "The two most commonly used attention functions are additive attention [2], and dot-product (multi- plicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor 1 of . Additive attention computes the compatibility function using a feed-forward network with dk a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efcient in practice, since it can be implemented using highly optimized matrix multiplication code.\n",
      "\n",
      "While for small values of dk the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dk [3]. We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients 4. To counteract this effect, we scale the dot products by 1 . dk\n",
      "\n",
      "3.2.2 Multi-Head Attention\n",
      "\n",
      "Instead of performing a single attention function with dmodel-dimensional keys, values and queries, we found it benecial to linearly project the queries, keys and values h times with different, learned linear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional output values. These are concatenated and once again projected, resulting in the nal values, as depicted in Figure 2.\n",
      "\n",
      "Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.\n",
      "\n",
      "To illustrate why the dot products get large, assume that the components of q and k are independent random variables with mean 0 and variance 1. Then their dot product, g -k = ves, qiki, has mean 0 and variance dx.\n",
      "\n",
      "4\n",
      "\n",
      "MultiHead(Q,K,V ) = Concat(head1,...,headh)W O where headi = Attention(QW Q i ,KW K i ,V W V i )\n",
      "\n",
      "Where the projections are parameter matrices W Q and W O  Rhdvdmodel. i  Rdmodeldk, W K i  Rdmodeldk, W V i  Rdmodeldv\n",
      "\n",
      "In this work we employ h = 8 parallel attention layers, or heads. For each of these we use dk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "docs = retriever.invoke(\n",
    "    \"what is multi head attention?\"\n",
    ")\n",
    "for doc in docs:\n",
    "    print(str(doc) + \"\\n\\n\" + \"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "2674d1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RAG PipleLine\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from base64 import b64decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "02fde482",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_docs(docs):\n",
    "    \"\"\"Split base64-encoded images and texts\"\"\"\n",
    "    b64 = []\n",
    "    text = []\n",
    "    for doc in docs:\n",
    "        try:\n",
    "            b64decode(doc)\n",
    "            b64.append(doc)\n",
    "        except Exception as e:\n",
    "            text.append(doc)\n",
    "    return {\"images\": b64, \"texts\": text}\n",
    "\n",
    "def build_prompt(kwargs):\n",
    "\n",
    "    docs_by_type = kwargs[\"context\"]\n",
    "    user_question = kwargs[\"question\"]\n",
    "\n",
    "    context_text = \"\"\n",
    "    if len(docs_by_type[\"texts\"]) > 0:\n",
    "        for text_element in docs_by_type[\"texts\"]:\n",
    "            context_text += text_element.text\n",
    "\n",
    "    # construct prompt with context (including images)\n",
    "    prompt_template = f\"\"\"\n",
    "    Answer the question based only on the following context, which can include text, tables, and the below image.\n",
    "    Context: {context_text}\n",
    "    Question: {user_question}\n",
    "    \"\"\"\n",
    "\n",
    "    prompt_content = [{\"type\": \"text\", \"text\": prompt_template}]\n",
    "\n",
    "    if len(docs_by_type[\"images\"]) > 0:\n",
    "        for image in docs_by_type[\"images\"]:\n",
    "            prompt_content.append(\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image}\"},\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            HumanMessage(content=prompt_content),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": retriever | RunnableLambda(parse_docs),\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | RunnableLambda(build_prompt)\n",
    "    | ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain_with_sources = {\n",
    "    \"context\": retriever | RunnableLambda(parse_docs),\n",
    "    \"question\": RunnablePassthrough(),\n",
    "} | RunnablePassthrough().assign(\n",
    "    response=(\n",
    "        RunnableLambda(build_prompt)\n",
    "        | ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "        | StrOutputParser()\n",
    "    )\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "af91d464",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke(\n",
    "    \"What is the attention mechanism?\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf472793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "589"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "e1e4e2fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The attention mechanism is a technique in neural networks that allows the model to focus on specific parts of the input sequence when producing an output. It enables the model to draw dependencies between input and output elements without regard to their distance in the sequence. This means that the model can attend to different parts of the input when generating each element of the output, effectively capturing relationships that may not be sequentially close. In the context of the Transformer architecture, attention is used without recurrence or convolutions, allowing for more parallelization and improved computational efficiency.\n",
      "\n",
      "\n",
      "Context:\n",
      "Attention Is All You Need\n",
      "\n",
      "Ashish Vaswani Google Brain avaswani@google.com\n",
      "\n",
      "Noam Shazeer Google Brain noam@google.com\n",
      "\n",
      "Niki Parmar\n",
      "\n",
      "Google Research nikip@google.com\n",
      "\n",
      "Jakob Uszkoreit Google Research usz@google.com\n",
      "\n",
      "Llion Jones Google Research llion@google.com\n",
      "\n",
      "Aidan N. Gomez  University of Toronto aidan@cs.toronto.edu\n",
      "\n",
      "ukasz Kaiser Google Brain lukaszkaiser@google.com\n",
      "\n",
      "Illia Polosukhin \n",
      "\n",
      "illia.polosukhin@gmail.com\n",
      "\n",
      "Abstract\n",
      "\n",
      "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signicantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English- to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.\n",
      "\n",
      "1 Introduction\n",
      "\n",
      "Recurrent neural networks, long short-term memory [12] and gated recurrent [7] neural networks in particular, have been rmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [29, 2, 5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [31, 21, 13].\n",
      "\n",
      "Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the rst Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.\n",
      "\n",
      "Work performed while at Google Brain.\n",
      "\n",
      "Work performed while at Google Research.\n",
      "\n",
      "31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\n",
      "\n",
      "Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states ht, as a function of the previous hidden state ht1 and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved signicant improvements in computational efciency through factorization tricks [18] and conditional computation [26], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains.\n",
      "\n",
      "Attention mechanisms have become an integral part of compelling sequence modeling and transduc- tion models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms are used in conjunction with a recurrent network.\n",
      "\n",
      "In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for signicantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
      "Page number:  1\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Attention Is All You Need\n",
      "\n",
      "Ashish Vaswani Google Brain avaswani@google.com\n",
      "\n",
      "Noam Shazeer Google Brain noam@google.com\n",
      "\n",
      "Niki Parmar\n",
      "\n",
      "Google Research nikip@google.com\n",
      "\n",
      "Jakob Uszkoreit Google Research usz@google.com\n",
      "\n",
      "Llion Jones Google Research llion@google.com\n",
      "\n",
      "Aidan N. Gomez  University of Toronto aidan@cs.toronto.edu\n",
      "\n",
      "ukasz Kaiser Google Brain lukaszkaiser@google.com\n",
      "\n",
      "Illia Polosukhin \n",
      "\n",
      "illia.polosukhin@gmail.com\n",
      "\n",
      "Abstract\n",
      "\n",
      "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signicantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English- to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.\n",
      "\n",
      "1 Introduction\n",
      "\n",
      "Recurrent neural networks, long short-term memory [12] and gated recurrent [7] neural networks in particular, have been rmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [29, 2, 5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [31, 21, 13].\n",
      "\n",
      "Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the rst Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.\n",
      "\n",
      "Work performed while at Google Brain.\n",
      "\n",
      "Work performed while at Google Research.\n",
      "\n",
      "31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\n",
      "\n",
      "Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states ht, as a function of the previous hidden state ht1 and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved signicant improvements in computational efciency through factorization tricks [18] and conditional computation [26], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains.\n",
      "\n",
      "Attention mechanisms have become an integral part of compelling sequence modeling and transduc- tion models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms are used in conjunction with a recurrent network.\n",
      "\n",
      "In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for signicantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
      "Page number:  1\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chain_with_sources.invoke(\n",
    "    \"What is the attention mechanism?\"\n",
    ")\n",
    "\n",
    "print(\"Response:\", response['response'])\n",
    "\n",
    "print(\"\\n\\nContext:\")\n",
    "for text in response['context']['texts']:\n",
    "    print(text.text)\n",
    "    print(\"Page number: \", text.metadata.page_number)\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "for image in response['context']['images']:\n",
    "    display_base64_image(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "6ec2aa0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': {'images': [],\n",
       "  'texts': [<unstructured.documents.elements.CompositeElement at 0x22ab6be20b0>,\n",
       "   <unstructured.documents.elements.CompositeElement at 0x22ab6be20b0>]},\n",
       " 'question': 'What is multihead?',\n",
       " 'response': 'Multi-head attention is a mechanism that consists of multiple parallel attention layers (or heads) that allow a model to jointly attend to information from different representation subspaces at various positions. Instead of performing a single attention function with dimensional keys, values, and queries, the queries, keys, and values are linearly projected multiple times (h times), producing different representations. Each projected version undergoes the attention function in parallel, resulting in output values that are then concatenated and projected again to yield the final output. This approach enables the model to capture a broader spectrum of relationships and complexities in the input data. In the context provided, 8 parallel attention layers are used, where each attention head has reduced dimensions.'}"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "a049cb4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Maximum Path Length for the Self-Attention Layer Type is O(1).\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke(\n",
    "    \"What is the Maximum Path Length for Self-Attention  Layer Type?\"\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "ca80e73c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Maximum Path Length for the Self-Attention Layer Type is O(1).'"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "edc14d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke(\n",
    "    \"What is the next step after SoftMax in Scaled Dot-Product Attention?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "5eaaec8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After applying the SoftMax function in Scaled Dot-Product Attention, the next step is to use the resulting weights to compute a weighted sum of the values. Specifically, you multiply the SoftMax output by the matrix of values \\( V \\) to obtain the final output of the attention mechanism. The formula for this step is:\n",
      "\n",
      "\\[\n",
      "\\text{Attention}(Q,K,V) = \\text{SoftMax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
      "\\]\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d09743",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
